{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging: Agenda\n",
    "* <a href=\"#section1\">What are parts of speech? Why are they useful?</a>\n",
    "* <a href=\"#section2\">How do you use them with SpaCy?</a>\n",
    "* <a href=\"#section3\">How do we infer them?</a>\n",
    "* <a href=\"#section4\"> How do we learn them with SpaCy?</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.en import English\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "def rep_sentences(texts):\n",
    "    html = []\n",
    "    for text in texts:\n",
    "        html.append(rep_sentence(text))\n",
    "    return HTML(\"\".join(html))\n",
    "\n",
    "def rep_sentence(text, display_pos = True):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'purple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    doc = nlp(text)\n",
    "    n_words = len(doc)\n",
    "    unique_pos = list(set(map(lambda x: x.pos_, doc)))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    html.append(\"<tr>\")            \n",
    "    for i in range(n_words):\n",
    "        word_string= doc[i].orth_\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "    html.append(\"</tr>\")\n",
    "    if display_pos:\n",
    "        html.append(\"<tr>\")            \n",
    "        for i in range(n_words):\n",
    "            pos = doc[i].pos_\n",
    "            color = pos_to_color[pos]\n",
    "            html.append(\"<td><span class='{0}'>{0}</span></td>\".format(pos))\n",
    "        html.append(\"</tr>\")\n",
    "    html = \"\".join(html)\n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "def custom_tag_table(list_of_word_tag_tuples):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'MediumPurple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    \n",
    "    n_words = len(list_of_word_tag_tuples)\n",
    "    words, pos_list = zip(*list_of_word_tag_tuples)\n",
    "    unique_pos = list(set([pos for pair in pos_list for pos in pair]))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    for i in range(n_words):\n",
    "        html.append(\"<tr>\")            \n",
    "        word_string= words[i]\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "        row = []\n",
    "        pos_sublist = pos_list[i]\n",
    "        for pos in pos_sublist:\n",
    "            entry = \"<span class='{0}'>{0}</span> \".format(pos)\n",
    "            #print entry\n",
    "            row.append(entry)\n",
    "        row = \"\".join(row)\n",
    "        html.append(\"<td>{}</td>\".format(row))\n",
    "        html.append(\"</tr>\")\n",
    "    return \"\".join(html)\n",
    "        \n",
    "    \n",
    "\n",
    "def nltk_corpus(corpus_name):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "#read nltk corpora\n",
    "def nltk_reader(corpus_name, limit = None):\n",
    "    corpus = nltk_corpus(corpus_name)\n",
    "    fileids = corpus.fileids()\n",
    "    \n",
    "    if limit:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n",
    "    else:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n",
    "    return doc_iter\n",
    "\n",
    "universal_tags = [\n",
    "     ['Open Class Words','ADJ','adjective']\n",
    "    ,['Open Class Words','ADV','adverb']\n",
    "    ,['Open Class Words','INTJ','interjection']\n",
    "    ,['Open Class Words','NOUN','noun']\n",
    "    ,['Open Class Words','PROPN','proper noun']\n",
    "    ,['Open Class Words','VERB','verb']\n",
    "    ,['Closed Class Words','ADP','adposition']\n",
    "    ,['Closed Class Words','AUX','auxiliary']\n",
    "    ,['Closed Class Words','CCONJ','coordination conjunction']\n",
    "    ,['Closed Class Words','DET','determiner']\n",
    "    ,['Closed Class Words','NUM','numeral']\n",
    "    ,['Closed Class Words','PART','particle']\n",
    "    ,['Closed Class Words','PRON','pronoun']\n",
    "    ,['Closed Class Words','SCONJ','subordinating conjection']\n",
    "    ,['Other','PUNCT','punctuation']\n",
    "    ,['Other','SYM','symbol']\n",
    "    ,['Other','X','other']\n",
    "]\n",
    "tag_table = pd.DataFrame(universal_tags, columns = ['Category','Abbrev','Part of Speech'])\n",
    "tag_table = tag_table.set_index(['Category','Abbrev'])\n",
    "\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section1\"></a>\n",
    "\n",
    "### What are Parts of Speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Part of Speech</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Abbrev</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Open Class Words</th>\n",
       "      <th>ADJ</th>\n",
       "      <td>adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>interjection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>proper noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">Closed Class Words</th>\n",
       "      <th>ADP</th>\n",
       "      <td>adposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>auxiliary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>coordination conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>numeral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>particle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>subordinating conjection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Other</th>\n",
       "      <th>PUNCT</th>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>symbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Part of Speech\n",
       "Category           Abbrev                          \n",
       "Open Class Words   ADJ                    adjective\n",
       "                   ADV                       adverb\n",
       "                   INTJ                interjection\n",
       "                   NOUN                        noun\n",
       "                   PROPN                proper noun\n",
       "                   VERB                        verb\n",
       "Closed Class Words ADP                   adposition\n",
       "                   AUX                    auxiliary\n",
       "                   CCONJ   coordination conjunction\n",
       "                   DET                   determiner\n",
       "                   NUM                      numeral\n",
       "                   PART                    particle\n",
       "                   PRON                     pronoun\n",
       "                   SCONJ   subordinating conjection\n",
       "Other              PUNCT                punctuation\n",
       "                   SYM                       symbol\n",
       "                   X                          other"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.PRON{background-color:red;}</style><style>.DET{background-color:YellowGreen;}</style><style>.PUNCT{background-color:yellow;}</style><style>.VERB{background-color:orange;}</style><style>.ADP{background-color:pink;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>get</span></td><td><span class='word'>a</span></td><td><span class='word'>discount</span></td><td><span class='word'>on</span></td><td><span class='word'>newspapers</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr><table width=100%><style>.word{font-weight:bold;}</style><style>.VERB{background-color:SkyBlue;}</style><style>.NOUN{background-color:red;}</style><style>.PRON{background-color:YellowGreen;}</style><style>.ADP{background-color:yellow;}</style><style>.PUNCT{background-color:orange;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>discount</span></td><td><span class='word'>that</span></td><td><span class='word'>newspaper</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'I get a discount on newspapers.'\n",
    "sentence2 = 'I discount that newspaper.'\n",
    "\n",
    "rep_sentences([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='applications'></a>\n",
    "### Applications\n",
    "* Rule based systems:\n",
    "    * <a href=\"#qacode\">Example of rule based question answering component</a>\n",
    "* Feature engineering for statistical models\n",
    "    * <a href=\"#wordsense\">Feature for word disambiguation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section2\"></a>\n",
    "### Parts of Speech with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos (coarse)</th>\n",
       "      <th>pos (fine)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount</th>\n",
       "      <td>discount</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>get</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newspapers</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lemma pos (coarse) pos (fine)\n",
       "I              -PRON-         PRON        PRP\n",
       "a                   a          DET         DT\n",
       "discount     discount         NOUN         NN\n",
       "get               get         VERB        VBP\n",
       "newspapers  newspaper         NOUN        NNS\n",
       "on                 on          ADP         IN"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Accessing\n",
    "doc = nlp('I get a discount on newspapers')\n",
    "tags = {}\n",
    "\n",
    "for word in doc:\n",
    "    tags[word.orth_] = {'lemma': word.lemma_, \n",
    "                        'pos (coarse)': word.pos_, \n",
    "                        'pos (fine)':word.tag_}\n",
    "pd.DataFrame(tags).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Building word vectors that are Part of Speech specific\n",
    "Steps:\n",
    "* get documents\n",
    "* tokenize the documents, and append the part of speech to each token, e.g. dog|NOUN\n",
    "* train a word2vec model with gensim\n",
    "* compare the most similar words of 'back||||VERB' vs 'back||||NOUN' (or other combo)\n",
    "\n",
    "* Hints:\n",
    "    * model.wv.vocab contains the vocabulary.\n",
    "    * using a completely unique join character will make it easier to split later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def return_documents():\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    dataset = fetch_20newsgroups()\n",
    "    corpus = dataset.data\n",
    "    return corpus\n",
    "\n",
    "def tokenize_and_tag_documents(documents, nlp, sep_char=\"|\"):\n",
    "    pass\n",
    "\n",
    "def build_model(tokenized_docs):\n",
    "    pass\n",
    "\n",
    "def compare_most_similar_words_across_pos(word):\n",
    "    pass\n",
    "\n",
    "documents = return_documents()\n",
    "tokenized_and_tagged_documents = tokenize_and_tag_documents(documents, nlp)\n",
    "model = build_model(tokenized_and_tagged_documents)\n",
    "compare_most_similar_words_across_pos('back')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section3\"></a>\n",
    "### How do we infer parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.NOUN{background-color:SkyBlue;}</style><style>.PRON{background-color:red;}</style><style>.DET{background-color:YellowGreen;}</style><style>.ADP{background-color:yellow;}</style><style>.PUNCT{background-color:orange;}</style><style>.ADJ{background-color:pink;}</style><style>.PROPN{background-color:brown;}</style><style>.VERB{background-color:purple;}</style><style>.PART{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'>loble</span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'>effix</span></td><td><span class='word'>by</span></td><td><span class='word'>klepping</span></td><td><span class='word'>the</span></td><td><span class='word'>Dongle</span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Button\n",
    "class reveal(object):\n",
    "    def __init__(self):\n",
    "        self.text = 'I was loble to find the effix by klepping the Dongle search engine.'\n",
    "        self.toggle = Button(description='Toggle POS', )\n",
    "        self.toggle.on_click(self.toggle_pos)\n",
    "        self.state = False\n",
    "        display(self.toggle)\n",
    "        self.display()\n",
    "        \n",
    "    def toggle_pos(self, b):\n",
    "        self.state = not self.state\n",
    "        self.display()\n",
    "        \n",
    "    def display(self):\n",
    "        clear_output()\n",
    "        display(HTML(rep_sentence(text, display_pos = self.state)))\n",
    "        \n",
    "r = reveal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants of Part of Speech:\n",
    "* Word: some words can only be used in a single way; we can memorize these.\n",
    "* Word shape: if the first letter is capitalized, its likely a proper noun.\n",
    "* Neighboring part of speech: there are common patterns, such as noun phrases commonly following a determiner. to the beach\n",
    "\n",
    "\n",
    "\n",
    "| Feature | Notes | Example|\n",
    "|------|------|------|\n",
    "|   Word Identity  | Some words can only be used in a single way; we can memorize these.| \"the\" -> determiner| \n",
    "| Word Shape|Capitalization, dashes,  |\"I stayed at the Park Hotel.\"|\n",
    "|Neighboring parts of speech|There are common patterns what tags can neighbor others|\"to the beach\" (noun following determiner)|\n",
    "|Morphological Structures|Word prefixes and suffixes can rule out certain tag types|\"-ly\" -> adverb|\n",
    "|Syntactic Dependencies|Syntax may establish expectations that only certain tags can logically fill|\"I was told __\" -> adpositional phrase or object entity|\n",
    "|?|?|?|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section4\"></a>\n",
    "### Training your own tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "corpus = nltk_corpus('brown')\n",
    "all_data = np.array(corpus.tagged_sents(tagset='brown'))\n",
    "all_data = [list(zip(*i)) for i in all_data]\n",
    "train, test = train_test_split(all_data, test_size = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_(f, iterable):\n",
    "    return list(filter(f,iterable))\n",
    "\n",
    "def map_(f, iterable):\n",
    "    return list(map(f,iterable))\n",
    "def zip_(*args):\n",
    "    return list(zip(*args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': 116884, 'correct': 97184, 'accuracy': 0.8314568289928476}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized gold tag: JJ-TL. tag_map.json must contain all gold tags, to maintain coarse-grained mapping.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b406457ac3bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mgold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoldParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mcurrent_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/spacy/tagger.pyx\u001b[0m in \u001b[0;36mspacy.tagger.Tagger.update (spacy/tagger.cpp:6889)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized gold tag: JJ-TL. tag_map.json must contain all gold tags, to maintain coarse-grained mapping."
     ]
    }
   ],
   "source": [
    "#EXPERIMENTAL\n",
    "from spacy.symbols import *\n",
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "from nltk.tag import tagset_mapping\n",
    "from spacy.en import English\n",
    "from spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n",
    "                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n",
    "                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n",
    "                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n",
    "                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate(test_data, tagger):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tagmap = tagger.vocab.morphology.tag_map\n",
    "    for words, tags in test_data:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        tagger(doc)\n",
    "        predictions = map_(lambda token: tagmap[token.tag_], doc)\n",
    "        actual = map_(lambda tag: tagmap[tag], tags)\n",
    "        \n",
    "        correct_predictions = filter_(lambda x: x[0] == x[1], zip(predictions, actual))\n",
    "        n_correct = len(correct_predictions)\n",
    "        correct += n_correct\n",
    "        total += len(words)\n",
    "        \n",
    "    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n",
    "    return result\n",
    "\n",
    "def generate_tagmap():\n",
    "    def adjust_value(x):\n",
    "        if x == '.':\n",
    "            val = PUNCT\n",
    "        elif x=='PRT':\n",
    "            val = PART\n",
    "        else:\n",
    "            val = getattr(spacy.symbols, x)\n",
    "        return {POS:val}\n",
    "    nltk_map = tagset_mapping('en-brown','universal')\n",
    "    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n",
    "    return adj_map\n",
    "\n",
    "features = [(W_cluster,), (W_lemma,),    (W_pos,), (W_prefix,),  (W_suffix,), (W_shape,)\n",
    ",(P2_cluster,), (P2_lemma,), (P2_pos,), (P2_prefix,), (P2_suffix,), (P2_shape,)\n",
    ",(P1_cluster,), (P1_lemma,), (P1_pos,), (P1_prefix,), (P1_suffix,), (P1_shape,)\n",
    ",(N1_cluster,), (N1_lemma,), (N1_pos,), (N1_prefix,), (N1_suffix,), (N1_shape,)\n",
    ",(N2_cluster,), (N2_lemma,), (N2_pos,), (N2_prefix,), (N2_suffix,), (N2_shape,)]\n",
    "\n",
    "tagmap = generate_tagmap()\n",
    "gold_tagmap = nlp.vocab.morphology.tag_map\n",
    "\n",
    "for key in tagmap:\n",
    "    gold_tagmap[key] = tagmap[key]\n",
    "\n",
    "vocab = nlp.vocab\n",
    "tagger = nlp.tagger\n",
    "\n",
    "\n",
    "pretraining_accuracy = validate(test, tagger)\n",
    "print(pretraining_accuracy)\n",
    "\n",
    "for i in range(10):\n",
    "    train_fold, test_fold = train_test_split(train, test_size = .2)\n",
    "    for words, tags in train_fold:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "        tagger.update(doc, gold)\n",
    "    current_accuracy = validate(test_fold, tagger)\n",
    "    print(current_accuracy)\n",
    "    np.random.shuffle(train)\n",
    "tagger.model.end_training()\n",
    "\n",
    "posttraining_accuracy = validate(test, tagger)\n",
    "print(posttraining_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': 116884, 'correct': 14981, 'accuracy': 0.12816980938366243}\n",
      "{'words': 208437, 'correct': 52799, 'accuracy': 0.2533091533652854}\n",
      "{'words': 210067, 'correct': 66090, 'accuracy': 0.31461390889573326}\n",
      "{'words': 209335, 'correct': 56648, 'accuracy': 0.27060931043542646}\n",
      "{'words': 211223, 'correct': 56042, 'accuracy': 0.2653214848761735}\n",
      "{'words': 207431, 'correct': 57799, 'accuracy': 0.27864205446630447}\n",
      "{'words': 207494, 'correct': 68565, 'accuracy': 0.3304432899264557}\n",
      "{'words': 207906, 'correct': 65606, 'accuracy': 0.31555606860792856}\n",
      "{'words': 208654, 'correct': 64619, 'accuracy': 0.30969451819759025}\n",
      "{'words': 210271, 'correct': 63388, 'accuracy': 0.30145859390976404}\n",
      "{'words': 210742, 'correct': 72974, 'accuracy': 0.3462717445976597}\n",
      "{'words': 116884, 'correct': 48794, 'accuracy': 0.4174566236610657}\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import *\n",
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tagger import Tagger\n",
    "from spacy.tokens import Doc\n",
    "from spacy.gold import GoldParse\n",
    "from nltk.tag import tagset_mapping\n",
    "from spacy.en import English\n",
    "from spacy.tagger import W_cluster, W_lemma, W_pos, W_prefix, W_suffix, W_shape\\\n",
    "                         ,P2_cluster, P2_lemma, P2_pos, P2_prefix, P2_suffix, P2_shape\\\n",
    "                         ,P1_cluster, P1_lemma, P1_pos, P1_prefix, P1_suffix, P1_shape\\\n",
    "                         ,N1_cluster, N1_lemma, N1_pos, N1_prefix, N1_suffix, N1_shape\\\n",
    "                         ,N2_cluster, N2_lemma, N2_pos, N2_prefix, N2_suffix, N2_shape\n",
    "\n",
    "def validate(test_data, tagger):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for words, tags in test_data:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        tagger(doc)\n",
    "        predictions = map_(lambda token: tagmap[token.tag_], doc)\n",
    "        actual = map_(lambda tag: tagmap[tag], tags)\n",
    "        \n",
    "        correct_predictions = filter_(lambda x: x[0] == x[1], zip_(predictions, actual))\n",
    "        n_correct = len(correct_predictions)\n",
    "        correct += n_correct\n",
    "        total += len(words)\n",
    "        \n",
    "    result = {'correct':correct, 'words':total, 'accuracy':correct / float(total)}\n",
    "    return result\n",
    "\n",
    "def generate_tagmap():\n",
    "    def adjust_value(x):\n",
    "        if x == '.':\n",
    "            val = PUNCT\n",
    "        elif x=='PRT':\n",
    "            val = PART\n",
    "        else:\n",
    "            val = getattr(spacy.symbols, x)\n",
    "        return {POS:val}\n",
    "    nltk_map = tagset_mapping('en-brown','universal')\n",
    "    adj_map = {key:adjust_value(value) for key, value in nltk_map.items()}\n",
    "    return adj_map\n",
    "\n",
    "features = [(W_cluster,), (W_lemma,),    (W_pos,), (W_prefix,),  (W_suffix,), (W_shape,)\n",
    ",(P2_cluster,), (P2_lemma,), (P2_pos,), (P2_prefix,), (P2_suffix,), (P2_shape,)\n",
    ",(P1_cluster,), (P1_lemma,), (P1_pos,), (P1_prefix,), (P1_suffix,), (P1_shape,)\n",
    ",(N1_cluster,), (N1_lemma,), (N1_pos,), (N1_prefix,), (N1_suffix,), (N1_shape,)\n",
    ",(N2_cluster,), (N2_lemma,), (N2_pos,), (N2_prefix,), (N2_suffix,), (N2_shape,)]\n",
    "\n",
    "tagmap = generate_tagmap()\n",
    "vocab = Vocab(tag_map = tagmap)\n",
    "tagger = Tagger(vocab)\n",
    "\n",
    "\n",
    "pretraining_accuracy = validate(test, tagger)\n",
    "print(pretraining_accuracy)\n",
    "\n",
    "for i in range(10):\n",
    "    train_fold, test_fold = train_test_split(train, test_size = .2)\n",
    "    for words, tags in train_fold:\n",
    "        doc = Doc(vocab, words=words)\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "        tagger.update(doc, gold)\n",
    "    current_accuracy = validate(test_fold, tagger)\n",
    "    print(current_accuracy)\n",
    "    np.random.shuffle(train)\n",
    "tagger.model.end_training()\n",
    "\n",
    "posttraining_accuracy = validate(test, tagger)\n",
    "print(posttraining_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{74: 82}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.vocab.morphology.tag_map['JJ-TL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{74: 82}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_tagmap['JJ-TL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='qacode'></a>\n",
    "### Example Rule Based QA Component Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_answer_requirements(token):\n",
    "    if token.tag_ == 'WRB':\n",
    "        if token.lower_ == 'where':\n",
    "            #Where was Star Wars Filmed\n",
    "            return ['LOCATION']\n",
    "        elif token.lower_ == 'when':\n",
    "            #When was Star Wars Filmed\n",
    "            return ['DATE']\n",
    "        elif token.lower_ == 'how':\n",
    "            #How much did Star Wars make?\n",
    "            if token.nbor().lower_ in ('much', 'many'):\n",
    "                return ['QUANTITY']\n",
    "\n",
    "            #How old is star wars?\n",
    "            elif token.nbor().lower_ in ('long', 'old'):\n",
    "                return ['DURATION']\n",
    "            else:\n",
    "                return False\n",
    "        elif token.lower() == 'whom':\n",
    "            #Whom did you see?\n",
    "            return ['PERSON','ORG']      \n",
    "        else:\n",
    "            return False\n",
    "    elif token.tag_ == 'WP':\n",
    "        #Asking for Identity\n",
    "        if token.lower_ in ('who', 'whose'):\n",
    "            #Who directed Star Wars?\n",
    "            return ['PERSON','ORG']\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #What is Star Wars\n",
    "            return False \n",
    "        else: \n",
    "            return False\n",
    "    elif token.tag_ == 'WDT':\n",
    "        #asking for a choice among options\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #which Star Wars did you like best?\n",
    "            return [token.nbor().lower_] #return neighbor\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>\n",
    "<a name=\"wordsense\"></a>\n",
    "##### Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Synset('shower.n.01') a plumbing fixture that sprays water over you\n",
      "Synset('shower.n.02') washing yourself by standing upright under water sprayed from a nozzle\n",
      "Synset('shower.n.03') a brief period of precipitation\n",
      "Synset('shower.n.04') a sudden downpour (as of tears or sparks etc) likened to a rain shower\n",
      "Synset('exhibitor.n.01') someone who organizes an exhibit for others to see\n",
      "Synset('shower.n.06') a party of friends assembled to present gifts (usually of a specified kind) to a person\n",
      "Synset('lavish.v.01') expend profusely; also used with abstract nouns\n",
      "Synset('shower.v.02') spray or sprinkle with\n",
      "Synset('shower.v.03') take a shower; wash one's body in the shower\n",
      "Synset('shower.v.04') rain abundantly\n",
      "Synset('shower.v.05') provide abundantly with\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "for syn in wn.synsets('shower'):\n",
    "    print(syn, syn.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
