{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging: Agenda\n",
    "* <a href=\"#section1\">What are parts of speech? Why are they useful?</a>\n",
    "* <a href=\"#section2\">How do you use them with SpaCy?</a>\n",
    "* <a href=\"#section3\">How do we infer them?</a>\n",
    "* <a href=\"#section4\"> How do we learn them with SpaCy?</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Refresh notebook to reflect widget availability.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from warnings import warn\n",
    "def from_scratch():\n",
    "    !pip install gensim >> ~/gensim.log\n",
    "    !pip install spacy >> ~/spacy.log\n",
    "    !python -m spacy download en >> ~/spacy.log\n",
    "    !jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "    warn(\"Refresh notebook to reflect widget availability.\")\n",
    "\n",
    "    \n",
    "from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.en import English\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "\n",
    "def as_list(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return list(f(*args, **kwargs))\n",
    "    \n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "map_ = as_list(map)\n",
    "filter_ = as_list(filter)\n",
    "zip_ = as_list(zip)\n",
    "\n",
    "def rep_sentences(texts):\n",
    "    html = []\n",
    "    for text in texts:\n",
    "        html.append(rep_sentence(text))\n",
    "    return HTML(\"\".join(html))\n",
    "\n",
    "def rep_sentence(text, display_pos = True):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'purple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    doc = nlp(text)\n",
    "    n_words = len(doc)\n",
    "    unique_pos = list(set(map(lambda x: x.pos_, doc)))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    html.append(\"<tr>\")            \n",
    "    for i in range(n_words):\n",
    "        word_string= doc[i].orth_\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "    html.append(\"</tr>\")\n",
    "    if display_pos:\n",
    "        html.append(\"<tr>\")            \n",
    "        for i in range(n_words):\n",
    "            pos = doc[i].pos_\n",
    "            color = pos_to_color[pos]\n",
    "            html.append(\"<td><span class='{0}'>{0}</span></td>\".format(pos))\n",
    "        html.append(\"</tr>\")\n",
    "    html = \"\".join(html)\n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "def custom_tag_table(list_of_word_tag_tuples):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'MediumPurple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    \n",
    "    n_words = len(list_of_word_tag_tuples)\n",
    "    words, pos_list = zip(*list_of_word_tag_tuples)\n",
    "    unique_pos = list(set([pos for pair in pos_list for pos in pair]))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    for i in range(n_words):\n",
    "        html.append(\"<tr>\")            \n",
    "        word_string= words[i]\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "        row = []\n",
    "        pos_sublist = pos_list[i]\n",
    "        for pos in pos_sublist:\n",
    "            entry = \"<span class='{0}'>{0}</span> \".format(pos)\n",
    "            #print entry\n",
    "            row.append(entry)\n",
    "        row = \"\".join(row)\n",
    "        html.append(\"<td>{}</td>\".format(row))\n",
    "        html.append(\"</tr>\")\n",
    "    return \"\".join(html)\n",
    "        \n",
    "    \n",
    "\n",
    "def nltk_corpus(corpus_name):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "#read nltk corpora\n",
    "def nltk_reader(corpus_name, limit = None):\n",
    "    corpus = nltk_corpus(corpus_name)\n",
    "    fileids = corpus.fileids()\n",
    "    \n",
    "    if limit:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n",
    "    else:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n",
    "    return doc_iter\n",
    "\n",
    "universal_tags = [\n",
    "     ['Open Class Words','ADJ','adjective', 'happy, great, technical']\n",
    "    ,['Open Class Words','ADV','adverb', 'happily, greatly, technically']\n",
    "    ,['Open Class Words','INTJ','interjection', 'ouch, wow']\n",
    "    ,['Open Class Words','NOUN','noun', 'happiness, greatness, technicality']\n",
    "    ,['Open Class Words','PROPN','proper noun', 'Jupyter, Lakers']\n",
    "    ,['Open Class Words','VERB','verb', 'run, enlighten']\n",
    "    ,['Closed Class Words','ADP','adposition', 'in, under, towards']\n",
    "    ,['Closed Class Words','AUX','auxiliary', 'should [eat], had [been]']\n",
    "    ,['Closed Class Words','CCONJ','coordination conjunction', 'and, or, but']\n",
    "    ,['Closed Class Words','DET','determiner', 'the, a']\n",
    "    ,['Closed Class Words','NUM','numeral', 'five, 5']\n",
    "    ,['Closed Class Words','PART','particle', \"'s, not\"]\n",
    "    ,['Closed Class Words','PRON','pronoun', 'him, she']\n",
    "    ,['Closed Class Words','SCONJ','subordinating conjection', 'that, if']\n",
    "    ,['Other','PUNCT','punctuation', \"., ,\"]\n",
    "    ,['Other','SYM','symbol', \"#, $\"]\n",
    "    ,['Other','X','other', \"#2017, @JupyterCon\"]\n",
    "]\n",
    "tag_table = pd.DataFrame(universal_tags, columns = ['Category','Abbrev','Part of Speech', 'Example'])\n",
    "tag_table = tag_table.set_index(['Category','Abbrev'])\n",
    "\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section1\"></a>\n",
    "\n",
    "### What are Parts of Speech?\n",
    "\n",
    "<a name=\"universaltags\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Part of Speech</th>\n",
       "      <th>Example</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Abbrev</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Open Class Words</th>\n",
       "      <th>ADJ</th>\n",
       "      <td>adjective</td>\n",
       "      <td>happy, great, technical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>adverb</td>\n",
       "      <td>happily, greatly, technically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>interjection</td>\n",
       "      <td>ouch, wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>noun</td>\n",
       "      <td>happiness, greatness, technicality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>proper noun</td>\n",
       "      <td>Jupyter, Lakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>verb</td>\n",
       "      <td>run, enlighten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">Closed Class Words</th>\n",
       "      <th>ADP</th>\n",
       "      <td>adposition</td>\n",
       "      <td>in, under, towards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>auxiliary</td>\n",
       "      <td>should [eat], had [been]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>coordination conjunction</td>\n",
       "      <td>and, or, but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>determiner</td>\n",
       "      <td>the, a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>numeral</td>\n",
       "      <td>five, 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>particle</td>\n",
       "      <td>'s, not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>pronoun</td>\n",
       "      <td>him, she</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>subordinating conjection</td>\n",
       "      <td>that, if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Other</th>\n",
       "      <th>PUNCT</th>\n",
       "      <td>punctuation</td>\n",
       "      <td>., ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>symbol</td>\n",
       "      <td>#, $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>other</td>\n",
       "      <td>#2017, @JupyterCon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Part of Speech  \\\n",
       "Category           Abbrev                             \n",
       "Open Class Words   ADJ                    adjective   \n",
       "                   ADV                       adverb   \n",
       "                   INTJ                interjection   \n",
       "                   NOUN                        noun   \n",
       "                   PROPN                proper noun   \n",
       "                   VERB                        verb   \n",
       "Closed Class Words ADP                   adposition   \n",
       "                   AUX                    auxiliary   \n",
       "                   CCONJ   coordination conjunction   \n",
       "                   DET                   determiner   \n",
       "                   NUM                      numeral   \n",
       "                   PART                    particle   \n",
       "                   PRON                     pronoun   \n",
       "                   SCONJ   subordinating conjection   \n",
       "Other              PUNCT                punctuation   \n",
       "                   SYM                       symbol   \n",
       "                   X                          other   \n",
       "\n",
       "                                                      Example  \n",
       "Category           Abbrev                                      \n",
       "Open Class Words   ADJ                happy, great, technical  \n",
       "                   ADV          happily, greatly, technically  \n",
       "                   INTJ                             ouch, wow  \n",
       "                   NOUN    happiness, greatness, technicality  \n",
       "                   PROPN                      Jupyter, Lakers  \n",
       "                   VERB                        run, enlighten  \n",
       "Closed Class Words ADP                     in, under, towards  \n",
       "                   AUX               should [eat], had [been]  \n",
       "                   CCONJ                         and, or, but  \n",
       "                   DET                                 the, a  \n",
       "                   NUM                                five, 5  \n",
       "                   PART                               's, not  \n",
       "                   PRON                              him, she  \n",
       "                   SCONJ                             that, if  \n",
       "Other              PUNCT                                 ., ,  \n",
       "                   SYM                                   #, $  \n",
       "                   X                       #2017, @JupyterCon  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.PRON{background-color:SkyBlue;}</style><style>.DET{background-color:red;}</style><style>.NOUN{background-color:YellowGreen;}</style><style>.VERB{background-color:yellow;}</style><style>.PUNCT{background-color:orange;}</style><style>.ADP{background-color:pink;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>get</span></td><td><span class='word'>a</span></td><td><span class='word'>discount</span></td><td><span class='word'>on</span></td><td><span class='word'>newspapers</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr><table width=100%><style>.word{font-weight:bold;}</style><style>.PRON{background-color:SkyBlue;}</style><style>.NOUN{background-color:red;}</style><style>.VERB{background-color:YellowGreen;}</style><style>.ADP{background-color:yellow;}</style><style>.PUNCT{background-color:orange;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>discount</span></td><td><span class='word'>that</span></td><td><span class='word'>newspaper</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'I get a discount on newspapers.'\n",
    "sentence2 = 'I discount that newspaper.'\n",
    "\n",
    "rep_sentences([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='applications'></a>\n",
    "### Applications\n",
    "* Rule based systems:\n",
    "    * <a href=\"#qacode\">Example of rule based question answering component</a>\n",
    "* Feature engineering for statistical models\n",
    "    * <a href=\"#wordsense\">Feature for word disambiguation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section2\"></a>\n",
    "### Parts of Speech with SpaCy\n",
    "Lets see how we can access parts of speech with spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos (coarse)</th>\n",
       "      <th>pos (fine)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount</th>\n",
       "      <td>discount</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>get</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newspapers</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lemma pos (coarse) pos (fine)\n",
       "I              -PRON-         PRON        PRP\n",
       "a                   a          DET         DT\n",
       "discount     discount         NOUN         NN\n",
       "get               get         VERB        VBP\n",
       "newspapers  newspaper         NOUN        NNS\n",
       "on                 on          ADP         IN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Accessing\n",
    "doc = nlp('I get a discount on newspapers')\n",
    "tags = {}\n",
    "\n",
    "for word in doc:\n",
    "    tags[word.orth_] = {'lemma': word.lemma_, \n",
    "                        'pos (coarse)': word.pos_, \n",
    "                        'pos (fine)':word.tag_}\n",
    "pd.DataFrame(tags).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Building word vectors that are Part of Speech specific\n",
    "Steps:\n",
    "* get documents\n",
    "* tokenize the documents, and append the part of speech to each token, e.g. dog||||NOUN\n",
    "* train a word2vec model with gensim\n",
    "* compare the most similar words of 'back||||VERB' vs 'back||||NOUN' (or other combo)\n",
    "\n",
    "* Hints:\n",
    "    * model.wv.vocab contains the vocabulary.\n",
    "    * using a completely unique join character will make it easier to split later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def return_documents():\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    dataset = fetch_20newsgroups()\n",
    "    corpus = dataset.data\n",
    "    return corpus\n",
    "\n",
    "def tokenize_and_tag_documents(documents, nlp, sep_char=\"||||\"):\n",
    "    pass\n",
    "\n",
    "def build_model(tokenized_docs):\n",
    "    pass\n",
    "\n",
    "def compare_most_similar_words_across_pos(word):\n",
    "    pass\n",
    "\n",
    "documents = return_documents()\n",
    "tokenized_and_tagged_documents = tokenize_and_tag_documents(documents, nlp)\n",
    "model = build_model(tokenized_and_tagged_documents)\n",
    "compare_most_similar_words_across_pos('back')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section3\"></a>\n",
    "### How do we infer parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.PRON{background-color:SkyBlue;}</style><style>.ADJ{background-color:red;}</style><style>.DET{background-color:YellowGreen;}</style><style>.NOUN{background-color:yellow;}</style><style>.PROPN{background-color:orange;}</style><style>.VERB{background-color:pink;}</style><style>.PUNCT{background-color:brown;}</style><style>.PART{background-color:purple;}</style><style>.ADP{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'>loble</span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'>effix</span></td><td><span class='word'>by</span></td><td><span class='word'>klepping</span></td><td><span class='word'>the</span></td><td><span class='word'>Dongle</span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Button\n",
    "class reveal(object):\n",
    "    def __init__(self):\n",
    "        self.text = 'I was loble to find the effix by klepping the Dongle search engine.'\n",
    "        self.toggle = Button(description='Toggle POS', )\n",
    "        self.toggle.on_click(self.toggle_pos)\n",
    "        self.state = False\n",
    "        display(self.toggle)\n",
    "        self.display()\n",
    "        \n",
    "    def toggle_pos(self, b):\n",
    "        self.state = not self.state\n",
    "        self.display()\n",
    "        \n",
    "    def display(self):\n",
    "        clear_output()\n",
    "        display(HTML(rep_sentence(self.text, display_pos = self.state)))\n",
    "        \n",
    "r = reveal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants of Part of Speech:\n",
    "\n",
    "| Feature | Notes | Example|\n",
    "|------|------|------|\n",
    "|   Word Identity  | Some words can only be used in a single way; we can memorize these.| \"the\" -> determiner| \n",
    "| Word Shape|Capitalization, dashes,  |\"I stayed at the Park Hotel.\"|\n",
    "|Neighboring parts of speech|There are common patterns what tags can neighbor others|\"to the beach\" (noun following determiner)|\n",
    "|Morphological Structures|Word prefixes and suffixes can rule out certain tag types|\"-ly\" -> adverb|\n",
    "|Syntactic Dependencies|Syntax may establish expectations that only certain tags can logically fill|\"I was told __\" -> adpositional phrase or object entity|\n",
    "|?|?|?|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section4\"></a>\n",
    "### Training your own tagger\n",
    "\n",
    "**Steps** : \n",
    "* load <a href=\"#load_data\">**training data**</a>, where each observation is represented as (list_of_words, list_of_tags)\n",
    "* Pick a <a href=\"#model_dir\">**model directory**</a>. Using the existing English model will allow us to leverage lexeme information, including Brown clusters, which is an excellent feature for tagging.\n",
    "* Look at your dataset. Build a <a href=\"#tagmap\">**tag map**</a> mapping from the part of speech tags to the <a href=\"#universaltagset\">universal tagset</a>.\n",
    "* Decide which <a href=\"#featureextractors\">**features**</a> to use\n",
    "* create a <a href=\"#vocab\">**vocabulary object, a statistical model, and a tagger**</a>\n",
    "* <a href=\"#training\">**Train the model**</a>\n",
    "* <a href=\"#save\">**Save the model**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"load_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
     ]
    }
   ],
   "source": [
    "#load conll2000 corpus\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "def nltk_corpus(corpus_name):\n",
    "    '''returns nltk corpus by name. if not loaded, download.'''\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    if x == '-LCB-':\n",
    "        return '{'\n",
    "    elif x=='-RCB-':\n",
    "        return '}'\n",
    "    elif x == '-RRB-':\n",
    "        return \")\"\n",
    "    elif x == '-LRB-':\n",
    "        return \"(\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def conll_to_data():\n",
    "    corpus = nltk_corpus('conll2000')\n",
    "    all_data= map_(lambda x: [(clean(i[0]), i[1]) for i in x], corpus.iob_sents())\n",
    "    all_data = [zip_(*i) for i in all_data]\n",
    "    return all_data\n",
    "\n",
    "c2000 = conll_to_data()\n",
    "training_data, testing_data = train_test_split(c2000, test_size = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a name=\"model_dir\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "modelpath = PosixPath('/home/jupyter/mymodel')\n",
    "if not modelpath.exists():\n",
    "    modelpath.mkdir()\n",
    "    \n",
    "nlp.save_to_directory(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a href=\"tagmap\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.attrs import POS\n",
    "from spacy.symbols import PUNCT\n",
    "\n",
    "def adjust_tagmap(tagmap):\n",
    "    tagmap['('] = tagmap['-LRB-']\n",
    "    tagmap[')'] = tagmap['-RRB-']\n",
    "    tagmap['{'] = tagmap['-LRB-']\n",
    "    tagmap['}'] = tagmap['-RRB-']\n",
    "    tagmap['$'] = {POS: PUNCT}\n",
    "    return tagmap\n",
    "\n",
    "tagmap = adjust_tagmap(TAG_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a name=\"featureextractors\"></a>\n",
    "### Features\n",
    "\n",
    "* Example features: pos of previous word, identity of current word, etc...\n",
    "* spacy.tagger.N1_cluster, spacy.tagger.N1_pos, etc...\n",
    "* which word?\n",
    "    * N1: Next\n",
    "    * N0: Current\n",
    "    * P1: Previous\n",
    "    * etc...\n",
    "* which attribute:\n",
    "    * prefix\n",
    "    * tag\n",
    "    * cluster\n",
    "    * etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tagger import *\n",
    "\n",
    "\n",
    "features = [\n",
    "    #current word attributes\n",
    "    (W_orth,),(W_shape,),(W_cluster,),(W_flags,),(W_suffix,),(W_prefix,),\n",
    "\n",
    "    #-1 word attributes    \n",
    "    (P1_pos,),(P1_cluster,),(P1_flags,),(P1_suffix,),\n",
    "\n",
    "    #-2 word attributes     \n",
    "    (P2_pos,),(P2_cluster,),(P2_flags,),\n",
    "\n",
    "    #+1 word attributes    \n",
    "    (N1_orth,),(N1_suffix,),(N1_cluster,),(N1_flags,),    \n",
    "\n",
    "    #+2 word attributes    \n",
    "    (N2_orth,),(N2_cluster,),(N2_flags,),\n",
    "\n",
    "    #combination attributes\n",
    "    (P1_lemma, P1_pos),(P2_lemma, P2_pos), (P1_pos, P2_pos),(P1_pos, W_orth)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a name=\"vocab\"></a>\n",
    "### Vocabulary, Tagger, and Statistical Model\n",
    "* The **Vocab** object will receive all the lexeme data (Brown clusters, word vectors, etc) from the English model.\n",
    "* The **Statistical Model**  will consume the features we defined, using them to make predictions.\n",
    "* The **Tagger** will consume our vocabulary object, and our statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: <function spacy.orth.is_alpha>,\n",
       " 2: <function spacy.orth.is_ascii>,\n",
       " 3: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 4: <function spacy.orth.is_lower>,\n",
       " 5: <function spacy.orth.is_punct>,\n",
       " 6: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 7: <function spacy.orth.is_title>,\n",
       " 8: <function spacy.orth.is_upper>,\n",
       " 9: <function spacy.orth.like_url>,\n",
       " 10: <function spacy.orth.like_number>,\n",
       " 11: <function spacy.orth.like_email>,\n",
       " 12: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 13: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 14: <function spacy.orth.is_bracket>,\n",
       " 15: <function spacy.orth.is_quote>,\n",
       " 16: <function spacy.orth.is_left_punct>,\n",
       " 17: <function spacy.orth.is_right_punct>,\n",
       " 66: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 67: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 68: <function spacy.orth.word_shape>,\n",
       " 69: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 70: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 72: <function spacy.language.BaseDefaults.<lambda>>,\n",
       " 81: <cyfunction load.<locals>.<lambda> at 0x7ff17781eea8>,\n",
       " 82: <function spacy.en.English.Defaults.<lambda>>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Defaults.lex_attr_getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "\n",
    "def get_lemmatizer():\n",
    "    return None\n",
    "\n",
    "def make_tagger(vocab, templates):\n",
    "    model = spacy.tagger.TaggerModel(templates)\n",
    "    return spacy.tagger.Tagger(vocab,model)\n",
    "\n",
    "def get_feature_extractors(nlp):\n",
    "    #return nlp.vocab.lex_attr_getters\n",
    "    return nlp.Defaults.lex_attr_getters\n",
    "\n",
    "#get requirements\n",
    "\n",
    "feature_extractors = get_feature_extractors(nlp)\n",
    "lemmatizer = get_lemmatizer()\n",
    "vocab = Vocab.load(modelpath, feature_extractors, lemmatizer, tagmap)\n",
    "statistical_model = spacy.tagger.TaggerModel(features)\n",
    "tagger = spacy.tagger.Tagger(vocab, statistical_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PUNCT'),\n",
       " ('can', 'PUNCT'),\n",
       " ('always', 'PUNCT'),\n",
       " ('learn', 'PUNCT'),\n",
       " ('more', 'PUNCT'),\n",
       " ('by', 'PUNCT'),\n",
       " ('reading', 'PUNCT')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The untrained tagger is awful...\n",
    "words = ['You','can','always','learn','more','by','reading']\n",
    "doc = Doc(vocab, words = words)\n",
    "tagger(doc)\n",
    "map_(lambda x: (x.orth_, x.pos_), doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#section4\">Back</a>\n",
    "<a name=\"training\"></a>\n",
    "### Training the Model\n",
    "\n",
    "##### Neuron Prediction:\n",
    "\n",
    "**Inputs:** \n",
    "\n",
    "$<x_1, x_2, ..., x_n>$\n",
    "\n",
    "**Each Neuron j:**\n",
    "\n",
    "$prediction_j  = \\bigg[\\sum_{d=1}^D w_dx_{dj} \\bigg] + b > 0$\n",
    "\n",
    "##### Perceptron Learning:\n",
    "```\n",
    "For each training epoch:\n",
    "    For each document, label:\n",
    "        prediction = weights * document + bias\n",
    "\n",
    "        if sign(label) != sign(prediction):\n",
    "            weights = weights + (label*features)\n",
    "            bias = bias + label\n",
    "shuffle data\n",
    "```\n",
    "\n",
    "### Exercise: \n",
    "Implement a perceptron algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "class PerceptronClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.iter = 0\n",
    "\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        \"\"\"Fits self.weights, self.biases \"\"\"\n",
    "        self.initialize(X)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for row, label in zip(X, y):\n",
    "                prediction = self.predict(row)\n",
    "                self.update(prediction, label, row)\n",
    "                self.iter += 1\n",
    "            X, y = shuffle(X, y) #important to reshuffle to avoid getting trapped\n",
    "            \n",
    "    def initialize(self, X):\n",
    "        self.bias = 0\n",
    "        self.weights = np.zeros(X.shape[1])#[:, np.newaxis]\n",
    "                    \n",
    "    def update(self, prediction, label, row):\n",
    "        \"\"\"Updates weights and biases based on the ground truth label\n",
    "        and the row\"\"\"\n",
    "        \n",
    "        if prediction*label <= 0:\n",
    "            update = label *  row            \n",
    "            self.weights += update\n",
    "            self.bias += label\n",
    "        \n",
    "    def predict_score(self, x):\n",
    "        \"\"\"Generates scores of \"x\". Uses self.weights and self.bias\"\"\"\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\" Converts prediction scores to 1s ands 0s.\"\"\"\n",
    "        predictions = np.where(self.predict_score(x) > 0, 1, -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "#generate some data\n",
    "def generate_data(dim=1, N=1000):\n",
    "    dim = 1\n",
    "    true_b = np.random.normal(0, 2)\n",
    "    X = np.random.normal(0, 10, size=(1000, dim))\n",
    "    true_w = np.random.normal(0, 2, size=(dim,))\n",
    "\n",
    "    labels = (np.dot(X, true_w) > true_b).astype(float)\n",
    "    labels[np.where(labels==0)] = -1\n",
    "    return X, labels, true_w, true_b\n",
    "\n",
    "X, labels, true_w, true_b = generate_data()\n",
    "b = PerceptronClassifier()\n",
    "b.fit(X, labels, epochs=100)\n",
    "\n",
    "acc_rate = (((b.predict(X) * labels) > 0).mean())\n",
    "print(\"Training Accuracy Rate: {}\".format(acc_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True bias: -0.7975384842656151\n",
      "True weights: [-1.05746448]\n",
      "Current x: [ 2.79105113]\n",
      "Current bias: 1.0\n",
      "Current weights: [-8.99100828]\n",
      "Current Score: [ 2.79105113] * [-8.99100828] + 1.0 = -24.094363852621186\n",
      "Current Prediction: -1\n",
      "Current True Label: -1\n",
      "Error: False\n",
      "Bias Update: 0\n",
      "Weights Update: 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8IAAAGtCAYAAADd1s0LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff141ac9b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Button, Textarea\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "class PerceptronAnimator(object):\n",
    "    def __init__(self, X, y, true_w, true_b):\n",
    "        self.y_min = -1.1 \n",
    "        self.y_max = 1.1        \n",
    "        self.row_i = 0\n",
    "        self.advance_button = Button(description=\"Advance\")\n",
    "        self.advance_button.on_click(self.advance)\n",
    "        self.reverse_botton = Button(description=\"Reverse\")\n",
    "        self.reverse_botton.on_click(self.reverse)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = PerceptronClassifier\n",
    "        display(self.advance_button)\n",
    "        display(self.reverse_botton)\n",
    "        self.true_b = true_b\n",
    "        self.true_w = true_w\n",
    "        self.thresholds = []\n",
    "        self.errors = []\n",
    "        self.advance(self.advance_button)\n",
    "\n",
    "        \n",
    "    def get_data(self, row_i):\n",
    "        return self.X[:row_i], self.y[:row_i]\n",
    "    \n",
    "    def get_model(self, X, y):\n",
    "        model = self.model()\n",
    "        model.fit(X, y, epochs=1)\n",
    "        return model\n",
    "    \n",
    "    def make_plots(self, ):\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        ax1.set_ylim(self.y_min, self.y_max)\n",
    "        ax1.set_xlabel(\"X\")\n",
    "        ax1.set_ylabel(\"Predict(X) / Y\")\n",
    "        ax2.set_ylabel(\"Steps\")\n",
    "        ax2.set_xlabel(\"Weight Adjustment\")        \n",
    "        return ax1, ax2\n",
    "    \n",
    "    def advance(self, b):\n",
    "        ax1, ax2 = self.make_plots()\n",
    "        next_x, next_y, ax1 = self.display_next(ax1)        \n",
    "        threshold, error, ax1 = self.display_state(next_x, next_y, ax1)\n",
    "        self.row_i += 1\n",
    "        self.thresholds.append(threshold)\n",
    "        self.display_thresholds(ax1)\n",
    "        self.errors.append(error)\n",
    "        self.plot_errors(ax2)\n",
    "        \n",
    "    def reverse(self, b):\n",
    "        ax1, ax2 = self.make_plots()\n",
    "        if self.row_i > 0:\n",
    "            self.row_i -= 1\n",
    "            x, y, ax1 = self.display_next(ax1)            \n",
    "            threshold, error, ax1 = self.display_state(x, y, ax1)\n",
    "             \n",
    "            self.thresholds = self.thresholds[:-1]\n",
    "            self.display_thresholds(ax)\n",
    "            self.errors = self.errors[:-1]\n",
    "            self.plot_errors(ax2)\n",
    "            \n",
    "    def plot_errors(self, ax):\n",
    "        \n",
    "        for i, err in enumerate(self.errors):\n",
    "            ax.barh(i, err)\n",
    "        ax.set_ylim(-1, len(self.errors) + 2)\n",
    "        ax.set_xlim(min(-1, min(self.errors)), max(1, max(self.errors)))\n",
    "        ax.axvline(0)\n",
    "    def display_thresholds(self, ax):\n",
    "        if len(self.thresholds) == 1:\n",
    "            ax.axvline(self.thresholds[0], color='green', label='current threshold value')\n",
    "        elif len(self.thresholds) > 1:\n",
    "            ax.axvline(self.thresholds[-1], color='green', label='current threshold value')            \n",
    "            ax.axvline(self.thresholds[-2], color='green', linestyle=\"--\", label='last threshold value')  \n",
    "        plt.legend()\n",
    "\n",
    "    def display_next(self, ax):\n",
    "        x, y = self.get_data(self.row_i +1)\n",
    "        model = self.get_model(x[:-1], y[:-1])\n",
    "        x, y = x[-1], y[-1]\n",
    "        correct = int(y) == int(model.predict(x))\n",
    "\n",
    "        #raise ValueError(\"\")\n",
    "        if correct:\n",
    "            ax.scatter(x, y, color='black', s=100.)\n",
    "        else:\n",
    "            ax.scatter(x, y, color='black', s=100., marker = 'X')\n",
    "        return x, y, ax\n",
    "        \n",
    "        \n",
    "    def display_state(self, next_x, next_y, ax):\n",
    "        \n",
    "        \n",
    "        clear_output()\n",
    "        X, Y = self.get_data(self.row_i)\n",
    "        model = self.get_model(X, Y)\n",
    "        threshold = self.find_threshold(model)\n",
    "        color_direction = self.pick_color_direction(model, threshold[-1])\n",
    "        ax.scatter(X, Y)\n",
    "        x_, y_ = self.make_line(ax, model)   \n",
    "        ax.plot(x_, y_)            \n",
    "        \n",
    "        error = self.update_text_box(model, next_x, next_y) \n",
    "        plt.legend()\n",
    "        ax = self.shade_plot(threshold[-1], ax, color_direction)\n",
    "        return threshold, error, ax\n",
    "    \n",
    "    def shade_plot(self, threshold, ax, color_direction):\n",
    "        y = range(int(self.y_min - 1), int(self.y_max + 1))\n",
    "        min_x = min(ax.get_xticks())\n",
    "        max_x = max(ax.get_xticks())\n",
    "        if color_direction: \n",
    "            ax.fill_betweenx(y, threshold, max_x, color='green', alpha=.2)\n",
    "            ax.fill_betweenx(y, min_x, threshold, color='red', alpha=.2)\n",
    "        else:\n",
    "            ax.fill_betweenx(y, threshold, max_x, color='red', alpha=.2)\n",
    "            ax.fill_betweenx(y, min_x, threshold, color='green', alpha=.2)            \n",
    "        return ax\n",
    "        \n",
    "        \n",
    "    def update_text_box(self, model, next_x, next_y):\n",
    "        next_pred = model.predict(next_x)\n",
    "        if next_pred == 0:\n",
    "            next_pred = -1\n",
    "        error = next_pred * next_y < 0\n",
    "        update_b = next_y if error else 0\n",
    "        update_w = next_y * next_x if error else 0\n",
    "        msg = \"True bias: {}\".format(self.true_b)\n",
    "        msg += \"\\n\"\n",
    "        msg +=\"True weights: {}\".format(self.true_w)\n",
    "        msg += \"\\n\"              \n",
    "        msg += \"Current x: {}\".format(next_x)\n",
    "        msg += \"\\n\"        \n",
    "        msg += \"Current bias: {}\".format(model.bias)\n",
    "        msg += \"\\n\"\n",
    "        msg +=\"Current weights: {}\".format(model.weights) \n",
    "        msg += \"\\n\"\n",
    "        msg +=\"Current Score: {0} * {1} + {2} = {3}\".format(next_x, model.weights, model.bias, model.predict_score(next_x)) \n",
    "        msg += \"\\n\"        \n",
    "        msg += \"Current Prediction: {}\".format(next_pred)        \n",
    "        msg += \"\\n\"\n",
    "        msg += \"Current True Label: {}\".format(int(next_y))        \n",
    "        msg += \"\\n\"        \n",
    "        msg += \"Error: {}\".format(error)\n",
    "        msg += \"\\n\"\n",
    "        msg += \"Bias Update: {}\".format(update_b)\n",
    "        msg += \"\\n\"        \n",
    "        msg += \"Weights Update: {}\".format(update_w)\n",
    "        msg += \"\\n\"        \n",
    "        print(msg)\n",
    "        return update_w\n",
    "        \n",
    "        \n",
    "    def make_line(self, ax, model):\n",
    "        x = [i / 10. for i in range(-300, 300)]\n",
    "        y = np.array([(model.bias + i * model.weights[0]) > 0 for i in x]).astype(float)\n",
    "        y[np.where(y==0)] = -1\n",
    "        return x, y\n",
    "    \n",
    "    def find_threshold(self, model):\n",
    "        x_range = np.array([i/10. for i in range(-300, 300)])[:, np.newaxis]\n",
    "        y = abs(model.predict_score(x_range))\n",
    "        \n",
    "        idx = abs(y).argmin()\n",
    "        return x_range[idx]\n",
    "    \n",
    "    def pick_color_direction(self, model, threshold):\n",
    "        if model.predict(threshold + 1) > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "X, labels, true_w, true_b = generate_data()\n",
    "p = PerceptronAnimator(X, labels, true_w, true_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What/how the model learns is sensitive to the ordering of the Xs. If X is multidimensional, and the last value is an outlier, our updates can be large, throwing the weights out of wack.\n",
    "\n",
    "Solution: averaging the weights over an epoch, so that weights **that usually work, and thus stick around** are used.\n",
    "\n",
    "##### Averaged Perceptron Learning:\n",
    "```\n",
    "list_of_weights, list_of_biases = list(), list()\n",
    "For each training epoch:\n",
    "    For each document, label:\n",
    "        prediction = weights * document + bias\n",
    "\n",
    "        if sign(label) != sign(prediction):\n",
    "            weights = weights + (label*features)\n",
    "            bias = bias + label\n",
    "        add weights to list_of_weights\n",
    "        add bias to list_of_biases\n",
    "shuffle data\n",
    "```\n",
    "\n",
    "##### Averaged Perceptron Prediction:\n",
    "```\n",
    "weights = average(list_of_weights)\n",
    "bias = average(list_of_bias)\n",
    "\n",
    "For each training epoch:\n",
    "    For each document, label:\n",
    "        prediction = weights * document + bias\n",
    "\n",
    "        if sign(label) != sign(prediction):\n",
    "            weights = weights + (label*features)\n",
    "            bias = bias + label\n",
    "        add weights to list_of_weights\n",
    "        add bias to list_of_biases\n",
    "shuffle data\n",
    "```\n",
    "\n",
    "\n",
    "##### Optional exercise:\n",
    "Modify your perceptron model to implement an **averaged perceptron** model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Training Accuracy: 0.0 Pre Test Accuracy 0.0\n",
      "Iteration 0 Training Accuracy: 94.6284744833259 Test Accuracy 93.4171597633136\n",
      "Iteration 1 Training Accuracy: 96.83783459574322 Test Accuracy 95.48816568047337\n",
      "Iteration 2 Training Accuracy: 98.09396099667546 Test Accuracy 96.5587044534413\n",
      "Iteration 3 Training Accuracy: 98.58107413373548 Test Accuracy 96.7650264715042\n",
      "Iteration 4 Training Accuracy: 99.12859444082667 Test Accuracy 97.38788539395827\n",
      "Iteration 5 Training Accuracy: 99.1024608424444 Test Accuracy 97.12706322018063\n",
      "Iteration 6 Training Accuracy: 99.62856016725503 Test Accuracy 97.71099345998131\n",
      "Iteration 7 Training Accuracy: 99.6088528635569 Test Accuracy 97.59810028028652\n",
      "Iteration 8 Training Accuracy: 99.43791342495801 Test Accuracy 97.38009965742759\n",
      "Iteration 9 Training Accuracy: 99.70096308736333 Test Accuracy 97.67595764559327\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "def predict_cycle(vocab, tagger, data, train=True):\n",
    "    \"\"\"For each document in data, creates a document and tags\n",
    "    it. Creates a goldparse object to hold the ground truth label.\n",
    "    If train=True, the tagger's statistical model is updated with the \n",
    "    result.\"\"\"\n",
    "    \n",
    "    scorer = Scorer()\n",
    "    \n",
    "    for words, tags in data:\n",
    "        #create a document, passing in words to become tokens\n",
    "        doc = Doc(vocab, words = words)\n",
    "        tagger(doc)\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "\n",
    "        scorer.score(doc, gold)              \n",
    "        \n",
    "        if train:\n",
    "            #train the model        \n",
    "            tagger.update(doc, gold)\n",
    "    return tagger, scorer\n",
    "\n",
    "def train(vocab, tagger, training_data, testing_data, epochs = 10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tagger (spacy.tagger.Tagger): \n",
    "        The tagger to train.\n",
    "        \n",
    "    training_data (list):\n",
    "        Training data containing words and annotated tags. \n",
    "        Should have form: [(word1, word2,...),(tag1, tag2, .....)]\n",
    "        \n",
    "    epochs (int):\n",
    "        number of training iterations\n",
    "        \n",
    "    verbose (Bool):\n",
    "        whether to track and print training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    tagger, pre_train_scorer = predict_cycle(vocab, tagger, training_data, train=False)\n",
    "    tagger, pre_test_scorer = predict_cycle(vocab, tagger, testing_data, train=False)\n",
    "    \n",
    "    print(\"Pre Training Accuracy: {0} Pre Test Accuracy {1}\".format(pre_train_scorer.tags_acc, \n",
    "                                                                    pre_test_scorer.tags_acc))\n",
    "    \n",
    "    for train_cycle in range(epochs):\n",
    "            \n",
    "        tagger, _ = predict_cycle(vocab, tagger, training_data, train=True)\n",
    "        \n",
    "        \n",
    "        tagger, train_scorer = predict_cycle(vocab, tagger, training_data, train=False)\n",
    "        tagger, test_scorer = predict_cycle(vocab, tagger, testing_data, train=False)\n",
    "\n",
    "        print(\"Iteration {0} Training Accuracy: {1} Test Accuracy {2}\".format(train_cycle, \n",
    "                                                                              train_scorer.tags_acc, \n",
    "                                                                              test_scorer.tags_acc))\n",
    "        #shuffle data    \n",
    "        np.random.shuffle(training_data)\n",
    "    \n",
    "    tagger.model.end_training()\n",
    "        \n",
    "    return tagger\n",
    "\n",
    "tagger = train(vocab, tagger, training_data, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"save\"></a>\n",
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tagger(tagger, model_dir):\n",
    "    if model_dir is not None:\n",
    "        tagger.model.dump(str(model_dir / 'pos' / 'model'))\n",
    "        with (model_dir / 'vocab' / 'strings.json').open('w') as file_:\n",
    "            tagger.vocab.strings.dump(file_)\n",
    "            \n",
    "save_tagger(tagger, modelpath)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the Averaged Perceptron Model:\n",
    "* Can only learn linear decision boundaries.\n",
    "    * Solution: Multi-layered perceptron, neural network.\n",
    "    * Coming in SpaCy 2.0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='qacode'></a>\n",
    "### Example Rule Based QA Component Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_answer_requirements(token):\n",
    "    if token.tag_ == 'WRB':\n",
    "        if token.lower_ == 'where':\n",
    "            #Where was Star Wars Filmed\n",
    "            return ['LOCATION']\n",
    "        elif token.lower_ == 'when':\n",
    "            #When was Star Wars Filmed\n",
    "            return ['DATE']\n",
    "        elif token.lower_ == 'how':\n",
    "            #How much did Star Wars make?\n",
    "            if token.nbor().lower_ in ('much', 'many'):\n",
    "                return ['QUANTITY']\n",
    "\n",
    "            #How old is star wars?\n",
    "            elif token.nbor().lower_ in ('long', 'old'):\n",
    "                return ['DURATION']\n",
    "            else:\n",
    "                return False\n",
    "        elif token.lower() == 'whom':\n",
    "            #Whom did you see?\n",
    "            return ['PERSON','ORG']      \n",
    "        else:\n",
    "            return False\n",
    "    elif token.tag_ == 'WP':\n",
    "        #Asking for Identity\n",
    "        if token.lower_ in ('who', 'whose'):\n",
    "            #Who directed Star Wars?\n",
    "            return ['PERSON','ORG']\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #What is Star Wars\n",
    "            return False \n",
    "        else: \n",
    "            return False\n",
    "    elif token.tag_ == 'WDT':\n",
    "        #asking for a choice among options\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #which Star Wars did you like best?\n",
    "            return [token.nbor().lower_] #return neighbor\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['LOCATION'], False, False, False, False, False]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_answer_requirements, nlp('Where was Star Wars filmed?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PERSON', 'ORG'], False, False, False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_answer_requirements, nlp('Who directed Star Wars?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['movie'], False, False, False, False, False]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_answer_requirements, nlp('Which movie was first released?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, ['country'], False, False, False, False, False, False]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_answer_requirements, nlp('In which country was Star Wars filmed?')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>\n",
    "<a name=\"wordsense\"></a>\n",
    "##### Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Synset('shower.n.01') a plumbing fixture that sprays water over you\n",
      "Synset('shower.n.02') washing yourself by standing upright under water sprayed from a nozzle\n",
      "Synset('shower.n.03') a brief period of precipitation\n",
      "Synset('shower.n.04') a sudden downpour (as of tears or sparks etc) likened to a rain shower\n",
      "Synset('exhibitor.n.01') someone who organizes an exhibit for others to see\n",
      "Synset('shower.n.06') a party of friends assembled to present gifts (usually of a specified kind) to a person\n",
      "Synset('lavish.v.01') expend profusely; also used with abstract nouns\n",
      "Synset('shower.v.02') spray or sprinkle with\n",
      "Synset('shower.v.03') take a shower; wash one's body in the shower\n",
      "Synset('shower.v.04') rain abundantly\n",
      "Synset('shower.v.05') provide abundantly with\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "for syn in wn.synsets('shower'):\n",
    "    print(syn, syn.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
