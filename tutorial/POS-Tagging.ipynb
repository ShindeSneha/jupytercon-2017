{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging: Agenda\n",
    "* <a href=\"#section1\">What are parts of speech? Why are they useful?</a>\n",
    "* <a href=\"#section2\">How do you use them with SpaCy?</a>\n",
    "* <a href=\"#section3\">How do we infer them?</a>\n",
    "* <a href=\"#section4\"> How do we learn them with SpaCy?</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_scratch():\n",
    "    !pip install spacy >> ~/spacy.log\n",
    "    !python -m spacy download en >> ~/spacy.log\n",
    "    !jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "\n",
    "    \n",
    "from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def as_list(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return list(f(*args, **kwargs))\n",
    "    \n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_(lambda x: x + 1, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.en import English\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "\n",
    "def as_list(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return list(f(*args, **kwargs))\n",
    "    \n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "map_ = as_list(map)\n",
    "filter_ = as_list(filter)\n",
    "zip_ = as_list(zip)\n",
    "\n",
    "def rep_sentences(texts):\n",
    "    html = []\n",
    "    for text in texts:\n",
    "        html.append(rep_sentence(text))\n",
    "    return HTML(\"\".join(html))\n",
    "\n",
    "def rep_sentence(text, display_pos = True):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'purple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    doc = nlp(text)\n",
    "    n_words = len(doc)\n",
    "    unique_pos = list(set(map(lambda x: x.pos_, doc)))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    html.append(\"<tr>\")            \n",
    "    for i in range(n_words):\n",
    "        word_string= doc[i].orth_\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "    html.append(\"</tr>\")\n",
    "    if display_pos:\n",
    "        html.append(\"<tr>\")            \n",
    "        for i in range(n_words):\n",
    "            pos = doc[i].pos_\n",
    "            color = pos_to_color[pos]\n",
    "            html.append(\"<td><span class='{0}'>{0}</span></td>\".format(pos))\n",
    "        html.append(\"</tr>\")\n",
    "    html = \"\".join(html)\n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "def custom_tag_table(list_of_word_tag_tuples):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'MediumPurple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    \n",
    "    n_words = len(list_of_word_tag_tuples)\n",
    "    words, pos_list = zip(*list_of_word_tag_tuples)\n",
    "    unique_pos = list(set([pos for pair in pos_list for pos in pair]))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    for i in range(n_words):\n",
    "        html.append(\"<tr>\")            \n",
    "        word_string= words[i]\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "        row = []\n",
    "        pos_sublist = pos_list[i]\n",
    "        for pos in pos_sublist:\n",
    "            entry = \"<span class='{0}'>{0}</span> \".format(pos)\n",
    "            #print entry\n",
    "            row.append(entry)\n",
    "        row = \"\".join(row)\n",
    "        html.append(\"<td>{}</td>\".format(row))\n",
    "        html.append(\"</tr>\")\n",
    "    return \"\".join(html)\n",
    "        \n",
    "    \n",
    "\n",
    "def nltk_corpus(corpus_name):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "#read nltk corpora\n",
    "def nltk_reader(corpus_name, limit = None):\n",
    "    corpus = nltk_corpus(corpus_name)\n",
    "    fileids = corpus.fileids()\n",
    "    \n",
    "    if limit:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n",
    "    else:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n",
    "    return doc_iter\n",
    "\n",
    "universal_tags = [\n",
    "     ['Open Class Words','ADJ','adjective']\n",
    "    ,['Open Class Words','ADV','adverb']\n",
    "    ,['Open Class Words','INTJ','interjection']\n",
    "    ,['Open Class Words','NOUN','noun']\n",
    "    ,['Open Class Words','PROPN','proper noun']\n",
    "    ,['Open Class Words','VERB','verb']\n",
    "    ,['Closed Class Words','ADP','adposition']\n",
    "    ,['Closed Class Words','AUX','auxiliary']\n",
    "    ,['Closed Class Words','CCONJ','coordination conjunction']\n",
    "    ,['Closed Class Words','DET','determiner']\n",
    "    ,['Closed Class Words','NUM','numeral']\n",
    "    ,['Closed Class Words','PART','particle']\n",
    "    ,['Closed Class Words','PRON','pronoun']\n",
    "    ,['Closed Class Words','SCONJ','subordinating conjection']\n",
    "    ,['Other','PUNCT','punctuation']\n",
    "    ,['Other','SYM','symbol']\n",
    "    ,['Other','X','other']\n",
    "]\n",
    "tag_table = pd.DataFrame(universal_tags, columns = ['Category','Abbrev','Part of Speech'])\n",
    "tag_table = tag_table.set_index(['Category','Abbrev'])\n",
    "\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section1\"></a>\n",
    "\n",
    "### What are Parts of Speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Part of Speech</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Abbrev</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Open Class Words</th>\n",
       "      <th>ADJ</th>\n",
       "      <td>adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>interjection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>proper noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">Closed Class Words</th>\n",
       "      <th>ADP</th>\n",
       "      <td>adposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>auxiliary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>coordination conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>numeral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>particle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>subordinating conjection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Other</th>\n",
       "      <th>PUNCT</th>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>symbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Part of Speech\n",
       "Category           Abbrev                          \n",
       "Open Class Words   ADJ                    adjective\n",
       "                   ADV                       adverb\n",
       "                   INTJ                interjection\n",
       "                   NOUN                        noun\n",
       "                   PROPN                proper noun\n",
       "                   VERB                        verb\n",
       "Closed Class Words ADP                   adposition\n",
       "                   AUX                    auxiliary\n",
       "                   CCONJ   coordination conjunction\n",
       "                   DET                   determiner\n",
       "                   NUM                      numeral\n",
       "                   PART                    particle\n",
       "                   PRON                     pronoun\n",
       "                   SCONJ   subordinating conjection\n",
       "Other              PUNCT                punctuation\n",
       "                   SYM                       symbol\n",
       "                   X                          other"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.VERB{background-color:SkyBlue;}</style><style>.DET{background-color:red;}</style><style>.NOUN{background-color:YellowGreen;}</style><style>.PRON{background-color:yellow;}</style><style>.PUNCT{background-color:orange;}</style><style>.ADP{background-color:pink;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>get</span></td><td><span class='word'>a</span></td><td><span class='word'>discount</span></td><td><span class='word'>on</span></td><td><span class='word'>newspapers</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr><table width=100%><style>.word{font-weight:bold;}</style><style>.PUNCT{background-color:SkyBlue;}</style><style>.PRON{background-color:red;}</style><style>.VERB{background-color:YellowGreen;}</style><style>.ADP{background-color:yellow;}</style><style>.NOUN{background-color:orange;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>discount</span></td><td><span class='word'>that</span></td><td><span class='word'>newspaper</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'I get a discount on newspapers.'\n",
    "sentence2 = 'I discount that newspaper.'\n",
    "\n",
    "rep_sentences([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='applications'></a>\n",
    "### Applications\n",
    "* Rule based systems:\n",
    "    * <a href=\"#qacode\">Example of rule based question answering component</a>\n",
    "* Feature engineering for statistical models\n",
    "    * <a href=\"#wordsense\">Feature for word disambiguation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section2\"></a>\n",
    "### Parts of Speech with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos (coarse)</th>\n",
       "      <th>pos (fine)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount</th>\n",
       "      <td>discount</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>get</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newspapers</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lemma pos (coarse) pos (fine)\n",
       "I              -PRON-         PRON        PRP\n",
       "a                   a          DET         DT\n",
       "discount     discount         NOUN         NN\n",
       "get               get         VERB        VBP\n",
       "newspapers  newspaper         NOUN        NNS\n",
       "on                 on          ADP         IN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Accessing\n",
    "doc = nlp('I get a discount on newspapers')\n",
    "tags = {}\n",
    "\n",
    "for word in doc:\n",
    "    tags[word.orth_] = {'lemma': word.lemma_, \n",
    "                        'pos (coarse)': word.pos_, \n",
    "                        'pos (fine)':word.tag_}\n",
    "pd.DataFrame(tags).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Building word vectors that are Part of Speech specific\n",
    "Steps:\n",
    "* get documents\n",
    "* tokenize the documents, and append the part of speech to each token, e.g. dog|NOUN\n",
    "* train a word2vec model with gensim\n",
    "* compare the most similar words of 'back||||VERB' vs 'back||||NOUN' (or other combo)\n",
    "\n",
    "* Hints:\n",
    "    * model.wv.vocab contains the vocabulary.\n",
    "    * using a completely unique join character will make it easier to split later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def return_documents():\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    dataset = fetch_20newsgroups()\n",
    "    corpus = dataset.data\n",
    "    return corpus\n",
    "\n",
    "def tokenize_and_tag_documents(documents, nlp, sep_char=\"|\"):\n",
    "    pass\n",
    "\n",
    "def build_model(tokenized_docs):\n",
    "    pass\n",
    "\n",
    "def compare_most_similar_words_across_pos(word):\n",
    "    pass\n",
    "\n",
    "documents = return_documents()\n",
    "tokenized_and_tagged_documents = tokenize_and_tag_documents(documents, nlp)\n",
    "model = build_model(tokenized_and_tagged_documents)\n",
    "compare_most_similar_words_across_pos('back')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section3\"></a>\n",
    "### How do we infer parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.VERB{background-color:SkyBlue;}</style><style>.PROPN{background-color:red;}</style><style>.DET{background-color:YellowGreen;}</style><style>.ADJ{background-color:yellow;}</style><style>.NOUN{background-color:orange;}</style><style>.PRON{background-color:pink;}</style><style>.PUNCT{background-color:brown;}</style><style>.ADP{background-color:purple;}</style><style>.PART{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'>loble</span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'>effix</span></td><td><span class='word'>by</span></td><td><span class='word'>klepping</span></td><td><span class='word'>the</span></td><td><span class='word'>Dongle</span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Button\n",
    "class reveal(object):\n",
    "    def __init__(self):\n",
    "        self.text = 'I was loble to find the effix by klepping the Dongle search engine.'\n",
    "        self.toggle = Button(description='Toggle POS', )\n",
    "        self.toggle.on_click(self.toggle_pos)\n",
    "        self.state = False\n",
    "        display(self.toggle)\n",
    "        self.display()\n",
    "        \n",
    "    def toggle_pos(self, b):\n",
    "        self.state = not self.state\n",
    "        self.display()\n",
    "        \n",
    "    def display(self):\n",
    "        clear_output()\n",
    "        display(HTML(rep_sentence(self.text, display_pos = self.state)))\n",
    "        \n",
    "r = reveal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants of Part of Speech:\n",
    "* Word: some words can only be used in a single way; we can memorize these.\n",
    "* Word shape: if the first letter is capitalized, its likely a proper noun.\n",
    "* Neighboring part of speech: there are common patterns, such as noun phrases commonly following a determiner. to the beach\n",
    "\n",
    "\n",
    "\n",
    "| Feature | Notes | Example|\n",
    "|------|------|------|\n",
    "|   Word Identity  | Some words can only be used in a single way; we can memorize these.| \"the\" -> determiner| \n",
    "| Word Shape|Capitalization, dashes,  |\"I stayed at the Park Hotel.\"|\n",
    "|Neighboring parts of speech|There are common patterns what tags can neighbor others|\"to the beach\" (noun following determiner)|\n",
    "|Morphological Structures|Word prefixes and suffixes can rule out certain tag types|\"-ly\" -> adverb|\n",
    "|Syntactic Dependencies|Syntax may establish expectations that only certain tags can logically fill|\"I was told __\" -> adpositional phrase or object entity|\n",
    "|?|?|?|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section4\"></a>\n",
    "### Training your own tagger\n",
    "\n",
    "**Steps** : \n",
    "* load <a href=\"#load_data\">**training data**</a>, where each observation is represented as (list_of_words, list_of_tags)\n",
    "* Look at your dataset. Build a **tag map** mapping from the part of speech tags to the <a href=\"#universaltagset\">universal tagset</a>.\n",
    "* Decide which **features** to use\n",
    "    * e.g. pos of previous word, identity of current word, etc...\n",
    "    * spacy.tagger.N1_cluster, spacy.tagger.N1_pos, etc...\n",
    "    * which word?\n",
    "        * N1: Next\n",
    "        * N0: Current\n",
    "        * P1: Previous\n",
    "        * etc...\n",
    "    * which attribute:\n",
    "        * prefix\n",
    "        * tag\n",
    "        * cluster\n",
    "        * etc...\n",
    "* create a **vocabulary** object that consumes: \n",
    "    * the tagmap, the features we want to use, and a path from which we can load lexeme data (default english)\n",
    "    * `spacy.vocab.Vocab`\n",
    "* create a **statistical model** that consumes the features we want to use\n",
    "    * `spacy.tagger.TaggerModel`\n",
    "* create a **tagger** that consumes our vocabulary and our model\n",
    "    * `spacy.tagger.Tagger`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"load_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load conll2000 corpus\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "def nltk_corpus(corpus_name):\n",
    "    '''returns nltk corpus by name. if not loaded, download.'''\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    if x == '-LCB-':\n",
    "        return '{'\n",
    "    elif x=='-RCB-':\n",
    "        return '}'\n",
    "    elif x == '-RRB-':\n",
    "        return \")\"\n",
    "    elif x == '-LRB-':\n",
    "        return \"(\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def conll_to_data():\n",
    "    corpus = nltk_corpus('conll2000')\n",
    "    all_data= map_(lambda x: [(clean(i[0]), i[1]) for i in x], corpus.iob_sents())\n",
    "    all_data = [zip_(*i) for i in all_data]\n",
    "    return all_data\n",
    "\n",
    "c2000 = conll_to_data()\n",
    "training_data, testing_data = train_test_split(c2000, test_size = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "modelpath = PosixPath('/home/jupyter/mymodel')\n",
    "if not modelpath.exists():\n",
    "    modelpath.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tagger import *\n",
    "from spacy.language_data import TAG_MAP\n",
    "import os\n",
    "from pathlib import PosixPath\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import PosixPath\n",
    "modelpath = PosixPath('/home/jupyter/mymodel')\n",
    "if not modelpath.exists():\n",
    "    modelpath.mkdir()\n",
    "nlp.save_to_directory(modelpath)\n",
    "\n",
    "features = [\n",
    "    #current word attributes\n",
    "    (W_orth,),(W_shape,),(W_cluster,),(W_flags,),(W_suffix,),(W_prefix,),\n",
    "\n",
    "    #-1 word attributes    \n",
    "    (P1_pos,),(P1_cluster,),(P1_flags,),(P1_suffix,),\n",
    "\n",
    "    #-2 word attributes     \n",
    "    (P2_pos,),(P2_cluster,),(P2_flags,),\n",
    "\n",
    "    #+1 word attributes    \n",
    "    (N1_orth,),(N1_suffix,),(N1_cluster,),(N1_flags,),    \n",
    "\n",
    "    #+2 word attributes    \n",
    "    (N2_orth,),(N2_cluster,),(N2_flags,),\n",
    "\n",
    "    #combination attributes\n",
    "    (P1_lemma, P1_pos),(P2_lemma, P2_pos), (P1_pos, P2_pos),(P1_pos, W_orth)\n",
    "]\n",
    "\n",
    "def adjust_tagmap(tagmap):\n",
    "    tagmap['('] = tagmap['-LRB-']\n",
    "    tagmap[')'] = tagmap['-RRB-']\n",
    "    tagmap['{'] = tagmap['-LRB-']\n",
    "    tagmap['}'] = tagmap['-RRB-']\n",
    "    tagmap['$'] = {POS: PUNCT}\n",
    "    return tagmap\n",
    "\n",
    "\n",
    "def get_lemmatizer():\n",
    "    return None\n",
    "\n",
    "def make_tagger(vocab, templates):\n",
    "    model = spacy.tagger.TaggerModel(templates)\n",
    "    return spacy.tagger.Tagger(vocab,model)\n",
    "\n",
    "def get_feature_extractors():\n",
    "    #return nlp.vocab.lex_attr_getters\n",
    "    return Language.Defaults.lex_attr_getters\n",
    "\n",
    "#get requirements\n",
    "\n",
    "tagmap = adjust_tagmap(TAG_MAP)\n",
    "feature_extractors = get_feature_extractors()\n",
    "lemmatizer = get_lemmatizer()\n",
    "vocab = Vocab.load(MODELPATH, feature_extractors, lemmatizer, tagmap)\n",
    "statistical_model = spacy.tagger.TaggerModel(features)\n",
    "tagger = spacy.tagger.Tagger(vocab, statistical_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PUNCT'),\n",
       " ('can', 'PUNCT'),\n",
       " ('always', 'PUNCT'),\n",
       " ('learn', 'PUNCT'),\n",
       " ('more', 'PUNCT'),\n",
       " ('by', 'PUNCT'),\n",
       " ('reading', 'PUNCT')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The untrained tagger is awful...\n",
    "words = ['You','can','always','learn','more','by','reading']\n",
    "doc = Doc(vocab, words = words)\n",
    "tagger(doc)\n",
    "map_(lambda x: (x.orth_, x.pos_), doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Training Accuracy: 0.0 Pre Test Accuracy 0.0\n",
      "Iteration 0 Training Accuracy: 91.4423175789149 Test Accuracy 96.0234088494362\n",
      "Iteration 1 Training Accuracy: 96.69474243410906 Test Accuracy 96.83183672070467\n",
      "Iteration 2 Training Accuracy: 97.8249134592316 Test Accuracy 98.43841039174693\n",
      "Iteration 3 Training Accuracy: 98.43969565068376 Test Accuracy 98.75329883127121\n",
      "Iteration 4 Training Accuracy: 98.84669431401446 Test Accuracy 99.38136203173732\n",
      "Iteration 5 Training Accuracy: 99.118312369332 Test Accuracy 99.46576070192275\n",
      "Iteration 6 Training Accuracy: 99.29139390615896 Test Accuracy 99.45119443397196\n",
      "Iteration 7 Training Accuracy: 99.47133015731569 Test Accuracy 99.1907152894403\n",
      "Iteration 8 Training Accuracy: 99.56044144360284 Test Accuracy 99.19542790554203\n",
      "Iteration 9 Training Accuracy: 99.60928128320252 Test Accuracy 99.76522603420503\n",
      "Iteration 10 Training Accuracy: 99.69753573019844 Test Accuracy 99.74808924838058\n",
      "Iteration 11 Training Accuracy: 99.7566576412928 Test Accuracy 99.79264489152413\n",
      "Iteration 12 Training Accuracy: 99.7793638825102 Test Accuracy 99.86633307056928\n",
      "Iteration 13 Training Accuracy: 99.82391952565376 Test Accuracy 99.86076361517634\n",
      "Iteration 14 Training Accuracy: 99.82734688281866 Test Accuracy 99.91774342804264\n",
      "Iteration 15 Training Accuracy: 99.83762895431333 Test Accuracy 99.80763957912055\n",
      "Iteration 16 Training Accuracy: 99.85005312403605 Test Accuracy 99.80378380231004\n",
      "Iteration 17 Training Accuracy: 99.86633307056928 Test Accuracy 99.9391644103232\n",
      "Iteration 18 Training Accuracy: 99.90960345477602 Test Accuracy 99.87747198135519\n",
      "Iteration 19 Training Accuracy: 99.90360557973746 Test Accuracy 99.9207423655619\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "\n",
    "def predict_cycle(vocab, tagger, data, train=True):\n",
    "    scorer = Scorer()\n",
    "    \n",
    "    for words, tags in training_data:\n",
    "        #create a document, passing in words to become tokens\n",
    "        doc = Doc(vocab, words = words)\n",
    "        tagger(doc)\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "\n",
    "        \n",
    "        scorer.score(doc, gold)              \n",
    "        \n",
    "        if train:\n",
    "            #train the model        \n",
    "            tagger.update(doc, gold)\n",
    "    return tagger, scorer\n",
    "\n",
    "def train(vocab, tagger, training_data, testing_data, epochs = 10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tagger (spacy.tagger.Tagger): \n",
    "        The tagger to train.\n",
    "        \n",
    "    training_data (list):\n",
    "        Training data containing words and annotated tags. \n",
    "        Should have form: [(word1, word2,...),(tag1, tag2, .....)]\n",
    "        \n",
    "    epochs (int):\n",
    "        number of training iterations\n",
    "        \n",
    "    verbose (Bool):\n",
    "        whether to track and print training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    tagger, pre_train_scorer = predict_cycle(vocab, tagger, training_data, train=False)\n",
    "    tagger, pre_test_scorer = predict_cycle(vocab, tagger, testing_data, train=False)\n",
    "    print(\"Pre Training Accuracy: {0} Pre Test Accuracy {1}\".format(pre_train_scorer.tags_acc, \n",
    "                                                                    pre_test_scorer.tags_acc))\n",
    "    \n",
    "    for train_cycle in range(epochs):\n",
    "            \n",
    "        tagger, train_scorer = predict_cycle(vocab, tagger, training_data, train=True)\n",
    "        tagger, test_scorer = predict_cycle(vocab, tagger, testing_data, train=False)\n",
    "\n",
    "        print(\"Iteration {0} Training Accuracy: {1} Test Accuracy {2}\".format(train_cycle, \n",
    "                                                                              train_scorer.tags_acc, \n",
    "                                                                              test_scorer.tags_acc))\n",
    "        #shuffle data    \n",
    "        np.random.shuffle(training_data)\n",
    "    \n",
    "    tagger.model.end_training()\n",
    "        \n",
    "    return tagger\n",
    "\n",
    "tagger = train(vocab, tagger, training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tagger(tagger, model_dir):\n",
    "    if model_dir is not None:\n",
    "        tagger.model.dump(str(model_dir / 'pos' / 'model'))\n",
    "        with (model_dir / 'vocab' / 'strings.json').open('w') as file_:\n",
    "            tagger.vocab.strings.dump(file_)\n",
    "            \n",
    "save_tagger(tagger, modelpath)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='qacode'></a>\n",
    "### Example Rule Based QA Component Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_answer_requirements(token):\n",
    "    if token.tag_ == 'WRB':\n",
    "        if token.lower_ == 'where':\n",
    "            #Where was Star Wars Filmed\n",
    "            return ['LOCATION']\n",
    "        elif token.lower_ == 'when':\n",
    "            #When was Star Wars Filmed\n",
    "            return ['DATE']\n",
    "        elif token.lower_ == 'how':\n",
    "            #How much did Star Wars make?\n",
    "            if token.nbor().lower_ in ('much', 'many'):\n",
    "                return ['QUANTITY']\n",
    "\n",
    "            #How old is star wars?\n",
    "            elif token.nbor().lower_ in ('long', 'old'):\n",
    "                return ['DURATION']\n",
    "            else:\n",
    "                return False\n",
    "        elif token.lower() == 'whom':\n",
    "            #Whom did you see?\n",
    "            return ['PERSON','ORG']      \n",
    "        else:\n",
    "            return False\n",
    "    elif token.tag_ == 'WP':\n",
    "        #Asking for Identity\n",
    "        if token.lower_ in ('who', 'whose'):\n",
    "            #Who directed Star Wars?\n",
    "            return ['PERSON','ORG']\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #What is Star Wars\n",
    "            return False \n",
    "        else: \n",
    "            return False\n",
    "    elif token.tag_ == 'WDT':\n",
    "        #asking for a choice among options\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #which Star Wars did you like best?\n",
    "            return [token.nbor().lower_] #return neighbor\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>\n",
    "<a name=\"wordsense\"></a>\n",
    "##### Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Synset('shower.n.01') a plumbing fixture that sprays water over you\n",
      "Synset('shower.n.02') washing yourself by standing upright under water sprayed from a nozzle\n",
      "Synset('shower.n.03') a brief period of precipitation\n",
      "Synset('shower.n.04') a sudden downpour (as of tears or sparks etc) likened to a rain shower\n",
      "Synset('exhibitor.n.01') someone who organizes an exhibit for others to see\n",
      "Synset('shower.n.06') a party of friends assembled to present gifts (usually of a specified kind) to a person\n",
      "Synset('lavish.v.01') expend profusely; also used with abstract nouns\n",
      "Synset('shower.v.02') spray or sprinkle with\n",
      "Synset('shower.v.03') take a shower; wash one's body in the shower\n",
      "Synset('shower.v.04') rain abundantly\n",
      "Synset('shower.v.05') provide abundantly with\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "for syn in wn.synsets('shower'):\n",
    "    print(syn, syn.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
