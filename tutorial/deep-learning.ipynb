{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section:\n",
    "* Thoughts on word embeddings as a means of representing text\n",
    "* Integrating your embeddings into SpaCy\n",
    "* Thoughts on LSTMs for classification\n",
    "* Integrating your classifier into SpaCy\n",
    "* Visualizing your embeddings[if time permits]\n",
    "* Visualizing your model[if time permits]\n",
    "\n",
    "\n",
    "### How to represent words for NLP\n",
    "* motivator on why bag of words/ one hot encoding is bad\n",
    "    * curse of dimensionality, sparsity, ignores context, new words, etc.\n",
    "* Word vectors\n",
    "    * distributional hypothesis \n",
    "        * describing the landscape of models as using different types of \"context\"   \n",
    "        * count and predictive approachs [<a href=\"#note1\">note.</a>]\n",
    "        * larger context: semantic relatedness (e.g. “boat” – “water”)\n",
    "        * smaller context: semantic similarity (e.g. “boat” – “ship”)\n",
    "    * quick overview on methods\n",
    "    * SVD on doc/word matrices\n",
    "    * SVD on co-occurance matrices with window\n",
    "    \n",
    "    * some issues:\n",
    "        * large matrices!\n",
    "        * expensive to SVD (quadratic time)\n",
    "        * Sparse\n",
    "    * Glove\n",
    "    * word2vec: make word vectors the parameters of a model with the objective of defining local context.\n",
    "    * go over word2vec in a little more detail\n",
    "        * skip gram\n",
    "        * cbow\n",
    "        * negative sampling\n",
    "    * word embeddings in python:\n",
    "        * sklearn/pydsm + numpy (vectorizers + matrix decompositions)\n",
    "        * gensim (word2vec)\n",
    "* Neural models \n",
    "\n",
    "        \n",
    "        \n",
    "* Inspecting results of word embeddings:\n",
    "    * self organizing maps\n",
    "* Validating word vectors:\n",
    "    * intrinsic vs extrinsic\n",
    "    \n",
    "* A note on NNs:\n",
    "    * transferable features in shallow parts of a network, theres an analogy their with word2vec (shallow networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://localhost:6006 width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=http://localhost:6006 width=700 height=350></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "b'<!doctype html>\\n<!--\\n@license\\nCopyright 2016 The TensorFlow Authors. All Rights Reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n<html>\\n  <head>\\n    <title>TensorBoard</title>\\n    <script src=\"webcomponentsjs/webcomponents-lite.min.js\"></script>\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"lib/css/global.css\">\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"plottable/plottable.css\">\\n    <link rel=\"shortcut icon\" href=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMQAAADECAMAAAD3eH5ZAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAD/UExURfFlKfaELvFmKfNyK/67NvWALf68Nv69NvNxK/20NfyyNP22NfN0K/JrKvqhMv2zNf25Nf24Nf23NfeOL/yzNPyvNPJoKviWMPmeMfN1K/WBLfePL/FnKfeML/qlMvR7LPmcMfeLL/aJLvR5LPFoKfJuKvR3LP66NvywNPeNL/V/LfaILv21Nf26NfNzK/NvK/R6LPmaMfyxNPqfMvV+LfurM/iSMPmbMfJvKvmdMfumM/qiMvmZMfytNPJqKvysNPN2K/iYMPNwK/upM/JtKvJsKviVMPaHLvaGLvJpKvR8LPaKLvqkMvuqM/aFLvR4LPuoM/iTMPWDLfiRMPmYMXS0ngkAAALoSURBVHja7drnctpAFIbhFUISSKJ3MKYa0+y4xTW9937/15JkJhlTjhrSrHRmvuf/as6L0YLFCgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBJ6njenqspzgnPrsrGX9Zpi2tCrmnc6+dYNthVY5WpMmxQLWPdMsOuYVwzNj3ei2t3mQwaV43BJPDCS2NbJ5aEeuX/+9qcjQOtfFIkIkrvY2g4MVcmOBsFWbowKO/kNyj62gRpJcDaPBlxLr1B0zdG0C/8LzbJiJrshuvy1gzlA9+rD8mIkuyIJjFE3/dqnYwoSm7IUEPoD/wut8iIguSIDjlFxe/yfXL5vuSI21BTZLLhXoOILMO8Hxwa/L8bI0LfmUdhGowb2ZvT0e57pFNDgB06IlVyjmmIBl2T/nl9Rw6SD9GgSG/Q0uQkaW3XhmovKQ3eFQ4N2Uo9OQ1eFZsNerf7vP+rO4rhmY1Lg3vFVoP8+8BXg1sFnwbnCk4NThW8GuiKBDdkVVtTNFvNelVsNqTbyWnIOM2oeTRoyWvwmpJHg/ucXBrcJuXT4DwrpwZi2vy0VCx8YtXg/D2bU4OfiuQ3eFfE2KD4bfCqiLNB993gXsGlwa2CT4NzBacGIVQ6YsipQdh0xEdODUKjIxrSp88onZ8zbbFLg1DoiFO5BXvDGv2My9/JhUT8JUZTI0yDaNHLBzIbvqTDNYhUiVw/kdjQ1kM2CHFDPjKW+KzyRTF0g/ga9w9y+fANQpxvX8CU+Ny7FUWDeF3Y+g3lROIf4k0UDX9eCyvO531PyYhHga9zvPZJU5b73Y/eXj8Hv9D48n6HaF5LbcjRt8TZTtda5M1DfXnbkX1C0SHCFKzQB5Fe8op4GNGNHavvZESbVwT5r6W1xyuCPBY3Y9YgDqzknH/e3YfNzzuL30l0IebrZ5kKtuDIXt1n868ET6kf3/49tLvrCcZyF8Pu215dAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcPIbNrBhOaBXucoAAAAASUVORK5CYII=\">\\n    <link rel=\"import\" href=\"dist/bazel-html-imports.html\">\\n    <link rel=\"import\" href=\"dist/tf-tensorboard.html\">\\n  </head>\\n  <body>\\n    <tf-tensorboard use-hash></tf-tensorboard>\\n  </body>\\n</html>\\n'"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html=requests.get('http://localhost:6006').content\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(str(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!doctype html>\\n<!--\\n@license\\nCopyright 2016 The TensorFlow Authors. All Rights Reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n<html>\\n  <head>\\n    <title>TensorBoard</title>\\n    <script src=\"webcomponentsjs/webcomponents-lite.min.js\"></script>\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"lib/css/global.css\">\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"plottable/plottable.css\">\\n    <link rel=\"shortcut icon\" href=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMQAAADECAMAAAD3eH5ZAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAD/UExURfFlKfaELvFmKfNyK/67NvWALf68Nv69NvNxK/20NfyyNP22NfN0K/JrKvqhMv2zNf25Nf24Nf23NfeOL/yzNPyvNPJoKviWMPmeMfN1K/WBLfePL/FnKfeML/qlMvR7LPmcMfeLL/aJLvR5LPFoKfJuKvR3LP66NvywNPeNL/V/LfaILv21Nf26NfNzK/NvK/R6LPmaMfyxNPqfMvV+LfurM/iSMPmbMfJvKvmdMfumM/qiMvmZMfytNPJqKvysNPN2K/iYMPNwK/upM/JtKvJsKviVMPaHLvaGLvJpKvR8LPaKLvqkMvuqM/aFLvR4LPuoM/iTMPWDLfiRMPmYMXS0ngkAAALoSURBVHja7drnctpAFIbhFUISSKJ3MKYa0+y4xTW9937/15JkJhlTjhrSrHRmvuf/as6L0YLFCgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBJ6njenqspzgnPrsrGX9Zpi2tCrmnc6+dYNthVY5WpMmxQLWPdMsOuYVwzNj3ei2t3mQwaV43BJPDCS2NbJ5aEeuX/+9qcjQOtfFIkIkrvY2g4MVcmOBsFWbowKO/kNyj62gRpJcDaPBlxLr1B0zdG0C/8LzbJiJrshuvy1gzlA9+rD8mIkuyIJjFE3/dqnYwoSm7IUEPoD/wut8iIguSIDjlFxe/yfXL5vuSI21BTZLLhXoOILMO8Hxwa/L8bI0LfmUdhGowb2ZvT0e57pFNDgB06IlVyjmmIBl2T/nl9Rw6SD9GgSG/Q0uQkaW3XhmovKQ3eFQ4N2Uo9OQ1eFZsNerf7vP+rO4rhmY1Lg3vFVoP8+8BXg1sFnwbnCk4NThW8GuiKBDdkVVtTNFvNelVsNqTbyWnIOM2oeTRoyWvwmpJHg/ucXBrcJuXT4DwrpwZi2vy0VCx8YtXg/D2bU4OfiuQ3eFfE2KD4bfCqiLNB993gXsGlwa2CT4NzBacGIVQ6YsipQdh0xEdODUKjIxrSp88onZ8zbbFLg1DoiFO5BXvDGv2My9/JhUT8JUZTI0yDaNHLBzIbvqTDNYhUiVw/kdjQ1kM2CHFDPjKW+KzyRTF0g/ga9w9y+fANQpxvX8CU+Ny7FUWDeF3Y+g3lROIf4k0UDX9eCyvO531PyYhHga9zvPZJU5b73Y/eXj8Hv9D48n6HaF5LbcjRt8TZTtda5M1DfXnbkX1C0SHCFKzQB5Fe8op4GNGNHavvZESbVwT5r6W1xyuCPBY3Y9YgDqzknH/e3YfNzzuL30l0IebrZ5kKtuDIXt1n868ET6kf3/49tLvrCcZyF8Pu215dAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcPIbNrBhOaBXucoAAAAASUVORK5CYII=\">\\n    <link rel=\"import\" href=\"dist/bazel-html-imports.html\">\\n    <link rel=\"import\" href=\"dist/tf-tensorboard.html\">\\n  </head>\\n  <body>\\n    <tf-tensorboard use-hash></tf-tensorboard>\\n  </body>\\n</html>\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "b'<!doctype html>\\n<!--\\n@license\\nCopyright 2016 The TensorFlow Authors. All Rights Reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n<html>\\n  <head>\\n    <title>TensorBoard</title>\\n    <script src=\"webcomponentsjs/webcomponents-lite.min.js\"></script>\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"lib/css/global.css\">\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"plottable/plottable.css\">\\n    <link rel=\"shortcut icon\" href=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMQAAADECAMAAAD3eH5ZAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAD/UExURfFlKfaELvFmKfNyK/67NvWALf68Nv69NvNxK/20NfyyNP22NfN0K/JrKvqhMv2zNf25Nf24Nf23NfeOL/yzNPyvNPJoKviWMPmeMfN1K/WBLfePL/FnKfeML/qlMvR7LPmcMfeLL/aJLvR5LPFoKfJuKvR3LP66NvywNPeNL/V/LfaILv21Nf26NfNzK/NvK/R6LPmaMfyxNPqfMvV+LfurM/iSMPmbMfJvKvmdMfumM/qiMvmZMfytNPJqKvysNPN2K/iYMPNwK/upM/JtKvJsKviVMPaHLvaGLvJpKvR8LPaKLvqkMvuqM/aFLvR4LPuoM/iTMPWDLfiRMPmYMXS0ngkAAALoSURBVHja7drnctpAFIbhFUISSKJ3MKYa0+y4xTW9937/15JkJhlTjhrSrHRmvuf/as6L0YLFCgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBJ6njenqspzgnPrsrGX9Zpi2tCrmnc6+dYNthVY5WpMmxQLWPdMsOuYVwzNj3ei2t3mQwaV43BJPDCS2NbJ5aEeuX/+9qcjQOtfFIkIkrvY2g4MVcmOBsFWbowKO/kNyj62gRpJcDaPBlxLr1B0zdG0C/8LzbJiJrshuvy1gzlA9+rD8mIkuyIJjFE3/dqnYwoSm7IUEPoD/wut8iIguSIDjlFxe/yfXL5vuSI21BTZLLhXoOILMO8Hxwa/L8bI0LfmUdhGowb2ZvT0e57pFNDgB06IlVyjmmIBl2T/nl9Rw6SD9GgSG/Q0uQkaW3XhmovKQ3eFQ4N2Uo9OQ1eFZsNerf7vP+rO4rhmY1Lg3vFVoP8+8BXg1sFnwbnCk4NThW8GuiKBDdkVVtTNFvNelVsNqTbyWnIOM2oeTRoyWvwmpJHg/ucXBrcJuXT4DwrpwZi2vy0VCx8YtXg/D2bU4OfiuQ3eFfE2KD4bfCqiLNB993gXsGlwa2CT4NzBacGIVQ6YsipQdh0xEdODUKjIxrSp88onZ8zbbFLg1DoiFO5BXvDGv2My9/JhUT8JUZTI0yDaNHLBzIbvqTDNYhUiVw/kdjQ1kM2CHFDPjKW+KzyRTF0g/ga9w9y+fANQpxvX8CU+Ny7FUWDeF3Y+g3lROIf4k0UDX9eCyvO531PyYhHga9zvPZJU5b73Y/eXj8Hv9D48n6HaF5LbcjRt8TZTtda5M1DfXnbkX1C0SHCFKzQB5Fe8op4GNGNHavvZESbVwT5r6W1xyuCPBY3Y9YgDqzknH/e3YfNzzuL30l0IebrZ5kKtuDIXt1n868ET6kf3/49tLvrCcZyF8Pu215dAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcPIbNrBhOaBXucoAAAAASUVORK5CYII=\">\\n    <link rel=\"import\" href=\"dist/bazel-html-imports.html\">\\n    <link rel=\"import\" href=\"dist/tf-tensorboard.html\">\\n  </head>\\n  <body>\\n    <tf-tensorboard use-hash></tf-tensorboard>\\n  </body>\\n</html>\\n'"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.display import HTML\n",
    "HTML(str(requests.get('http://localhost:6006').content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\r\n",
      "<!--\r\n",
      "@license\r\n",
      "Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n",
      "\r\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
      "you may not use this file except in compliance with the License.\r\n",
      "You may obtain a copy of the License at\r\n",
      "\r\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "\r\n",
      "Unless required by applicable law or agreed to in writing, software\r\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "See the License for the specific language governing permissions and\r\n",
      "limitations under the License.\r\n",
      "-->\r\n",
      "\r\n",
      "<html>\r\n",
      "  <head>\r\n",
      "    <title>TensorBoard</title>\r\n",
      "    <script src=\"webcomponentsjs/webcomponents-lite.min.js\"></script>\r\n",
      "    <link rel=\"stylesheet\" type=\"text/css\" href=\"lib/css/global.css\">\r\n",
      "    <link rel=\"stylesheet\" type=\"text/css\" href=\"plottable/plottable.css\">\r\n",
      "    <link rel=\"shortcut icon\" href=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMQAAADECAMAAAD3eH5ZAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAD/UExURfFlKfaELvFmKfNyK/67NvWALf68Nv69NvNxK/20NfyyNP22NfN0K/JrKvqhMv2zNf25Nf24Nf23NfeOL/yzNPyvNPJoKviWMPmeMfN1K/WBLfePL/FnKfeML/qlMvR7LPmcMfeLL/aJLvR5LPFoKfJuKvR3LP66NvywNPeNL/V/LfaILv21Nf26NfNzK/NvK/R6LPmaMfyxNPqfMvV+LfurM/iSMPmbMfJvKvmdMfumM/qiMvmZMfytNPJqKvysNPN2K/iYMPNwK/upM/JtKvJsKviVMPaHLvaGLvJpKvR8LPaKLvqkMvuqM/aFLvR4LPuoM/iTMPWDLfiRMPmYMXS0ngkAAALoSURBVHja7drnctpAFIbhFUISSKJ3MKYa0+y4xTW9937/15JkJhlTjhrSrHRmvuf/as6L0YLFCgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBJ6njenqspzgnPrsrGX9Zpi2tCrmnc6+dYNthVY5WpMmxQLWPdMsOuYVwzNj3ei2t3mQwaV43BJPDCS2NbJ5aEeuX/+9qcjQOtfFIkIkrvY2g4MVcmOBsFWbowKO/kNyj62gRpJcDaPBlxLr1B0zdG0C/8LzbJiJrshuvy1gzlA9+rD8mIkuyIJjFE3/dqnYwoSm7IUEPoD/wut8iIguSIDjlFxe/yfXL5vuSI21BTZLLhXoOILMO8Hxwa/L8bI0LfmUdhGowb2ZvT0e57pFNDgB06IlVyjmmIBl2T/nl9Rw6SD9GgSG/Q0uQkaW3XhmovKQ3eFQ4N2Uo9OQ1eFZsNerf7vP+rO4rhmY1Lg3vFVoP8+8BXg1sFnwbnCk4NThW8GuiKBDdkVVtTNFvNelVsNqTbyWnIOM2oeTRoyWvwmpJHg/ucXBrcJuXT4DwrpwZi2vy0VCx8YtXg/D2bU4OfiuQ3eFfE2KD4bfCqiLNB993gXsGlwa2CT4NzBacGIVQ6YsipQdh0xEdODUKjIxrSp88onZ8zbbFLg1DoiFO5BXvDGv2My9/JhUT8JUZTI0yDaNHLBzIbvqTDNYhUiVw/kdjQ1kM2CHFDPjKW+KzyRTF0g/ga9w9y+fANQpxvX8CU+Ny7FUWDeF3Y+g3lROIf4k0UDX9eCyvO531PyYhHga9zvPZJU5b73Y/eXj8Hv9D48n6HaF5LbcjRt8TZTtda5M1DfXnbkX1C0SHCFKzQB5Fe8op4GNGNHavvZESbVwT5r6W1xyuCPBY3Y9YgDqzknH/e3YfNzzuL30l0IebrZ5kKtuDIXt1n868ET6kf3/49tLvrCcZyF8Pu215dAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcPIbNrBhOaBXucoAAAAASUVORK5CYII=\">\r\n",
      "    <link rel=\"import\" href=\"dist/bazel-html-imports.html\">\r\n",
      "    <link rel=\"import\" href=\"dist/tf-tensorboard.html\">\r\n",
      "  </head>\r\n",
      "  <body>\r\n",
      "    <tf-tensorboard use-hash></tf-tensorboard>\r\n",
      "  </body>\r\n",
      "</html>\r\n"
     ]
    }
   ],
   "source": [
    "!curl localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9eb8facdc4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m body = requests.get('https://beta.datascience.com:6007', \n\u001b[1;32m      6\u001b[0m     cookies={\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;34m'datascience-platform'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiIwNTBjZGY2Ny0wMzFlLTRmYjItOGRkZi1kZDc4YWI2OTQ1NzEiLCJzZXJ2aWNlTmFtZSI6ImRlcGxveS1pbnRydWRlci1lYXJseS13YXJuaW5nLXN5LTQ0NDUtdjUiLCJpYXQiOjE0OTkzMDE2ODl9.LOqyHJ1fN4PPYNf5HVXdjqMLZGQGO4G7k0ws-T4RNRk'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    511\u001b[0m         }\n\u001b[1;32m    512\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 141\u001b[0;31m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import requests\n",
    "results = {}\n",
    "\n",
    "body = requests.get('https://beta.datascience.com:6007', \n",
    "    cookies={\n",
    "        'datascience-platform': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiIwNTBjZGY2Ny0wMzFlLTRmYjItOGRkZi1kZDc4YWI2OTQ1NzEiLCJzZXJ2aWNlTmFtZSI6ImRlcGxveS1pbnRydWRlci1lYXJseS13YXJuaW5nLXN5LTQ0NDUtdjUiLCJpYXQiOjE0OTkzMDE2ODl9.LOqyHJ1fN4PPYNf5HVXdjqMLZGQGO4G7k0ws-T4RNRk'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 404,\n",
       " 2: 404,\n",
       " 3: 404,\n",
       " 4: 404,\n",
       " 5: 404,\n",
       " 6: 404,\n",
       " 7: 404,\n",
       " 8: 404,\n",
       " 9: 404,\n",
       " 10: 404,\n",
       " 11: 404,\n",
       " 12: 404,\n",
       " 13: 404,\n",
       " 14: 404,\n",
       " 15: 404,\n",
       " 16: 404,\n",
       " 17: 404,\n",
       " 18: 404,\n",
       " 19: 404,\n",
       " 20: 404,\n",
       " 21: 404,\n",
       " 22: 404,\n",
       " 23: 404,\n",
       " 24: 404,\n",
       " 25: 404,\n",
       " 26: 404,\n",
       " 27: 404,\n",
       " 28: 404,\n",
       " 29: 404,\n",
       " 30: 404,\n",
       " 31: 404,\n",
       " 32: 404,\n",
       " 33: 404,\n",
       " 34: 404,\n",
       " 35: 404,\n",
       " 36: 404,\n",
       " 37: 404,\n",
       " 38: 404,\n",
       " 39: 404,\n",
       " 40: 404,\n",
       " 41: 404,\n",
       " 42: 404,\n",
       " 43: 404,\n",
       " 44: 404,\n",
       " 45: 404,\n",
       " 46: 404,\n",
       " 47: 404,\n",
       " 48: 404,\n",
       " 49: 404,\n",
       " 50: 404,\n",
       " 51: 404,\n",
       " 52: 404,\n",
       " 53: 404,\n",
       " 54: 404,\n",
       " 55: 404,\n",
       " 56: 404,\n",
       " 57: 404,\n",
       " 58: 404,\n",
       " 59: 404,\n",
       " 60: 404,\n",
       " 61: 404,\n",
       " 62: 404,\n",
       " 63: 404,\n",
       " 64: 404,\n",
       " 65: 404,\n",
       " 66: 404,\n",
       " 67: 404,\n",
       " 68: 404,\n",
       " 69: 404,\n",
       " 70: 404,\n",
       " 71: 404,\n",
       " 72: 404,\n",
       " 73: 404,\n",
       " 74: 404,\n",
       " 75: 404,\n",
       " 76: 404,\n",
       " 77: 404,\n",
       " 78: 404,\n",
       " 79: 404,\n",
       " 80: 404,\n",
       " 81: 404,\n",
       " 82: 404,\n",
       " 83: 404,\n",
       " 84: 404,\n",
       " 85: 404,\n",
       " 86: 404,\n",
       " 87: 404,\n",
       " 88: 404,\n",
       " 89: 404,\n",
       " 90: 404,\n",
       " 91: 404,\n",
       " 92: 404,\n",
       " 93: 404,\n",
       " 94: 404,\n",
       " 95: 404,\n",
       " 96: 404,\n",
       " 97: 404,\n",
       " 98: 404,\n",
       " 99: 404,\n",
       " 100: 404,\n",
       " 101: 404,\n",
       " 102: 404,\n",
       " 103: 404,\n",
       " 104: 404,\n",
       " 105: 404,\n",
       " 106: 404,\n",
       " 107: 404,\n",
       " 108: 404,\n",
       " 109: 404,\n",
       " 110: 404,\n",
       " 111: 404,\n",
       " 112: 404,\n",
       " 113: 404,\n",
       " 114: 404,\n",
       " 115: 404,\n",
       " 116: 404,\n",
       " 117: 404,\n",
       " 118: 404,\n",
       " 119: 404,\n",
       " 120: 404,\n",
       " 121: 404,\n",
       " 122: 404,\n",
       " 123: 404,\n",
       " 124: 404,\n",
       " 125: 404,\n",
       " 126: 404,\n",
       " 127: 404,\n",
       " 128: 404,\n",
       " 129: 404,\n",
       " 130: 404,\n",
       " 131: 404,\n",
       " 132: 404,\n",
       " 133: 404,\n",
       " 134: 404,\n",
       " 135: 404,\n",
       " 136: 404,\n",
       " 137: 404,\n",
       " 138: 404,\n",
       " 139: 404,\n",
       " 140: 404,\n",
       " 141: 404,\n",
       " 142: 404,\n",
       " 143: 404,\n",
       " 144: 404,\n",
       " 145: 404,\n",
       " 146: 404,\n",
       " 147: 404,\n",
       " 148: 404,\n",
       " 149: 404,\n",
       " 150: 404,\n",
       " 151: 404,\n",
       " 152: 404,\n",
       " 153: 404,\n",
       " 154: 404,\n",
       " 155: 404,\n",
       " 156: 404,\n",
       " 157: 404,\n",
       " 158: 404,\n",
       " 159: 404,\n",
       " 160: 404,\n",
       " 161: 404,\n",
       " 162: 404,\n",
       " 163: 404,\n",
       " 164: 404,\n",
       " 165: 404,\n",
       " 166: 404,\n",
       " 167: 404,\n",
       " 168: 404,\n",
       " 169: 404,\n",
       " 170: 404,\n",
       " 171: 404,\n",
       " 172: 404,\n",
       " 173: 404,\n",
       " 174: 404,\n",
       " 175: 404,\n",
       " 176: 404,\n",
       " 177: 404,\n",
       " 178: 404,\n",
       " 179: 404,\n",
       " 180: 404,\n",
       " 181: 404,\n",
       " 182: 404,\n",
       " 183: 404,\n",
       " 184: 404,\n",
       " 185: 404,\n",
       " 186: 404,\n",
       " 187: 404,\n",
       " 188: 404,\n",
       " 189: 404,\n",
       " 190: 404,\n",
       " 191: 404,\n",
       " 192: 404,\n",
       " 193: 404,\n",
       " 194: 404,\n",
       " 195: 404,\n",
       " 196: 404,\n",
       " 197: 404,\n",
       " 198: 404,\n",
       " 199: 404,\n",
       " 200: 404,\n",
       " 201: 404,\n",
       " 202: 404,\n",
       " 203: 404,\n",
       " 204: 404,\n",
       " 205: 404,\n",
       " 206: 404,\n",
       " 207: 404,\n",
       " 208: 404,\n",
       " 209: 404,\n",
       " 210: 404,\n",
       " 211: 404,\n",
       " 212: 404,\n",
       " 213: 404,\n",
       " 214: 404,\n",
       " 215: 404,\n",
       " 216: 404,\n",
       " 217: 404,\n",
       " 218: 404,\n",
       " 219: 404,\n",
       " 220: 404,\n",
       " 221: 404,\n",
       " 222: 404,\n",
       " 223: 404,\n",
       " 224: 404,\n",
       " 225: 404,\n",
       " 226: 404,\n",
       " 227: 404,\n",
       " 228: 404,\n",
       " 229: 404,\n",
       " 230: 404,\n",
       " 231: 404,\n",
       " 232: 404,\n",
       " 233: 404,\n",
       " 234: 404,\n",
       " 235: 404,\n",
       " 236: 404,\n",
       " 237: 404,\n",
       " 238: 404,\n",
       " 239: 404,\n",
       " 240: 404,\n",
       " 241: 404,\n",
       " 242: 404,\n",
       " 243: 404,\n",
       " 244: 404,\n",
       " 245: 404,\n",
       " 246: 404,\n",
       " 247: 404,\n",
       " 248: 404,\n",
       " 249: 404,\n",
       " 250: 404,\n",
       " 251: 404,\n",
       " 252: 404,\n",
       " 253: 404,\n",
       " 254: 404,\n",
       " 255: 404,\n",
       " 256: 404,\n",
       " 257: 404,\n",
       " 258: 404,\n",
       " 259: 404,\n",
       " 260: 404,\n",
       " 261: 404,\n",
       " 262: 404,\n",
       " 263: 404,\n",
       " 264: 404,\n",
       " 265: 404,\n",
       " 266: 404,\n",
       " 267: 404,\n",
       " 268: 404,\n",
       " 269: 404,\n",
       " 270: 404,\n",
       " 271: 404,\n",
       " 272: 404,\n",
       " 273: 404,\n",
       " 274: 404,\n",
       " 275: 404,\n",
       " 276: 404,\n",
       " 277: 404,\n",
       " 278: 404,\n",
       " 279: 404,\n",
       " 280: 404,\n",
       " 281: 404,\n",
       " 282: 404,\n",
       " 283: 404,\n",
       " 284: 404,\n",
       " 285: 404,\n",
       " 286: 404,\n",
       " 287: 404,\n",
       " 288: 404,\n",
       " 289: 404,\n",
       " 290: 404,\n",
       " 291: 404,\n",
       " 292: 404,\n",
       " 293: 404,\n",
       " 294: 404,\n",
       " 295: 404,\n",
       " 296: 404,\n",
       " 297: 404,\n",
       " 298: 404,\n",
       " 299: 404,\n",
       " 300: 404,\n",
       " 301: 404,\n",
       " 302: 404,\n",
       " 303: 404,\n",
       " 304: 404,\n",
       " 305: 404,\n",
       " 306: 404,\n",
       " 307: 404,\n",
       " 308: 404,\n",
       " 309: 404,\n",
       " 310: 404,\n",
       " 311: 404,\n",
       " 312: 404,\n",
       " 313: 404,\n",
       " 314: 404,\n",
       " 315: 404,\n",
       " 316: 404,\n",
       " 317: 404,\n",
       " 318: 404,\n",
       " 319: 404,\n",
       " 320: 404,\n",
       " 321: 404,\n",
       " 322: 404,\n",
       " 323: 404,\n",
       " 324: 404,\n",
       " 325: 404,\n",
       " 326: 404,\n",
       " 327: 404,\n",
       " 328: 404,\n",
       " 329: 404,\n",
       " 330: 404,\n",
       " 331: 404,\n",
       " 332: 404,\n",
       " 333: 404,\n",
       " 334: 404,\n",
       " 335: 404,\n",
       " 336: 404,\n",
       " 337: 404,\n",
       " 338: 404,\n",
       " 339: 404,\n",
       " 340: 404,\n",
       " 341: 404,\n",
       " 342: 404,\n",
       " 343: 404,\n",
       " 344: 404,\n",
       " 345: 404,\n",
       " 346: 404,\n",
       " 347: 404,\n",
       " 348: 404,\n",
       " 349: 404,\n",
       " 350: 404,\n",
       " 351: 404,\n",
       " 352: 404,\n",
       " 353: 404,\n",
       " 354: 404,\n",
       " 355: 404,\n",
       " 356: 404,\n",
       " 357: 404,\n",
       " 358: 404,\n",
       " 359: 404,\n",
       " 360: 404,\n",
       " 361: 404,\n",
       " 362: 404,\n",
       " 363: 404,\n",
       " 364: 404,\n",
       " 365: 404,\n",
       " 366: 404,\n",
       " 367: 404,\n",
       " 368: 404,\n",
       " 369: 404,\n",
       " 370: 404,\n",
       " 371: 404,\n",
       " 372: 404,\n",
       " 373: 404,\n",
       " 374: 404,\n",
       " 375: 404,\n",
       " 376: 404,\n",
       " 377: 404,\n",
       " 378: 404,\n",
       " 379: 404,\n",
       " 380: 404,\n",
       " 381: 404,\n",
       " 382: 404,\n",
       " 383: 404,\n",
       " 384: 404,\n",
       " 385: 404,\n",
       " 386: 404,\n",
       " 387: 404,\n",
       " 388: 404,\n",
       " 389: 404,\n",
       " 390: 404,\n",
       " 391: 404,\n",
       " 392: 404,\n",
       " 393: 404,\n",
       " 394: 404,\n",
       " 395: 404,\n",
       " 396: 404,\n",
       " 397: 404,\n",
       " 398: 404,\n",
       " 399: 404,\n",
       " 400: 404,\n",
       " 401: 404,\n",
       " 402: 404,\n",
       " 403: 404,\n",
       " 404: 404,\n",
       " 405: 404,\n",
       " 406: 404,\n",
       " 407: 404,\n",
       " 408: 404,\n",
       " 409: 404,\n",
       " 410: 404,\n",
       " 411: 404,\n",
       " 412: 404,\n",
       " 413: 404,\n",
       " 414: 404,\n",
       " 415: 404,\n",
       " 416: 404,\n",
       " 417: 404,\n",
       " 418: 404,\n",
       " 419: 404,\n",
       " 420: 404,\n",
       " 421: 404,\n",
       " 422: 404,\n",
       " 423: 404,\n",
       " 424: 404,\n",
       " 425: 404,\n",
       " 426: 404,\n",
       " 427: 404,\n",
       " 428: 404,\n",
       " 429: 404,\n",
       " 430: 404,\n",
       " 431: 404,\n",
       " 432: 404,\n",
       " 433: 404,\n",
       " 434: 404,\n",
       " 435: 404,\n",
       " 436: 404,\n",
       " 437: 404,\n",
       " 438: 404,\n",
       " 439: 404,\n",
       " 440: 404,\n",
       " 441: 404,\n",
       " 442: 404,\n",
       " 443: 404,\n",
       " 444: 404,\n",
       " 445: 404,\n",
       " 446: 404,\n",
       " 447: 404,\n",
       " 448: 404,\n",
       " 449: 404,\n",
       " 450: 404,\n",
       " 451: 404,\n",
       " 452: 404,\n",
       " 453: 404,\n",
       " 454: 404,\n",
       " 455: 404,\n",
       " 456: 404,\n",
       " 457: 404,\n",
       " 458: 404,\n",
       " 459: 404,\n",
       " 460: 404,\n",
       " 461: 404,\n",
       " 462: 404,\n",
       " 463: 404,\n",
       " 464: 404,\n",
       " 465: 404,\n",
       " 466: 404,\n",
       " 467: 404,\n",
       " 468: 404,\n",
       " 469: 404,\n",
       " 470: 404,\n",
       " 471: 404,\n",
       " 472: 404,\n",
       " 473: 404,\n",
       " 474: 404,\n",
       " 475: 404,\n",
       " 476: 404,\n",
       " 477: 404,\n",
       " 478: 404,\n",
       " 479: 404,\n",
       " 480: 404,\n",
       " 481: 404,\n",
       " 482: 404,\n",
       " 483: 404,\n",
       " 484: 404,\n",
       " 485: 404,\n",
       " 486: 404,\n",
       " 487: 404,\n",
       " 488: 404,\n",
       " 489: 404,\n",
       " 490: 404,\n",
       " 491: 404,\n",
       " 492: 404,\n",
       " 493: 404,\n",
       " 494: 404,\n",
       " 495: 404,\n",
       " 496: 404,\n",
       " 497: 404,\n",
       " 498: 404,\n",
       " 499: 404,\n",
       " 500: 404,\n",
       " 501: 404,\n",
       " 502: 404,\n",
       " 503: 404,\n",
       " 504: 404,\n",
       " 505: 404,\n",
       " 506: 404,\n",
       " 507: 404,\n",
       " 508: 404,\n",
       " 509: 404,\n",
       " 510: 404,\n",
       " 511: 404,\n",
       " 512: 404,\n",
       " 513: 404,\n",
       " 514: 404,\n",
       " 515: 404,\n",
       " 516: 404,\n",
       " 517: 404,\n",
       " 518: 404,\n",
       " 519: 404,\n",
       " 520: 404,\n",
       " 521: 404,\n",
       " 522: 404,\n",
       " 523: 404,\n",
       " 524: 404,\n",
       " 525: 404,\n",
       " 526: 404,\n",
       " 527: 404,\n",
       " 528: 404,\n",
       " 529: 404,\n",
       " 530: 404,\n",
       " 531: 404,\n",
       " 532: 404,\n",
       " 533: 404,\n",
       " 534: 404,\n",
       " 535: 404,\n",
       " 536: 404,\n",
       " 537: 404,\n",
       " 538: 404,\n",
       " 539: 404,\n",
       " 540: 404,\n",
       " 541: 404,\n",
       " 542: 404,\n",
       " 543: 404,\n",
       " 544: 404,\n",
       " 545: 404,\n",
       " 546: 404,\n",
       " 547: 404,\n",
       " 548: 404,\n",
       " 549: 404,\n",
       " 550: 404,\n",
       " 551: 404,\n",
       " 552: 404,\n",
       " 553: 404,\n",
       " 554: 404,\n",
       " 555: 404,\n",
       " 556: 404,\n",
       " 557: 404,\n",
       " 558: 404,\n",
       " 559: 404,\n",
       " 560: 404,\n",
       " 561: 404,\n",
       " 562: 404,\n",
       " 563: 404,\n",
       " 564: 404,\n",
       " 565: 404,\n",
       " 566: 404,\n",
       " 567: 404,\n",
       " 568: 404,\n",
       " 569: 404,\n",
       " 570: 404,\n",
       " 571: 404,\n",
       " 572: 404,\n",
       " 573: 404,\n",
       " 574: 404,\n",
       " 575: 404,\n",
       " 576: 404,\n",
       " 577: 404,\n",
       " 578: 404,\n",
       " 579: 404,\n",
       " 580: 404,\n",
       " 581: 404,\n",
       " 582: 404,\n",
       " 583: 404,\n",
       " 584: 404,\n",
       " 585: 404,\n",
       " 586: 404,\n",
       " 587: 404,\n",
       " 588: 404,\n",
       " 589: 404,\n",
       " 590: 404,\n",
       " 591: 404,\n",
       " 592: 404,\n",
       " 593: 404,\n",
       " 594: 404,\n",
       " 595: 404,\n",
       " 596: 404,\n",
       " 597: 404,\n",
       " 598: 404,\n",
       " 599: 404,\n",
       " 600: 404,\n",
       " 601: 404,\n",
       " 602: 404,\n",
       " 603: 404,\n",
       " 604: 404,\n",
       " 605: 404,\n",
       " 606: 404,\n",
       " 607: 404,\n",
       " 608: 404,\n",
       " 609: 404,\n",
       " 610: 404,\n",
       " 611: 404,\n",
       " 612: 404,\n",
       " 613: 404,\n",
       " 614: 404,\n",
       " 615: 404,\n",
       " 616: 404,\n",
       " 617: 404,\n",
       " 618: 404,\n",
       " 619: 404,\n",
       " 620: 404,\n",
       " 621: 404,\n",
       " 622: 404,\n",
       " 623: 404,\n",
       " 624: 404,\n",
       " 625: 404,\n",
       " 626: 404,\n",
       " 627: 404,\n",
       " 628: 404,\n",
       " 629: 404,\n",
       " 630: 404,\n",
       " 631: 404,\n",
       " 632: 404,\n",
       " 633: 404,\n",
       " 634: 404,\n",
       " 635: 404,\n",
       " 636: 404,\n",
       " 637: 404,\n",
       " 638: 404,\n",
       " 639: 404,\n",
       " 640: 404,\n",
       " 641: 404,\n",
       " 642: 404,\n",
       " 643: 404,\n",
       " 644: 404,\n",
       " 645: 404,\n",
       " 646: 404,\n",
       " 647: 404,\n",
       " 648: 404,\n",
       " 649: 404,\n",
       " 650: 404,\n",
       " 651: 404,\n",
       " 652: 404,\n",
       " 653: 404,\n",
       " 654: 404,\n",
       " 655: 404,\n",
       " 656: 404,\n",
       " 657: 404,\n",
       " 658: 404,\n",
       " 659: 404,\n",
       " 660: 404,\n",
       " 661: 404,\n",
       " 662: 404,\n",
       " 663: 404,\n",
       " 664: 404,\n",
       " 665: 404,\n",
       " 666: 404,\n",
       " 667: 404,\n",
       " 668: 404,\n",
       " 669: 404,\n",
       " 670: 404,\n",
       " 671: 404,\n",
       " 672: 404,\n",
       " 673: 404,\n",
       " 674: 404,\n",
       " 675: 404,\n",
       " 676: 404,\n",
       " 677: 404,\n",
       " 678: 404,\n",
       " 679: 404,\n",
       " 680: 404,\n",
       " 681: 404,\n",
       " 682: 404,\n",
       " 683: 404,\n",
       " 684: 404,\n",
       " 685: 404,\n",
       " 686: 404,\n",
       " 687: 404,\n",
       " 688: 404,\n",
       " 689: 404,\n",
       " 690: 404,\n",
       " 691: 404,\n",
       " 692: 404,\n",
       " 693: 404,\n",
       " 694: 404,\n",
       " 695: 404,\n",
       " 696: 404,\n",
       " 697: 404,\n",
       " 698: 404,\n",
       " 699: 404,\n",
       " 700: 404,\n",
       " 701: 404,\n",
       " 702: 404,\n",
       " 703: 404,\n",
       " 704: 404,\n",
       " 705: 404,\n",
       " 706: 404,\n",
       " 707: 404,\n",
       " 708: 404,\n",
       " 709: 404,\n",
       " 710: 404,\n",
       " 711: 404,\n",
       " 712: 404,\n",
       " 713: 404,\n",
       " 714: 404,\n",
       " 715: 404,\n",
       " 716: 404,\n",
       " 717: 404,\n",
       " 718: 404,\n",
       " 719: 404,\n",
       " 720: 404,\n",
       " 721: 404,\n",
       " 722: 404,\n",
       " 723: 404,\n",
       " 724: 404,\n",
       " 725: 404,\n",
       " 726: 404,\n",
       " 727: 404,\n",
       " 728: 404,\n",
       " 729: 404,\n",
       " 730: 404,\n",
       " 731: 404,\n",
       " 732: 404,\n",
       " 733: 404,\n",
       " 734: 404,\n",
       " 735: 404,\n",
       " 736: 404,\n",
       " 737: 404,\n",
       " 738: 404,\n",
       " 739: 404,\n",
       " 740: 404,\n",
       " 741: 404,\n",
       " 742: 404,\n",
       " 743: 404,\n",
       " 744: 404,\n",
       " 745: 404,\n",
       " 746: 404,\n",
       " 747: 404,\n",
       " 748: 404,\n",
       " 749: 404,\n",
       " 750: 404,\n",
       " 751: 404,\n",
       " 752: 404,\n",
       " 753: 404,\n",
       " 754: 404,\n",
       " 755: 404,\n",
       " 756: 404,\n",
       " 757: 404,\n",
       " 758: 404,\n",
       " 759: 404,\n",
       " 760: 404,\n",
       " 761: 404,\n",
       " 762: 404,\n",
       " 763: 404,\n",
       " 764: 404,\n",
       " 765: 404,\n",
       " 766: 404,\n",
       " 767: 404,\n",
       " 768: 404,\n",
       " 769: 404,\n",
       " 770: 404,\n",
       " 771: 404,\n",
       " 772: 404,\n",
       " 773: 404,\n",
       " 774: 404,\n",
       " 775: 404,\n",
       " 776: 404,\n",
       " 777: 404,\n",
       " 778: 404,\n",
       " 779: 404,\n",
       " 780: 404,\n",
       " 781: 404,\n",
       " 782: 404,\n",
       " 783: 404,\n",
       " 784: 404,\n",
       " 785: 404,\n",
       " 786: 404,\n",
       " 787: 404,\n",
       " 788: 404,\n",
       " 789: 404,\n",
       " 790: 404,\n",
       " 791: 404,\n",
       " 792: 404,\n",
       " 793: 404,\n",
       " 794: 404,\n",
       " 795: 404,\n",
       " 796: 404,\n",
       " 797: 404,\n",
       " 798: 404,\n",
       " 799: 404,\n",
       " 800: 404,\n",
       " 801: 404,\n",
       " 802: 404,\n",
       " 803: 404,\n",
       " 804: 404,\n",
       " 805: 404,\n",
       " 806: 404,\n",
       " 807: 404,\n",
       " 808: 404,\n",
       " 809: 404,\n",
       " 810: 404,\n",
       " 811: 404,\n",
       " 812: 404,\n",
       " 813: 404,\n",
       " 814: 404,\n",
       " 815: 404,\n",
       " 816: 404,\n",
       " 817: 404,\n",
       " 818: 404,\n",
       " 819: 404,\n",
       " 820: 404,\n",
       " 821: 404,\n",
       " 822: 404,\n",
       " 823: 404,\n",
       " 824: 404,\n",
       " 825: 404,\n",
       " 826: 404,\n",
       " 827: 404,\n",
       " 828: 404,\n",
       " 829: 404,\n",
       " 830: 404,\n",
       " 831: 404,\n",
       " 832: 404}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from boto.s3.connection import S3Connection\n",
    "# from boto.s3.key import Key\n",
    "# import h2o\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tabulate import tabulate\n",
    "\n",
    "# # initialize the model scoring server\n",
    "# h2o.init(nthreads=1,max_mem_size=1, start_h2o=True, strict_version_check = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Train your own word2vec model using dataset, and load those vectors into spacy. Visually inspect the results of the vector as a self organizing map.\n",
    "\n",
    "#### Gotchas and notes:\n",
    "* Due to the way the binary vector files are read and written, only single word lexemes can be mapped to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install spacy >> ../spacy-install.log\n",
    "#!python -m spacy download en >> ../spacy-download.log\n",
    "#!pip install gensim >> ../gensim-log.txt\n",
    "#!pip install tensorflow >> ../tensorflow-log.txt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "dataset = fetch_20newsgroups()\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install skater >> skaterlog.txt\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from skater.util.progressbar import ProgressBar\n",
    "        \n",
    "class TextProcesser(object):\n",
    "    \n",
    "    def __init__(self, nlp=None):\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        \n",
    "    def __call__(self, corpus, merge_ents=False, matcher=None):\n",
    "        p = ProgressBar(len(corpus))\n",
    "        for doc in self.nlp.pipe(corpus, parse=False, tag=bool(merge_ents)):\n",
    "            p.animate()\n",
    "            if matcher:\n",
    "                matcher(doc)\n",
    "            if merge_ents:\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge()\n",
    "            yield list(map(self.process_token, doc))\n",
    "            \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return 'URL'\n",
    "        elif token.like_email:\n",
    "            return 'EMAIL'\n",
    "        elif token.like_num:\n",
    "            return 'NUM'\n",
    "        else:\n",
    "            return token.lower_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] iterations ████████████████████ Time elapsed: 0 seconds[['lets', 'go', 'to', 'new', 'york', '.']]\n"
     ]
    }
   ],
   "source": [
    "from spacy.attrs import LOWER\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_entity('Twitter')\n",
    "matcher.add_pattern(\"Twitter\", [{LOWER: \"twitter\"},])\n",
    "processor = TextProcesser(nlp=nlp)\n",
    "print(list(processor([\"Lets go to New York.\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11314/11314] iterations ████████████████████ Time elapsed: 62 seconds"
     ]
    }
   ],
   "source": [
    "processed_sents = list(processor(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_sents, ###tokenized senteces, list of list of strings\n",
    "                 size=250,  #size of embedding vectors\n",
    "                 workers=8, #how many threads?\n",
    "                 min_count=5, #minimum number of token instances to be considered\n",
    "                 sample=0, #weight of downsampling common words? \n",
    "                 sg = 0, #should we use skip-gram? if 0, then cbow\n",
    "                 hs=0, #heirarchical softmax?\n",
    "                 iter=5 #training epocs\n",
    "        )\n",
    "with open('gensim-model.pkl', 'wb') as model_file:\n",
    "    model.save(model_file)\n",
    "    \n",
    "    \n",
    "#load vectors here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0.639094</td>\n",
       "      <td>0.538254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>0.549838</td>\n",
       "      <td>0.523014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>0.532845</td>\n",
       "      <td>0.512542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.210933</td>\n",
       "      <td>0.173088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             custom   default\n",
       "agency     0.639094  0.538254\n",
       "congress   0.549838  0.523014\n",
       "president  0.532845  0.512542\n",
       "sport      0.210933  0.173088"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sims = {}\n",
    "test_word = 'government'\n",
    "for word in ['president', 'congress', 'sport', 'agency']:\n",
    "    custom_similarity = model.similarity(test_word, word)\n",
    "    spacy_similarity = nlp(test_word).similarity(nlp(word))\n",
    "    sims[word] = {'default': spacy_similarity, 'custom':custom_similarity}\n",
    "pd.DataFrame(sims).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import spacy\n",
    "#write to disk\n",
    "vector_file = 'custom-vectors.word2vec'\n",
    "model.wv.save_word2vec_format(str(vector_file), binary=False)\n",
    "\n",
    "\n",
    "#delete first 2 lines; contains non-decodeable stuff\n",
    "with open(str(vector_file), 'rb') as f:\n",
    "    data = f.readlines()\n",
    "f.close()\n",
    "with open(str(vector_file), 'wb') as f:\n",
    "    f.writelines(data)\n",
    "    \n",
    "\n",
    "#compress via bz2\n",
    "compressed_filename = vector_file + '.bz2'\n",
    "z = bz2.compress(open(vector_file, 'rb').read())\n",
    "with open(compressed_filename, 'wb') as out:\n",
    "    out.write(z)\n",
    "spacy.vocab.write_binary_vectors('custom-vectors.word2vec.bz2', 'custom-vectors.bin')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_new = spacy.load('en')\n",
    "nlp_new.vocab.load_vectors_from_bin_loc('custom-vectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0.639094</td>\n",
       "      <td>0.639094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>0.549838</td>\n",
       "      <td>0.549838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>0.532845</td>\n",
       "      <td>0.532845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.210933</td>\n",
       "      <td>0.210933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             custom   default\n",
       "agency     0.639094  0.639094\n",
       "congress   0.549838  0.549838\n",
       "president  0.532845  0.532845\n",
       "sport      0.210933  0.210933"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sims = {}\n",
    "test_word = 'government'\n",
    "for word in ['president', 'congress', 'sport', 'agency']:\n",
    "    custom_similarity = model.similarity(test_word, word)\n",
    "    spacy_similarity = nlp(test_word).similarity(nlp_new(word))\n",
    "    sims[word] = {'default': spacy_similarity, 'custom':custom_similarity}\n",
    "pd.DataFrame(sims).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tailoring the word embeddings can improve classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X1 = np.array(list(map(lambda doc: doc.vector, nlp.pipe(corpus, tag=False, parse=False))))\n",
    "X2 = np.array(list(map(lambda doc: doc.vector, nlp_new.pipe(corpus, tag=False, parse=False))))\n",
    "y = dataset.target\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, test_size=.3)\n",
    "svm1 = SVC()\n",
    "svm1.fit(X1_train, y_train)\n",
    "\n",
    "svm2 = SVC()\n",
    "svm2.fit(X2_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "preds1 = svm1.predict(X1_test)\n",
    "preds2 = svm2.predict(X2_test)\n",
    "\n",
    "print(accuracy_score(y_test, preds1))\n",
    "print(accuracy_score(y_test, preds2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100\"\n",
       "            height=\"100\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f92bd2c4e80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://localhost:6006', width=100, height=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "LOG_DIR = '../../'\n",
    "\n",
    "embedding_var = tf.Variable(initial_value=model.syn1neg, trainable=False)\n",
    "with tf.Session() as s:\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    r = s.run(embedding_var)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(s, os.path.join(LOG_DIR, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "class SOM(object):\n",
    "    \"\"\"\n",
    "    2-D Self-Organizing Map with Gaussian Neighbourhood function\n",
    "    and linearly decreasing learning rate.\n",
    "    \"\"\"\n",
    " \n",
    "    #To check if the SOM has been trained\n",
    "    _trained = False\n",
    " \n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Initializes all necessary components of the TensorFlow\n",
    "        Graph.\n",
    " \n",
    "        m X n are the dimensions of the SOM. 'n_iterations' should\n",
    "        should be an integer denoting the number of iterations undergone\n",
    "        while training.\n",
    "        'dim' is the dimensionality of the training inputs.\n",
    "        'alpha' is a number denoting the initial time(iteration no)-based\n",
    "        learning rate. Default value is 0.3\n",
    "        'sigma' is the the initial neighbourhood value, denoting\n",
    "        the radius of influence of the BMU while training. By default, its\n",
    "        taken to be half of max(m, n).\n",
    "        \"\"\"\n",
    " \n",
    "        #Assign required variables first\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    " \n",
    "        ##INITIALIZE GRAPH\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        ##POPULATE GRAPH WITH NECESSARY COMPONENTS\n",
    "        with self._graph.as_default():\n",
    " \n",
    "            ##VARIABLES AND CONSTANT OPS FOR DATA STORAGE\n",
    " \n",
    "            #Randomly initialized weightage vectors for all neurons,\n",
    "            #stored together as a matrix Variable of size [m*n, dim]\n",
    "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            #Matrix of size [m*n, 2] for SOM grid locations\n",
    "            #of neurons\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "            ##PLACEHOLDERS FOR TRAINING INPUTS\n",
    "            #We need to assign them as attributes to self, since they\n",
    "            #will be fed in during training\n",
    " \n",
    "            #The training vector\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            #Iteration number\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            ##CONSTRUCT TRAINING OP PIECE BY PIECE\n",
    "            #Only the final, 'root' training op needs to be assigned as\n",
    "            #an attribute to self, since all the rest will be executed\n",
    "            #automatically during training\n",
    " \n",
    "            #To compute the Best Matching Unit given a vector\n",
    "            #Basically calculates the Euclidean distance between every\n",
    "            #neuron's weightage vector and the input, and returns the\n",
    "            #index of the neuron which gives the least value\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            #This will extract the location of the BMU based on the BMU's\n",
    "            #index\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]))),\n",
    "                                 [2])\n",
    " \n",
    "            #To compute the alpha and sigma values based on iteration\n",
    "            #number\n",
    "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
    "                                                  self._n_iterations))\n",
    "            _alpha_op = tf.multiply(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.multiply(sigma, learning_rate_op)\n",
    " \n",
    "            #Construct the op that will generate a vector with learning\n",
    "            #rates for all neurons, based on iteration number and location\n",
    "            #wrt BMU.\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                self._location_vects, tf.stack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            #Finally, the op that will use learning_rate_op to update\n",
    "            #the weightage vectors of all neurons based on a particular\n",
    "            #input\n",
    "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.multiply(\n",
    "                learning_rate_multiplier,\n",
    "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects))                                         \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            ##INITIALIZE SESSION\n",
    "            self._sess = tf.Session()\n",
    " \n",
    "            ##INITIALIZE VARIABLES\n",
    "            init_op = tf.initialize_all_variables()\n",
    "            self._sess.run(init_op)\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        \"\"\"\n",
    "        Yields one by one the 2-D locations of the individual neurons\n",
    "        in the SOM.\n",
    "        \"\"\"\n",
    "        #Nested iterations over both dimensions\n",
    "        #to generate all 2-D locations in the map\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM.\n",
    "        Current weightage vectors for all neurons(initially random) are\n",
    "        taken as starting conditions for training.\n",
    "        \"\"\"\n",
    " \n",
    "        #Training iterations\n",
    "        for iter_no in range(self._n_iterations):\n",
    "            #Train with each vector one by one\n",
    "            for input_vect in input_vects:\n",
    "                self._sess.run(self._training_op,\n",
    "                               feed_dict={self._vect_input: input_vect,\n",
    "                                          self._iter_input: iter_no})\n",
    " \n",
    "        #Store a centroid grid for easy retrieval later on\n",
    "        centroid_grid = [[] for i in range(self._m)]\n",
    "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "        self._locations = list(self._sess.run(self._location_vects))\n",
    "        for i, loc in enumerate(self._locations):\n",
    "            centroid_grid[loc[0]].append(self._weightages[i])\n",
    "        self._centroid_grid = centroid_grid\n",
    " \n",
    "        self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        \"\"\"\n",
    "        Returns a list of 'm' lists, with each inner list containing\n",
    "        the 'n' corresponding centroid locations as 1-D NumPy arrays.\n",
    "        \"\"\"\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        \"\"\"\n",
    "        Maps each input vector to the relevant neuron in the SOM\n",
    "        grid.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM.\n",
    "        Returns a list of 1-D NumPy arrays containing (row, column)\n",
    "        info for each input vector(in the same order), corresponding\n",
    "        to mapped neuron.\n",
    "        \"\"\"\n",
    " \n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "colors = np.array(\n",
    "     [[0., 0., 0.],\n",
    "      [0., 0., 1.],\n",
    "      [0., 0., 0.5],\n",
    "      [0.125, 0.529, 1.0],\n",
    "      [0.33, 0.4, 0.67],\n",
    "      [0.6, 0.5, 1.0],\n",
    "      [0., 1., 0.],\n",
    "      [1., 0., 0.],\n",
    "      [0., 1., 1.],\n",
    "      [1., 0., 1.],\n",
    "      [1., 1., 0.],\n",
    "      [1., 1., 1.],\n",
    "      [.33, .33, .33],\n",
    "      [.5, .5, .5],\n",
    "      [.66, .66, .66]])\n",
    "color_names = \\\n",
    "    ['black', 'blue', 'darkblue', 'skyblue',\n",
    "     'greyblue', 'lilac', 'green', 'red',\n",
    "     'cyan', 'violet', 'yellow', 'white',\n",
    "     'darkgrey', 'mediumgrey', 'lightgrey']\n",
    "    \n",
    "som = SOM(20, 30, 3, 400)\n",
    "som.train(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get output grid\n",
    "image_grid = som.get_centroids()\n",
    " \n",
    "#Map colours to their closest neurons\n",
    "mapped = som.map_vects(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7b0b02668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plot\n",
    "plt.imshow(image_grid)\n",
    "plt.title('Color SOM')\n",
    "for i, m in enumerate(mapped):\n",
    "    plt.text(m[1], m[0], color_names[i], ha='center', va='center',\n",
    "             bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<p id=\"note1\">\n",
    "turns out the distinction may not be that important. (see [Levy and Goldberg (2014), Pennington et al. (2014), Österlund et al. (2015)] as referenced in https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model outline\n",
    "#### Definitions phrase\n",
    "* Decide on an architecture\n",
    "* Define all variables as tensors\n",
    "* Define how to generate outputs from your inputs and variables\n",
    "* Define a cost function with respect to your predictions and you labels\n",
    "* Define an optimizer that minimizes your cost function\n",
    "#### Execution phase\n",
    "* create an execution session\n",
    "* Initialize your variables\n",
    "* over n epochs, run the optimizer, feeding it some data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchFeeder(object):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.i = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            yield self.__iter__(self)\n",
    "    def __iter__(self):\n",
    "        X = self.X[self.i:self.i + self.batch_size]\n",
    "        y = self.y[self.i:self.i + self.batch_size]\n",
    "        self.i += self.batch_size\n",
    "        return X, y\n",
    "        \n",
    "    def __next__(self):\n",
    "        return self.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, X, y, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.input_dim = X_train.shape[1]\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.input_dim))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.weights = {\n",
    "                    'weights1':tf.get_variable('weights1', (self.input_dim,self.hidden_dim ), dtype=self.default_dtype), \n",
    "                    'bias1':tf.get_variable('bias1', (self.hidden_dim, ), dtype=self.default_dtype), \n",
    "                    'weights2':tf.get_variable('weights2', (self.hidden_dim, self.n_classes ), dtype=self.default_dtype), \n",
    "                    'bias2':tf.get_variable('bias2', (self.n_classes, ), dtype=self.default_dtype)}\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, x_input, weights):\n",
    "        hidden = tf.matmul(x_input, weights['weights1'])\n",
    "        hidden = tf.add(hidden, weights['bias1'])\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        output = tf.matmul(hidden, weights['weights2'])\n",
    "        output = tf.add(output, weights['bias2'])\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = MLP(X, y)\n",
    "c.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.chkpt\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.91      0.93        67\n",
      "          1       0.94      0.97      0.96       104\n",
      "\n",
      "avg / total       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.argmax(y_test, axis=1), c.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.929648241206\n",
      "Test:  0.888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "gb = MLPClassifier(hidden_layer_sizes=(1000, ))\n",
    "gb.fit(X_train, np.argmax(y_train, axis=1))\n",
    "print(\"Train: \", np.mean(gb.predict(X_train) == np.argmax(y_train, axis=1)))\n",
    "print(\"Test: \", np.mean(gb.predict(X_test) == np.argmax(y_test, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Sequence modeling\n",
    "* Preprocessing\n",
    "* Create data structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim >> gensim-log.txt\n",
    "#!pip install spacy >> spacy-log.txt\n",
    "#!python -m spacy download en >> spacy-download.txt\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 0\n",
    "        self.MISSING_VAL = 1\n",
    "        self.START_VAL = 2\n",
    "        self.END_VAL = 3\n",
    "        self.INDEX_OFFSET = 4\n",
    "        self.vocab = OrderedDict()\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj), 0)\n",
    "        result = obj[:self.max_len] + [self.PADDING_VAL] * n_pads\n",
    "        result[-1] = self.END_VAL\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def check_word(self, word):\n",
    "        current_vocab_size = self.get_current_vocab_size() # 0\n",
    "        if current_vocab_size <= self.max_vocab_size:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab.update({word: current_vocab_size + self.INDEX_OFFSET}) #{'apple': 0}\n",
    "        try:\n",
    "            return self.vocab[word]\n",
    "        except KeyError:\n",
    "            return self.MISSING_VAL\n",
    "        \n",
    "    def __call__(self, corpus, merge_ents=True):\n",
    "        docs = []\n",
    "        if merge_ents:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False):\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge()\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                tokens = [self.START_VAL] + tokens + [self.END_VAL]\n",
    "                docs.append(self.padder(tokens))\n",
    "        else:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False, tag=False, entity=False):\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                docs.append(self.padder(tokens))\n",
    "        \n",
    "        return docs\n",
    "  \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return self.check_word(\"URL\")\n",
    "        elif token.like_email:\n",
    "            return self.check_word(\"EMAIL\")\n",
    "        elif token.like_num:\n",
    "            return self.check_word(\"NUM\")\n",
    "        else:\n",
    "            return self.check_word(token.lower_)\n",
    "\n",
    "\n",
    "#nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset = fetch_20newsgroups()\n",
    "#corpus = dataset.data\n",
    "processor = TextProcesser(nlp=nlp, max_len=100)\n",
    "processed_corpus = processor(corpus, merge_ents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, X, y, max_seq_len, lstm_size=1, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.max_seq_len = max_seq_len or () #len of X\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        self.vocab_size\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.max_seq_len))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.embeddings = tf.Variable(shape=(self.vocab_size, self.hidden_dim))\n",
    "                self.lstm_state = tf.zeros([batch_size, lstm.state_size])\n",
    "                self.lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, word_input):\n",
    "        hidden = tf.nn.embedding_lookup(self.embeddings, word_inputs)\n",
    "        output, self.lstm_state = self.lstm(hidden, self.lstm_state)\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class LSTM(object):\n",
    "    def __init__(self, X, y, max_seq_len, lstm_size=1, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.max_seq_len = max_seq_len or () #len of X\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        self.vocab_size\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.max_seq_len))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.embeddings = tf.Variable(shape=(self.vocab_size, self.hidden_dim))\n",
    "                self.lstm_state = tf.zeros([batch_size, lstm.state_size])\n",
    "                self.lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, word_input):\n",
    "        hidden = tf.nn.embedding_lookup(self.embeddings, word_inputs)\n",
    "        output, self.lstm_state = self.lstm(hidden, self.lstm_state)\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
