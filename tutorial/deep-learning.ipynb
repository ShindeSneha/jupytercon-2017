{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to represent words for NLP\n",
    "* motivator on why bag of words/ one hot encoding is bad\n",
    "    * curse of dimensionality, sparsity, ignores context, new words, etc.\n",
    "* Word vectors\n",
    "    * distributional hypothesis \n",
    "        * describing the landscape of models as using different types of \"context\"   \n",
    "        * count and predictive approachs [<a href=\"#note1\">note.</a>]\n",
    "        * larger context: semantic relatedness (e.g. “boat” – “water”)\n",
    "        * smaller context: semantic similarity (e.g. “boat” – “ship”)\n",
    "    * quick overview on methods\n",
    "    * SVD on doc/word matrices\n",
    "    * SVD on co-occurance matrices with window\n",
    "    \n",
    "    * some issues:\n",
    "        * large matrices!\n",
    "        * expensive to SVD (quadratic time)\n",
    "        * Sparse\n",
    "    * Glove\n",
    "    * word2vec: make word vectors the parameters of a model with the objective of defining local context.\n",
    "    * go over word2vec in a little more detail\n",
    "        * skip gram\n",
    "        * cbow\n",
    "        * negative sampling\n",
    "    * word embeddings in python:\n",
    "        * sklearn/pydsm + numpy (vectorizers + matrix decompositions)\n",
    "        * gensim (word2vec)\n",
    "* Neural models \n",
    "\n",
    "        \n",
    "        \n",
    "* Inspecting results of word embeddings:\n",
    "    * self organizing maps\n",
    "* Validating word vectors:\n",
    "    * intrinsic vs extrinsic\n",
    "    \n",
    "* A note on NNs:\n",
    "    * transferable features in shallow parts of a network, theres an analogy their with word2vec (shallow networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Train your own word2vec model using dataset, and load those vectors into spacy. Visually inspect the results of the vector as a self organizing map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim >> gensim-log.txt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups()\n",
    "corpus = dataset.data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install spacy >> spacy-install.log\n",
    "#!python -m spacy download en >> spacy-download.log\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_)\n",
    "\n",
    "def process_token(token):\n",
    "    if token.is_punct or token.is_space:\n",
    "        return False\n",
    "    elif token.like_url:\n",
    "        return'URL'\n",
    "    elif token.like_email:\n",
    "        return'EMAIL'\n",
    "    elif token.like_num:\n",
    "        return'NUM'\n",
    "    else:\n",
    "        return token.lower_.replace(\" \",'')\n",
    "    \n",
    "    \n",
    "def process_sentence(tokenized_sent):\n",
    "    tokens = []\n",
    "    \n",
    "    doc = Doc(nlp.vocab, words = tokenized_sent)\n",
    "    nlp.tagger(doc)\n",
    "    matcher(doc)\n",
    "    \n",
    "    for token in doc:\n",
    "        processed_token = process_token(token)\n",
    "        if processed_token:\n",
    "            tokens.append(processed_token)    \n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, nlp=None):\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        \n",
    "    def __call__(self, corpus):\n",
    "        for doc in self.nlp.pipe(corpus, parse=False):\n",
    "            for ent in doc.ents:\n",
    "                ent.merge()\n",
    "            yield from map(self.process_token, doc)\n",
    "\n",
    "            \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return'URL'\n",
    "        elif token.like_email:\n",
    "            return'EMAIL'\n",
    "        elif token.like_num:\n",
    "            return'NUM'\n",
    "        else:\n",
    "            return token.lower_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = TextProcesser(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = T(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_sents, ###tokenized senteces, list of list of strings\n",
    "                 size=300,  #size of embedding vectors\n",
    "                 workers=8, #how many threads?\n",
    "                 min_count=5, #minimum number of token instances to be considered\n",
    "                 sample=0, #weight of downsampling common words? \n",
    "                 sg = 0, #should we use skip-gram? if 0, then cbow\n",
    "                 hs=0, #heirarchical softmax?\n",
    "                 iter=5 #training epocs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<p id=\"note1\">\n",
    "turns out the distinction may not be that important. (see [Levy and Goldberg (2014), Pennington et al. (2014), Österlund et al. (2015)] as referenced in https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model outline\n",
    "#### Definitions phrase\n",
    "* Decide on an architecture\n",
    "* Define all variables as tensors\n",
    "* Define how to generate outputs from your inputs and variables\n",
    "* Define a cost function with respect to your predictions and you labels\n",
    "* Define an optimizer that minimizes your cost function\n",
    "#### Execution phase\n",
    "* create an execution session\n",
    "* Initialize your variables\n",
    "* over n epochs, run the optimizer, feeding it some data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_vector(shape):\n",
    "    init_vals = tf.random_normal(shape)\n",
    "    return tf.Variable(init_vals)\n",
    "\n",
    "def feed_forward(X, weights):\n",
    "    hidden = tf.nn.relu(tf.matmul(X, weights['weights1']) + weights['bias1'])\n",
    "    output = tf.matmul(hidden, weights['weights2']) + weights['bias2']\n",
    "    return tf.nn.softmax(tf.nn.sigmoid(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rows = X.shape[0]\n",
    "input_dim = X.shape[1]\n",
    "hidden_1_dim = 100\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "weights = {\n",
    "    'weights1':init_vector((input_dim,hidden_1_dim )), \n",
    "    'bias1':init_vector((hidden_1_dim, ))\n",
    "    'weights2':init_vector((hidden_1_dim,n_classes )), \n",
    "    'bias2':init_vector((n_classes, ))    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
