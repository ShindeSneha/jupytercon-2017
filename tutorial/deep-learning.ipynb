{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section:\n",
    "* Thoughts on word embeddings as a means of representing text\n",
    "* Integrating your embeddings into SpaCy\n",
    "* Thoughts on LSTMs for classification\n",
    "* Integrating your classifier into SpaCy\n",
    "* Visualizing your embeddings[if time permits]\n",
    "* Visualizing your model[if time permits]\n",
    "\n",
    "\n",
    "### How to represent words for NLP\n",
    "* motivator on why bag of words/ one hot encoding is bad\n",
    "    * curse of dimensionality, sparsity, ignores context, new words, etc.\n",
    "* Word vectors\n",
    "    * distributional hypothesis \n",
    "        * describing the landscape of models as using different types of \"context\"   \n",
    "        * count and predictive approachs [<a href=\"#note1\">note.</a>]\n",
    "        * larger context: semantic relatedness (e.g. “boat” – “water”)\n",
    "        * smaller context: semantic similarity (e.g. “boat” – “ship”)\n",
    "    * quick overview on methods\n",
    "    * SVD on doc/word matrices\n",
    "    * SVD on co-occurance matrices with window\n",
    "    \n",
    "    * some issues:\n",
    "        * large matrices!\n",
    "        * expensive to SVD (quadratic time)\n",
    "        * Sparse\n",
    "    * Glove\n",
    "    * word2vec: make word vectors the parameters of a model with the objective of defining local context.\n",
    "    * go over word2vec in a little more detail\n",
    "        * skip gram\n",
    "        * cbow\n",
    "        * negative sampling\n",
    "    * word embeddings in python:\n",
    "        * sklearn/pydsm + numpy (vectorizers + matrix decompositions)\n",
    "        * gensim (word2vec)\n",
    "* Neural models \n",
    "\n",
    "        \n",
    "        \n",
    "* Inspecting results of word embeddings:\n",
    "    * self organizing maps\n",
    "* Validating word vectors:\n",
    "    * intrinsic vs extrinsic\n",
    "    \n",
    "* A note on NNs:\n",
    "    * transferable features in shallow parts of a network, theres an analogy their with word2vec (shallow networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Train your own word2vec model using dataset, and load those vectors into spacy. Visually inspect the results of the vector as a self organizing map.\n",
    "\n",
    "#### Gotchas and notes:\n",
    "* Due to the way the binary vector files are read and written, only single word lexemes can be mapped to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy >> ../spacy-install.log\n",
    "#!python -m spacy download en >> ../spacy-download.log\n",
    "#!pip install gensim >> ../gensim-log.txt\n",
    "#!pip install tensorflow >> ../tensorflow-log.txt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "dataset = fetch_20newsgroups()\n",
    "corpus = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install skater >> skaterlog.txt\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from skater.util.progressbar import ProgressBar\n",
    "        \n",
    "class TextProcesser(object):\n",
    "    \n",
    "    def __init__(self, nlp=None):\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        \n",
    "    def __call__(self, corpus, merge_ents=False, matcher=None):\n",
    "        p = ProgressBar(len(corpus))\n",
    "        for doc in self.nlp.pipe(corpus, parse=False, tag=bool(merge_ents)):\n",
    "            p.animate()\n",
    "            if matcher:\n",
    "                matcher(doc)\n",
    "            if merge_ents:\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge()\n",
    "            yield list(map(self.process_token, doc))\n",
    "            \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return 'URL'\n",
    "        elif token.like_email:\n",
    "            return 'EMAIL'\n",
    "        elif token.like_num:\n",
    "            return 'NUM'\n",
    "        else:\n",
    "            return token.lower_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] iterations ████████████████████ Time elapsed: 0 seconds[['lets', 'go', 'to', 'new', 'york', '.']]\n"
     ]
    }
   ],
   "source": [
    "from spacy.attrs import LOWER\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_entity('Twitter')\n",
    "matcher.add_pattern(\"Twitter\", [{LOWER: \"twitter\"},])\n",
    "processor = TextProcesser(nlp=nlp)\n",
    "print(list(processor([\"Lets go to New York.\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11314/11314] iterations ████████████████████ Time elapsed: 62 seconds"
     ]
    }
   ],
   "source": [
    "processed_sents = list(processor(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_sents, ###tokenized senteces, list of list of strings\n",
    "                 size=250,  #size of embedding vectors\n",
    "                 workers=8, #how many threads?\n",
    "                 min_count=5, #minimum number of token instances to be considered\n",
    "                 sample=0, #weight of downsampling common words? \n",
    "                 sg = 0, #should we use skip-gram? if 0, then cbow\n",
    "                 hs=0, #heirarchical softmax?\n",
    "                 iter=5 #training epocs\n",
    "        )\n",
    "with open('gensim-model.pkl', 'wb') as model_file:\n",
    "    model.save(model_file)\n",
    "    \n",
    "    \n",
    "#load vectors here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0.639094</td>\n",
       "      <td>0.538254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>0.549838</td>\n",
       "      <td>0.523014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>0.532845</td>\n",
       "      <td>0.512542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.210933</td>\n",
       "      <td>0.173088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             custom   default\n",
       "agency     0.639094  0.538254\n",
       "congress   0.549838  0.523014\n",
       "president  0.532845  0.512542\n",
       "sport      0.210933  0.173088"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sims = {}\n",
    "test_word = 'government'\n",
    "for word in ['president', 'congress', 'sport', 'agency']:\n",
    "    custom_similarity = model.similarity(test_word, word)\n",
    "    spacy_similarity = nlp(test_word).similarity(nlp(word))\n",
    "    sims[word] = {'default': spacy_similarity, 'custom':custom_similarity}\n",
    "pd.DataFrame(sims).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import spacy\n",
    "#write to disk\n",
    "vector_file = 'custom-vectors.word2vec'\n",
    "model.wv.save_word2vec_format(str(vector_file), binary=False)\n",
    "\n",
    "\n",
    "#delete first 2 lines; contains non-decodeable stuff\n",
    "with open(str(vector_file), 'rb') as f:\n",
    "    data = f.readlines()\n",
    "f.close()\n",
    "with open(str(vector_file), 'wb') as f:\n",
    "    f.writelines(data)\n",
    "    \n",
    "\n",
    "#compress via bz2\n",
    "compressed_filename = vector_file + '.bz2'\n",
    "z = bz2.compress(open(vector_file, 'rb').read())\n",
    "with open(compressed_filename, 'wb') as out:\n",
    "    out.write(z)\n",
    "spacy.vocab.write_binary_vectors('custom-vectors.word2vec.bz2', 'custom-vectors.bin')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_new = spacy.load('en')\n",
    "nlp_new.vocab.load_vectors_from_bin_loc('custom-vectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agency</th>\n",
       "      <td>0.639094</td>\n",
       "      <td>0.639094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>0.549838</td>\n",
       "      <td>0.549838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>0.532845</td>\n",
       "      <td>0.532845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.210933</td>\n",
       "      <td>0.210933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             custom   default\n",
       "agency     0.639094  0.639094\n",
       "congress   0.549838  0.549838\n",
       "president  0.532845  0.532845\n",
       "sport      0.210933  0.210933"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sims = {}\n",
    "test_word = 'government'\n",
    "for word in ['president', 'congress', 'sport', 'agency']:\n",
    "    custom_similarity = model.similarity(test_word, word)\n",
    "    spacy_similarity = nlp(test_word).similarity(nlp_new(word))\n",
    "    sims[word] = {'default': spacy_similarity, 'custom':custom_similarity}\n",
    "pd.DataFrame(sims).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tailoring the word embeddings can improve classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X1 = np.array(list(map(lambda doc: doc.vector, nlp.pipe(corpus, tag=False, parse=False))))\n",
    "X2 = np.array(list(map(lambda doc: doc.vector, nlp_new.pipe(corpus, tag=False, parse=False))))\n",
    "y = dataset.target\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, test_size=.3)\n",
    "svm1 = SVC()\n",
    "svm1.fit(X1_train, y_train)\n",
    "\n",
    "svm2 = SVC()\n",
    "svm2.fit(X2_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "preds1 = svm1.predict(X1_test)\n",
    "preds2 = svm2.predict(X2_test)\n",
    "\n",
    "print(accuracy_score(y_test, preds1))\n",
    "print(accuracy_score(y_test, preds2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorBoard b'54' at http://09abb013e4c7:6006\n",
      "(Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=/home/jupyter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "LOG_DIR = '../../'\n",
    "\n",
    "embedding_var = tf.Variable(initial_value=model.syn1neg, trainable=False)\n",
    "with tf.Session() as s:\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    r = s.run(embedding_var)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(s, os.path.join(LOG_DIR, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "class SOM(object):\n",
    "    \"\"\"\n",
    "    2-D Self-Organizing Map with Gaussian Neighbourhood function\n",
    "    and linearly decreasing learning rate.\n",
    "    \"\"\"\n",
    " \n",
    "    #To check if the SOM has been trained\n",
    "    _trained = False\n",
    " \n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Initializes all necessary components of the TensorFlow\n",
    "        Graph.\n",
    " \n",
    "        m X n are the dimensions of the SOM. 'n_iterations' should\n",
    "        should be an integer denoting the number of iterations undergone\n",
    "        while training.\n",
    "        'dim' is the dimensionality of the training inputs.\n",
    "        'alpha' is a number denoting the initial time(iteration no)-based\n",
    "        learning rate. Default value is 0.3\n",
    "        'sigma' is the the initial neighbourhood value, denoting\n",
    "        the radius of influence of the BMU while training. By default, its\n",
    "        taken to be half of max(m, n).\n",
    "        \"\"\"\n",
    " \n",
    "        #Assign required variables first\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    " \n",
    "        ##INITIALIZE GRAPH\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        ##POPULATE GRAPH WITH NECESSARY COMPONENTS\n",
    "        with self._graph.as_default():\n",
    " \n",
    "            ##VARIABLES AND CONSTANT OPS FOR DATA STORAGE\n",
    " \n",
    "            #Randomly initialized weightage vectors for all neurons,\n",
    "            #stored together as a matrix Variable of size [m*n, dim]\n",
    "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            #Matrix of size [m*n, 2] for SOM grid locations\n",
    "            #of neurons\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "            ##PLACEHOLDERS FOR TRAINING INPUTS\n",
    "            #We need to assign them as attributes to self, since they\n",
    "            #will be fed in during training\n",
    " \n",
    "            #The training vector\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            #Iteration number\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            ##CONSTRUCT TRAINING OP PIECE BY PIECE\n",
    "            #Only the final, 'root' training op needs to be assigned as\n",
    "            #an attribute to self, since all the rest will be executed\n",
    "            #automatically during training\n",
    " \n",
    "            #To compute the Best Matching Unit given a vector\n",
    "            #Basically calculates the Euclidean distance between every\n",
    "            #neuron's weightage vector and the input, and returns the\n",
    "            #index of the neuron which gives the least value\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            #This will extract the location of the BMU based on the BMU's\n",
    "            #index\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]))),\n",
    "                                 [2])\n",
    " \n",
    "            #To compute the alpha and sigma values based on iteration\n",
    "            #number\n",
    "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
    "                                                  self._n_iterations))\n",
    "            _alpha_op = tf.multiply(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.multiply(sigma, learning_rate_op)\n",
    " \n",
    "            #Construct the op that will generate a vector with learning\n",
    "            #rates for all neurons, based on iteration number and location\n",
    "            #wrt BMU.\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                self._location_vects, tf.stack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            #Finally, the op that will use learning_rate_op to update\n",
    "            #the weightage vectors of all neurons based on a particular\n",
    "            #input\n",
    "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.multiply(\n",
    "                learning_rate_multiplier,\n",
    "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects))                                         \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            ##INITIALIZE SESSION\n",
    "            self._sess = tf.Session()\n",
    " \n",
    "            ##INITIALIZE VARIABLES\n",
    "            init_op = tf.initialize_all_variables()\n",
    "            self._sess.run(init_op)\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        \"\"\"\n",
    "        Yields one by one the 2-D locations of the individual neurons\n",
    "        in the SOM.\n",
    "        \"\"\"\n",
    "        #Nested iterations over both dimensions\n",
    "        #to generate all 2-D locations in the map\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM.\n",
    "        Current weightage vectors for all neurons(initially random) are\n",
    "        taken as starting conditions for training.\n",
    "        \"\"\"\n",
    " \n",
    "        #Training iterations\n",
    "        for iter_no in range(self._n_iterations):\n",
    "            #Train with each vector one by one\n",
    "            for input_vect in input_vects:\n",
    "                self._sess.run(self._training_op,\n",
    "                               feed_dict={self._vect_input: input_vect,\n",
    "                                          self._iter_input: iter_no})\n",
    " \n",
    "        #Store a centroid grid for easy retrieval later on\n",
    "        centroid_grid = [[] for i in range(self._m)]\n",
    "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "        self._locations = list(self._sess.run(self._location_vects))\n",
    "        for i, loc in enumerate(self._locations):\n",
    "            centroid_grid[loc[0]].append(self._weightages[i])\n",
    "        self._centroid_grid = centroid_grid\n",
    " \n",
    "        self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        \"\"\"\n",
    "        Returns a list of 'm' lists, with each inner list containing\n",
    "        the 'n' corresponding centroid locations as 1-D NumPy arrays.\n",
    "        \"\"\"\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        \"\"\"\n",
    "        Maps each input vector to the relevant neuron in the SOM\n",
    "        grid.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM.\n",
    "        Returns a list of 1-D NumPy arrays containing (row, column)\n",
    "        info for each input vector(in the same order), corresponding\n",
    "        to mapped neuron.\n",
    "        \"\"\"\n",
    " \n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "colors = np.array(\n",
    "     [[0., 0., 0.],\n",
    "      [0., 0., 1.],\n",
    "      [0., 0., 0.5],\n",
    "      [0.125, 0.529, 1.0],\n",
    "      [0.33, 0.4, 0.67],\n",
    "      [0.6, 0.5, 1.0],\n",
    "      [0., 1., 0.],\n",
    "      [1., 0., 0.],\n",
    "      [0., 1., 1.],\n",
    "      [1., 0., 1.],\n",
    "      [1., 1., 0.],\n",
    "      [1., 1., 1.],\n",
    "      [.33, .33, .33],\n",
    "      [.5, .5, .5],\n",
    "      [.66, .66, .66]])\n",
    "color_names = \\\n",
    "    ['black', 'blue', 'darkblue', 'skyblue',\n",
    "     'greyblue', 'lilac', 'green', 'red',\n",
    "     'cyan', 'violet', 'yellow', 'white',\n",
    "     'darkgrey', 'mediumgrey', 'lightgrey']\n",
    "    \n",
    "som = SOM(20, 30, 3, 400)\n",
    "som.train(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get output grid\n",
    "image_grid = som.get_centroids()\n",
    " \n",
    "#Map colours to their closest neurons\n",
    "mapped = som.map_vects(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7b0b02668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plot\n",
    "plt.imshow(image_grid)\n",
    "plt.title('Color SOM')\n",
    "for i, m in enumerate(mapped):\n",
    "    plt.text(m[1], m[0], color_names[i], ha='center', va='center',\n",
    "             bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<p id=\"note1\">\n",
    "turns out the distinction may not be that important. (see [Levy and Goldberg (2014), Pennington et al. (2014), Österlund et al. (2015)] as referenced in https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model outline\n",
    "#### Definitions phrase\n",
    "* Decide on an architecture\n",
    "* Define all variables as tensors\n",
    "* Define how to generate outputs from your inputs and variables\n",
    "* Define a cost function with respect to your predictions and you labels\n",
    "* Define an optimizer that minimizes your cost function\n",
    "#### Execution phase\n",
    "* create an execution session\n",
    "* Initialize your variables\n",
    "* over n epochs, run the optimizer, feeding it some data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchFeeder(object):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.i = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            yield self.__iter__(self)\n",
    "    def __iter__(self):\n",
    "        X = self.X[self.i:self.i + self.batch_size]\n",
    "        y = self.y[self.i:self.i + self.batch_size]\n",
    "        self.i += self.batch_size\n",
    "        return X, y\n",
    "        \n",
    "    def __next__(self):\n",
    "        return self.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, X, y, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.input_dim = X_train.shape[1]\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.input_dim))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.weights = {\n",
    "                    'weights1':tf.get_variable('weights1', (self.input_dim,self.hidden_dim ), dtype=self.default_dtype), \n",
    "                    'bias1':tf.get_variable('bias1', (self.hidden_dim, ), dtype=self.default_dtype), \n",
    "                    'weights2':tf.get_variable('weights2', (self.hidden_dim, self.n_classes ), dtype=self.default_dtype), \n",
    "                    'bias2':tf.get_variable('bias2', (self.n_classes, ), dtype=self.default_dtype)}\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, x_input, weights):\n",
    "        hidden = tf.matmul(x_input, weights['weights1'])\n",
    "        hidden = tf.add(hidden, weights['bias1'])\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        output = tf.matmul(hidden, weights['weights2'])\n",
    "        output = tf.add(output, weights['bias2'])\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = MLP(X, y)\n",
    "c.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.chkpt\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.91      0.93        67\n",
      "          1       0.94      0.97      0.96       104\n",
      "\n",
      "avg / total       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.argmax(y_test, axis=1), c.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.929648241206\n",
      "Test:  0.888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "gb = MLPClassifier(hidden_layer_sizes=(1000, ))\n",
    "gb.fit(X_train, np.argmax(y_train, axis=1))\n",
    "print(\"Train: \", np.mean(gb.predict(X_train) == np.argmax(y_train, axis=1)))\n",
    "print(\"Test: \", np.mean(gb.predict(X_test) == np.argmax(y_test, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Sequence modeling\n",
    "* Preprocessing\n",
    "* Create data structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim >> gensim-log.txt\n",
    "#!pip install spacy >> spacy-log.txt\n",
    "#!python -m spacy download en >> spacy-download.txt\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 0\n",
    "        self.MISSING_VAL = 1\n",
    "        self.START_VAL = 2\n",
    "        self.END_VAL = 3\n",
    "        self.INDEX_OFFSET = 4\n",
    "        self.vocab = OrderedDict()\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj), 0)\n",
    "        result = obj[:self.max_len] + [self.PADDING_VAL] * n_pads\n",
    "        result[-1] = self.END_VAL\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def check_word(self, word):\n",
    "        current_vocab_size = self.get_current_vocab_size() # 0\n",
    "        if current_vocab_size <= self.max_vocab_size:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab.update({word: current_vocab_size + self.INDEX_OFFSET}) #{'apple': 0}\n",
    "        try:\n",
    "            return self.vocab[word]\n",
    "        except KeyError:\n",
    "            return self.MISSING_VAL\n",
    "        \n",
    "    def __call__(self, corpus, merge_ents=True):\n",
    "        docs = []\n",
    "        if merge_ents:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False):\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge()\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                tokens = [self.START_VAL] + tokens + [self.END_VAL]\n",
    "                docs.append(self.padder(tokens))\n",
    "        else:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False, tag=False, entity=False):\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                docs.append(self.padder(tokens))\n",
    "        \n",
    "        return docs\n",
    "  \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return self.check_word(\"URL\")\n",
    "        elif token.like_email:\n",
    "            return self.check_word(\"EMAIL\")\n",
    "        elif token.like_num:\n",
    "            return self.check_word(\"NUM\")\n",
    "        else:\n",
    "            return self.check_word(token.lower_)\n",
    "\n",
    "\n",
    "#nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset = fetch_20newsgroups()\n",
    "#corpus = dataset.data\n",
    "processor = TextProcesser(nlp=nlp, max_len=100)\n",
    "processed_corpus = processor(corpus, merge_ents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, X, y, max_seq_len, lstm_size=1, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.max_seq_len = max_seq_len or () #len of X\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        self.vocab_size\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.max_seq_len))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.embeddings = tf.Variable(shape=(self.vocab_size, self.hidden_dim))\n",
    "                self.lstm_state = tf.zeros([batch_size, lstm.state_size])\n",
    "                self.lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, word_input):\n",
    "        hidden = tf.nn.embedding_lookup(self.embeddings, word_inputs)\n",
    "        output, self.lstm_state = self.lstm(hidden, self.lstm_state)\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class LSTM(object):\n",
    "    def __init__(self, X, y, max_seq_len, lstm_size=1, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.max_seq_len = max_seq_len or () #len of X\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        self.vocab_size\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.max_seq_len))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.embeddings = tf.Variable(shape=(self.vocab_size, self.hidden_dim))\n",
    "                self.lstm_state = tf.zeros([batch_size, lstm.state_size])\n",
    "                self.lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, word_input):\n",
    "        hidden = tf.nn.embedding_lookup(self.embeddings, word_inputs)\n",
    "        output, self.lstm_state = self.lstm(hidden, self.lstm_state)\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
