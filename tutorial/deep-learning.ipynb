{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model outline\n",
    "#### Definitions phrase\n",
    "* Decide on an architecture\n",
    "* Define all variables as tensors\n",
    "* Define how to generate outputs from your inputs and variables\n",
    "* Define a cost function with respect to your predictions and you labels\n",
    "* Define an optimizer that minimizes your cost function\n",
    "#### Execution phase\n",
    "* create an execution session\n",
    "* Initialize your variables\n",
    "* over n epochs, run the optimizer, feeding it some data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, X, y, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.input_dim = X_train.shape[1]\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.input_dim))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.weights = {\n",
    "                    'weights1':tf.get_variable('weights1', (self.input_dim,self.hidden_dim ), dtype=self.default_dtype), \n",
    "                    'bias1':tf.get_variable('bias1', (self.hidden_dim, ), dtype=self.default_dtype), \n",
    "                    'weights2':tf.get_variable('weights2', (self.hidden_dim, self.n_classes ), dtype=self.default_dtype), \n",
    "                    'bias2':tf.get_variable('bias2', (self.n_classes, ), dtype=self.default_dtype)}\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, x_input, weights):\n",
    "        hidden = tf.matmul(x_input, weights['weights1'])\n",
    "        hidden = tf.add(hidden, weights['bias1'])\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        output = tf.matmul(hidden, weights['weights2'])\n",
    "        output = tf.add(output, weights['bias2'])\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.chkpt\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.99      0.79        70\n",
      "          1       0.98      0.64      0.78       101\n",
      "\n",
      "avg / total       0.85      0.78      0.78       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)            \n",
    "c = MLP(X, y)\n",
    "c.fit()\n",
    "print(classification_report(np.argmax(y_test, axis=1), c.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.927135678392\n",
      "Test:  0.87134502924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "gb = MLPClassifier(hidden_layer_sizes=(1000, ))\n",
    "gb.fit(X_train, np.argmax(y_train, axis=1))\n",
    "print(\"Train: \", np.mean(gb.predict(X_train) == np.argmax(y_train, axis=1)))\n",
    "print(\"Test: \", np.mean(gb.predict(X_test) == np.argmax(y_test, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim >> gensim-log.txt\n",
    "#!pip install spacy >> spacy-log.txt\n",
    "#!python -m spacy download en >> spacy-download.txt\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 0\n",
    "        self.MISSING_VAL = 1\n",
    "        self.START_VAL = 2\n",
    "        self.END_VAL = 3\n",
    "        self.INDEX_OFFSET = 4\n",
    "        self.vocab = OrderedDict()\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj), 0)\n",
    "        result = obj[:self.max_len] + [self.PADDING_VAL] * n_pads\n",
    "        result[-1] = self.END_VAL\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def check_word(self, word):\n",
    "        current_vocab_size = self.get_current_vocab_size() # 0\n",
    "        if current_vocab_size <= self.max_vocab_size:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab.update({word: current_vocab_size + self.INDEX_OFFSET}) #{'apple': 0}\n",
    "        try:\n",
    "            return self.vocab[word]\n",
    "        except KeyError:\n",
    "            return self.MISSING_VAL\n",
    "        \n",
    "    def __call__(self, corpus, merge_ents=True):\n",
    "        docs = []\n",
    "        if merge_ents:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False):\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge()\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                tokens = [self.START_VAL] + tokens + [self.END_VAL]\n",
    "                docs.append(self.pad(tokens))\n",
    "        else:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False, tag=False, entity=False):\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                docs.append(self.pad(tokens))\n",
    "        \n",
    "        return docs\n",
    "  \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return self.check_word(\"URL\")\n",
    "        elif token.like_email:\n",
    "            return self.check_word(\"EMAIL\")\n",
    "        elif token.like_num:\n",
    "            return self.check_word(\"NUM\")\n",
    "        else:\n",
    "            return self.check_word(token.lower_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en')\n",
    "dataset = fetch_20newsgroups()\n",
    "corpus = dataset.data\n",
    "processor = TextProcesser(nlp=nlp, max_len=100)\n",
    "processed_corpus = processor(corpus, merge_ents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, X, y, max_seq_len, word_embedding_size=300):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.max_seq_len = max_seq_len or () #len of X\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        self.vocab_size\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('rnn_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.max_seq_len))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                \n",
    "                self.hidden_state = self.initial_state = tf.Variable(shape=(self.initial_state))\n",
    "                \n",
    "                #grab the vector associated with a given word\n",
    "                self.U = tf.Variable(shape=(self.vocab_size, word_embedding_size))\n",
    "                \n",
    "                # tranform the previous state\n",
    "                self.W = tf.Variable(shape=(word_embedding_size, word_embedding_size))\n",
    "                \n",
    "                #the bias\n",
    "                self.bias1 = tf.Variable(shape= (word_embedding_size, ))\n",
    "                \n",
    "                \n",
    "                self.get_logit_op = self.feed_forward(self.x_input)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, word_input):\n",
    "        current_word_vector = tf.nn.embedding_lookup(self.U, word_inputs)\n",
    "        previous_state_vector = \n",
    "        output, self.lstm_state = self.lstm(hidden, self.lstm_state)\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.contrib.learn.run_n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
