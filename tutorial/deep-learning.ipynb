{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model outline\n",
    "#### Definitions phrase\n",
    "* Decide on an architecture\n",
    "* Define all variables as tensors\n",
    "* Define how to generate outputs from your inputs and variables\n",
    "* Define a cost function with respect to your predictions and you labels\n",
    "* Define an optimizer that minimizes your cost function\n",
    "#### Execution phase\n",
    "* create an execution session\n",
    "* Initialize your variables\n",
    "* over n epochs, run the optimizer, feeding it some data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install gensim >> ../../gensim-log.txt\n",
    "!pip install spacy >> ../../spacy-log.txt\n",
    "!python -m spacy download en >> ../../spacy-download.txt\n",
    "!pip install tensorflow >> ../../tf.log\n",
    "!pip install skater >> ../../sakterlog.txt\n",
    "#!pip install keras ../../keras.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, X, y, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.input_dim = X_train.shape[1]\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.input_dim))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.weights = {\n",
    "                    'weights1':tf.get_variable('weights1', (self.input_dim,self.hidden_dim ), dtype=self.default_dtype), \n",
    "                    'bias1':tf.get_variable('bias1', (self.hidden_dim, ), dtype=self.default_dtype), \n",
    "                    'weights2':tf.get_variable('weights2', (self.hidden_dim, self.n_classes ), dtype=self.default_dtype), \n",
    "                    'bias2':tf.get_variable('bias2', (self.n_classes, ), dtype=self.default_dtype)}\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, x_input, weights):\n",
    "        hidden = tf.matmul(x_input, weights['weights1'])\n",
    "        hidden = tf.add(hidden, weights['bias1'])\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        output = tf.matmul(hidden, weights['weights2'])\n",
    "        output = tf.add(output, weights['bias2'])\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.chkpt\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.99      0.79        70\n",
      "          1       0.98      0.64      0.78       101\n",
      "\n",
      "avg / total       0.85      0.78      0.78       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)            \n",
    "c = MLP(X, y)\n",
    "c.fit()\n",
    "print(classification_report(np.argmax(y_test, axis=1), c.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.927135678392\n",
      "Test:  0.87134502924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "gb = MLPClassifier(hidden_layer_sizes=(1000, ))\n",
    "gb.fit(X_train, np.argmax(y_train, axis=1))\n",
    "print(\"Train: \", np.mean(gb.predict(X_train) == np.argmax(y_train, axis=1)))\n",
    "print(\"Test: \", np.mean(gb.predict(X_test) == np.argmax(y_test, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim >> gensim-log.txt\n",
    "#!pip install spacy >> spacy-log.txt\n",
    "#!python -m spacy download en >> spacy-download.txt\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 0\n",
    "        self.MISSING_VAL = 1\n",
    "        self.START_VAL = 2\n",
    "        self.END_VAL = 3\n",
    "        self.INDEX_OFFSET = 4\n",
    "        self.vocab = OrderedDict()\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj) - 2, 0)\n",
    "        we_can_take = self.max_len - 2\n",
    "        result = [self.START_VAL] + obj[:we_can_take] + [self.END_VAL] + [self.PADDING_VAL] * n_pads\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def check_word(self, word):\n",
    "        current_vocab_size = self.get_current_vocab_size() # 0\n",
    "        if current_vocab_size <= self.max_vocab_size:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab.update({word: current_vocab_size + self.INDEX_OFFSET}) #{'apple': 0}\n",
    "        try:\n",
    "            return self.vocab[word]\n",
    "        except KeyError:\n",
    "            return self.MISSING_VAL\n",
    "        \n",
    "    def __call__(self, corpus, merge_ents=True):\n",
    "        docs = []\n",
    "        if merge_ents:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False):\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge()\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                docs.append(self.pad(tokens))\n",
    "        else:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False, tag=False, entity=False):\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                docs.append(self.pad(tokens))\n",
    "        \n",
    "        return docs\n",
    "  \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return self.check_word(\"URL\")\n",
    "        elif token.like_email:\n",
    "            return self.check_word(\"EMAIL\")\n",
    "        elif token.like_num:\n",
    "            return self.check_word(\"NUM\")\n",
    "        else:\n",
    "            return self.check_word(token.lower_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 100)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "#nlp = spacy.load('en')\n",
    "#dataset = fetch_20newsgroups()\n",
    "#corpus = dataset.data\n",
    "processor = TextProcesser(nlp=nlp, max_len=100)\n",
    "processed_corpus = processor(corpus, merge_ents=False)\n",
    "\n",
    "X = np.array(processed_corpus)\n",
    "#y = label_binarize(dataset.target, classes=dataset.target_names)\n",
    "y = dataset.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import tensorflow as tf\n",
    "from skater.util.progressbar import ProgressBar\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, word_embedding_size=300, state_size=300, lstm_layers=2, keep_prob=.9):\n",
    "        \n",
    "        \n",
    "        # X data is stored as [n_documents_per_batch, seq_length_per_doc]\n",
    "        # each batch will increment row index by batch_size, and grab associated docs.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.n_classes = len(set(np.unique(y_train)).union(set(np.unique(y_test))))\n",
    "        self.n_docs = self.X_train.shape[0]\n",
    "        self.state_size = state_size\n",
    "        self.word_embedding_size = word_embedding_size        \n",
    "        self.seq_len = X_train.shape[1]\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float32\n",
    "        self.vocab_size = max(X_train.max(), X_test.max())\n",
    "\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('rnn_model') as scope:\n",
    "                \n",
    "                self.learning_rate = tf.Variable(0.001, dtype=tf.float32, trainable=False)\n",
    "                #a single document\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.seq_len), name='documents')\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (1, ), name='labels')\n",
    "                #self.states = []\n",
    "\n",
    "                #self.initial_state = tf.Variable(shape=(self.n_docs, self.state_size))\n",
    "                \n",
    "                #grab the vector associated with a given word\n",
    "                self.embedding = tf.Variable(tf.random_normal((self.vocab_size, word_embedding_size)))\n",
    "                self.embed = tf.nn.embedding_lookup(self.embedding, self.x_input)\n",
    "                \n",
    "                #lstm cell\n",
    "                self.lstm = tf.contrib.rnn.BasicLSTMCell(self.state_size)                \n",
    "                self.dropout = tf.contrib.rnn.DropoutWrapper(self.lstm, output_keep_prob=keep_prob)\n",
    "                self.cell = tf.contrib.rnn.MultiRNNCell([self.dropout] * lstm_layers)\n",
    "                self.lstm_state =  self.cell.zero_state(1, self.default_dtype)\n",
    "                self.outputs, self.final_state = tf.nn.dynamic_rnn(self.cell, self.embed, initial_state=self.lstm_state)                \n",
    "                self.predictions = tf.contrib.layers.fully_connected(self.outputs[:, -1], \n",
    "                                                                     self.n_classes, \n",
    "                                                                     activation_fn=tf.nn.softmax,\n",
    "                                                                     biases_initializer = tf.random_uniform_initializer()\n",
    "                                                                    )\n",
    "\n",
    "                self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y_output, \n",
    "                                                                                          logits=self.predictions))\n",
    "                self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "                # tranform the previous state\n",
    "                # self.W = tf.Variable(tf.random_normal((word_embedding_size, word_embedding_size)))\n",
    "                \n",
    "                #the first bias\n",
    "                # self.bias1 = tf.Variable(tf.random_normal((word_embedding_size, )))\n",
    "                \n",
    "                # transform the outputs to classes\n",
    "                # self.V = tf.Variable(tf.random_normal((word_embedding_size, self.n_classes)))\n",
    "                \n",
    "                # the second bias\n",
    "                # self.bias2 = tf.Variable(tf.zeros(self.n_classes, ))                \n",
    "                \n",
    "                # self.get_logit_op = self.feed_forward(self.x_input)\n",
    "                #self.predict_proba_op = tf.softmax(self.get_logit_op)\n",
    "                #self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                \n",
    "                #self.logit_ops = [tf.matmul(state, W2) + b2 for state in self.states] \n",
    "                #self.predictions_ops = [tf.nn.softmax(logits) for logits in self.logit_ops]\n",
    "                #self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                #self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, document):\n",
    "        current_state = tf.zeros([self.seq_len, self.state_size])\n",
    "        for word in tf.unstack(tf.transpose(document)):\n",
    "            current_word_vector = tf.nn.embedding_lookup(self.embedding, word)\n",
    "            output, current_state = self.lstm(current_word_vector, current_state)\n",
    "            \n",
    "        return tf.matmul(output, self.V) + self.bias2\n",
    "#             previous_state_vector = tf.matmul(self.W, self.hidden_state)\n",
    "#             hidden1 = tf.add(tf.add(self.bias, previous_state_vector), current_word_vector)\n",
    "#             hidden1 = tf.nn.tanh(hidden1)\n",
    "\n",
    "#             #set prev_state to this intermediate vector\n",
    "#             self.hidden_state = hidden1\n",
    "\n",
    "#             transormed_current_state = tf.matmul(self.V, hidden1)\n",
    "\n",
    "#             output = tf.add(self.bias2, transormed_current_state)\n",
    "        \n",
    "        return output\n",
    "                                           \n",
    "    def get_batches(self, x, y, batch_size=100):\n",
    "\n",
    "        n_batches = len(x)//batch_size\n",
    "        x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "        for ii in range(0, len(x), batch_size):\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predictions, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        p1 = ProgressBar(epochs)\n",
    "        epochs = range(epochs)\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "        \n",
    "            for epoch in epochs:\n",
    "                p2 = ProgressBar(self.n_docs)\n",
    "                p1.animate()\n",
    "                initial_state = sess.run(self.lstm_state)\n",
    "                for idx, (x, y) in enumerate(self.get_batches(self.X_train, self.y_train, 1)):                               \n",
    "                    p2.animate()\n",
    "                    sess.run(self.optimize, feed_dict={self.x_input: x, \n",
    "                                                        self.y_output: y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[971/7919] iterations ██------------------ Time elapsed: 340 secondsds"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-43e125029c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-08105e941e63>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     sess.run(self.optimize, feed_dict={self.x_input: x, \n\u001b[0;32m--> 135\u001b[0;31m                                                         self.y_output: y})\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn = RNN(X_train, y_train, X_test, y_test, word_embedding_size=300, state_size=300)\n",
    "rnn.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20004"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b533030167bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-7ea5bb885a5b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, y, max_seq_len, word_embedding_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_len\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#len of X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model.chkpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_size' is not defined"
     ]
    }
   ],
   "source": [
    "RNN(X, y, 100, word_embedding_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
