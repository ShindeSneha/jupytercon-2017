{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to represent words for NLP\n",
    "* motivator on why bag of words/ one hot encoding is bad\n",
    "    * curse of dimensionality, sparsity, ignores context, new words, etc.\n",
    "* Word vectors\n",
    "    * distributional hypothesis \n",
    "        * describing the landscape of models as using different types of \"context\"   \n",
    "        * count and predictive approachs [<a href=\"#note1\">note.</a>]\n",
    "        * larger context: semantic relatedness (e.g. “boat” – “water”)\n",
    "        * smaller context: semantic similarity (e.g. “boat” – “ship”)\n",
    "    * quick overview on methods\n",
    "    * SVD on doc/word matrices\n",
    "    * SVD on co-occurance matrices with window\n",
    "    \n",
    "    * some issues:\n",
    "        * large matrices!\n",
    "        * expensive to SVD (quadratic time)\n",
    "        * Sparse\n",
    "    * Glove\n",
    "    * word2vec: make word vectors the parameters of a model with the objective of defining local context.\n",
    "    * go over word2vec in a little more detail\n",
    "        * skip gram\n",
    "        * cbow\n",
    "        * negative sampling\n",
    "    * word embeddings in python:\n",
    "        * sklearn/pydsm + numpy (vectorizers + matrix decompositions)\n",
    "        * gensim (word2vec)\n",
    "* Neural models \n",
    "\n",
    "        \n",
    "        \n",
    "* Inspecting results of word embeddings:\n",
    "    * self organizing maps\n",
    "* Validating word vectors:\n",
    "    * intrinsic vs extrinsic\n",
    "    \n",
    "* A note on NNs:\n",
    "    * transferable features in shallow parts of a network, theres an analogy their with word2vec (shallow networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Train your own word2vec model using dataset, and load those vectors into spacy. Visually inspect the results of the vector as a self organizing map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim >> gensim-log.txt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups()\n",
    "corpus = dataset.data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy >> spacy-install.log\n",
    "#!python -m spacy download en >> spacy-download.log\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_)\n",
    "\n",
    "def process_token(token):\n",
    "    if token.is_punct or token.is_space:\n",
    "        return False\n",
    "    elif token.like_url:\n",
    "        return'URL'\n",
    "    elif token.like_email:\n",
    "        return'EMAIL'\n",
    "    elif token.like_num:\n",
    "        return'NUM'\n",
    "    else:\n",
    "        return token.lower_.replace(\" \",'')\n",
    "    \n",
    "    \n",
    "def process_sentence(tokenized_sent):\n",
    "    tokens = []\n",
    "    \n",
    "    doc = Doc(nlp.vocab, words = tokenized_sent)\n",
    "    nlp.tagger(doc)\n",
    "    matcher(doc)\n",
    "    \n",
    "    for token in doc:\n",
    "        processed_token = process_token(token)\n",
    "        if processed_token:\n",
    "            tokens.append(processed_token)    \n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, nlp=None):\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        \n",
    "    def __call__(self, corpus):\n",
    "        for doc in self.nlp.pipe(corpus, parse=False):\n",
    "            for ent in doc.ents:\n",
    "                ent.merge()\n",
    "            yield from map(self.process_token, doc)\n",
    "\n",
    "            \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return'URL'\n",
    "        elif token.like_email:\n",
    "            return'EMAIL'\n",
    "        elif token.like_num:\n",
    "            return'NUM'\n",
    "        else:\n",
    "            return token.lower_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = TextProcesser(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-a5debb05fe32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-b6538610b3f0>\u001b[0m in \u001b[0;36mprocess_sentence\u001b[0;34m(self, tokenized_sent)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.5/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__init__ (spacy/tokens/doc.cpp:4942)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "f = list(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object TextProcesser.process_sentence at 0x7f922e712eb8>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_sents, ###tokenized senteces, list of list of strings\n",
    "                 size=300,  #size of embedding vectors\n",
    "                 workers=8, #how many threads?\n",
    "                 min_count=5, #minimum number of token instances to be considered\n",
    "                 sample=0, #weight of downsampling common words? \n",
    "                 sg = 0, #should we use skip-gram? if 0, then cbow\n",
    "                 hs=0, #heirarchical softmax?\n",
    "                 iter=5 #training epocs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<p id=\"note1\">\n",
    "turns out the distinction may not be that important. (see [Levy and Goldberg (2014), Pennington et al. (2014), Österlund et al. (2015)] as referenced in https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/).\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
