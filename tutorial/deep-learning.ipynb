{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section:\n",
    "* Thoughts on word embeddings as a means of representing text\n",
    "* Integrating your embeddings into SpaCy\n",
    "* Thoughts on LSTMs for classification\n",
    "* Integrating your classifier into SpaCy\n",
    "* Visualizing your embeddings[if time permits]\n",
    "* Visualizing your model[if time permits]\n",
    "\n",
    "\n",
    "### How to represent words for NLP\n",
    "* motivator on why bag of words/ one hot encoding is bad\n",
    "    * curse of dimensionality, sparsity, ignores context, new words, etc.\n",
    "* Word vectors\n",
    "    * distributional hypothesis \n",
    "        * describing the landscape of models as using different types of \"context\"   \n",
    "        * count and predictive approachs [<a href=\"#note1\">note.</a>]\n",
    "        * larger context: semantic relatedness (e.g. “boat” – “water”)\n",
    "        * smaller context: semantic similarity (e.g. “boat” – “ship”)\n",
    "    * quick overview on methods\n",
    "    * SVD on doc/word matrices\n",
    "    * SVD on co-occurance matrices with window\n",
    "    \n",
    "    * some issues:\n",
    "        * large matrices!\n",
    "        * expensive to SVD (quadratic time)\n",
    "        * Sparse\n",
    "    * Glove\n",
    "    * word2vec: make word vectors the parameters of a model with the objective of defining local context.\n",
    "    * go over word2vec in a little more detail\n",
    "        * skip gram\n",
    "        * cbow\n",
    "        * negative sampling\n",
    "    * word embeddings in python:\n",
    "        * sklearn/pydsm + numpy (vectorizers + matrix decompositions)\n",
    "        * gensim (word2vec)\n",
    "* Neural models \n",
    "\n",
    "        \n",
    "        \n",
    "* Inspecting results of word embeddings:\n",
    "    * self organizing maps\n",
    "* Validating word vectors:\n",
    "    * intrinsic vs extrinsic\n",
    "    \n",
    "* A note on NNs:\n",
    "    * transferable features in shallow parts of a network, theres an analogy their with word2vec (shallow networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Train your own word2vec model using dataset, and load those vectors into spacy. Visually inspect the results of the vector as a self organizing map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim >> gensim-log.txt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups()\n",
    "corpus = dataset.data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy >> spacy-install.log\n",
    "!python -m spacy download en >> spacy-download.log\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "!pip install skater >> skaterlog.txt\n",
    "from skater.util.progressbar import ProgressBar\n",
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_)\n",
    "        \n",
    "class TextProcesser(object):\n",
    "    \n",
    "    def __init__(self, nlp=None, matcher=None):\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.matcher = matcher\n",
    "        \n",
    "    def __call__(self, corpus):\n",
    "        p = ProgressBar(len(corpus))\n",
    "        for doc in self.nlp.pipe(corpus, parse=False):\n",
    "            p.animate()\n",
    "            if self.matcher:\n",
    "                self.matcher(doc)\n",
    "            for ent in doc.ents:\n",
    "                ent.merge()\n",
    "            yield list(map(self.process_token, doc))\n",
    "            \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return 'URL'\n",
    "        elif token.like_email:\n",
    "            return 'EMAIL'\n",
    "        elif token.like_num:\n",
    "            return 'NUM'\n",
    "        else:\n",
    "            return token.lower_\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] iterations ████████████████████ Time elapsed: 0 seconds[['lets', 'go', 'to', 'new york', '.']]\n"
     ]
    }
   ],
   "source": [
    "from spacy.attrs import LOWER\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_entity('Twitter')\n",
    "matcher.add_pattern(\"Twitter\", [{LOWER: \"twitter\"},])\n",
    "processor = TextProcesser(nlp=nlp, matcher=matcher)\n",
    "print(list(processor([\"Lets go to New York.\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1460/11314] iterations ██------------------ Time elapsed: 32 seconds"
     ]
    }
   ],
   "source": [
    "processed_sents = list(processor(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_sents, ###tokenized senteces, list of list of strings\n",
    "                 size=250,  #size of embedding vectors\n",
    "                 workers=8, #how many threads?\n",
    "                 min_count=5, #minimum number of token instances to be considered\n",
    "                 sample=0, #weight of downsampling common words? \n",
    "                 sg = 0, #should we use skip-gram? if 0, then cbow\n",
    "                 hs=0, #heirarchical softmax?\n",
    "                 iter=5 #training epocs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sims = {}\n",
    "test_word = 'new york'\n",
    "for word, custom_similarity in model.most_similar(test_word):\n",
    "    spacy_similarity = nlp(test_word).similarity(nlp(word))\n",
    "    sims[word] = {'default': spacy_similarity, 'custom':custom_similarity}\n",
    "pd.DataFrame(sims).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_sents, ###tokenized senteces, list of list of strings\n",
    "                 size=300,  #size of embedding vectors\n",
    "                 workers=8, #how many threads?\n",
    "                 min_count=5, #minimum number of token instances to be considered\n",
    "                 sample=0, #weight of downsampling common words? \n",
    "                 sg = 0, #should we use skip-gram? if 0, then cbow\n",
    "                 hs=0, #heirarchical softmax?\n",
    "                 iter=5 #training epocs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import bz2\n",
    "\n",
    "#write to disk\n",
    "vector_file = 'custom-vectors.word2vec'\n",
    "model.wv.save_word2vec_format(str(vector_file), binary=False)\n",
    "\n",
    "\n",
    "#delete first 2 lines; contains non-decodeable stuff\n",
    "with open(str(vector_file), 'rb') as f:\n",
    "    data = f.readlines()\n",
    "f.close()\n",
    "with open(str(vector_file), 'wb') as f:\n",
    "    f.writelines(data[2:])\n",
    "    \n",
    "\n",
    "#compress via bz2\n",
    "compressed_filename = vector_file + '.bz2'\n",
    "z = bz2.compress(open(vector_file, 'rb').read())\n",
    "with open(compressed_filename, 'wb') as out:\n",
    "    out.write(z)\n",
    "spacy.vocab.write_binary_vectors('custom-vectors.word2vec.bz2', 'custom-vectors.bin')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<p id=\"note1\">\n",
    "turns out the distinction may not be that important. (see [Levy and Goldberg (2014), Pennington et al. (2014), Österlund et al. (2015)] as referenced in https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model outline\n",
    "#### Definitions phrase\n",
    "* Decide on an architecture\n",
    "* Define all variables as tensors\n",
    "* Define how to generate outputs from your inputs and variables\n",
    "* Define a cost function with respect to your predictions and you labels\n",
    "* Define an optimizer that minimizes your cost function\n",
    "#### Execution phase\n",
    "* create an execution session\n",
    "* Initialize your variables\n",
    "* over n epochs, run the optimizer, feeding it some data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchFeeder(object):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.i = 0\n",
    "        \n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            yield self.__iter__(self)\n",
    "    def __iter__(self):\n",
    "        X = self.X[self.i:self.i + self.batch_size]\n",
    "        y = self.y[self.i:self.i + self.batch_size]\n",
    "        self.i += self.batch_size\n",
    "        return X, y\n",
    "        \n",
    "    def __next__(self):\n",
    "        return self.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, X, y, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.input_dim = X_train.shape[1]\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.input_dim))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.weights = {\n",
    "                    'weights1':tf.get_variable('weights1', (self.input_dim,self.hidden_dim ), dtype=self.default_dtype), \n",
    "                    'bias1':tf.get_variable('bias1', (self.hidden_dim, ), dtype=self.default_dtype), \n",
    "                    'weights2':tf.get_variable('weights2', (self.hidden_dim, self.n_classes ), dtype=self.default_dtype), \n",
    "                    'bias2':tf.get_variable('bias2', (self.n_classes, ), dtype=self.default_dtype)}\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, x_input, weights):\n",
    "        hidden = tf.matmul(x_input, weights['weights1'])\n",
    "        hidden = tf.add(hidden, weights['bias1'])\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        output = tf.matmul(hidden, weights['weights2'])\n",
    "        output = tf.add(output, weights['bias2'])\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = MLP(X, y)\n",
    "c.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.chkpt\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.91      0.93        67\n",
      "          1       0.94      0.97      0.96       104\n",
      "\n",
      "avg / total       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.argmax(y_test, axis=1), c.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.929648241206\n",
      "Test:  0.888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "gb = MLPClassifier(hidden_layer_sizes=(1000, ))\n",
    "gb.fit(X_train, np.argmax(y_train, axis=1))\n",
    "print(\"Train: \", np.mean(gb.predict(X_train) == np.argmax(y_train, axis=1)))\n",
    "print(\"Test: \", np.mean(gb.predict(X_test) == np.argmax(y_test, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Sequence modeling\n",
    "* Preprocessing\n",
    "* Create data structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim >> gensim-log.txt\n",
    "#!pip install spacy >> spacy-log.txt\n",
    "#!python -m spacy download en >> spacy-download.txt\n",
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 0\n",
    "        self.MISSING_VAL = 1\n",
    "        self.START_VAL = 2\n",
    "        self.END_VAL = 3\n",
    "        self.INDEX_OFFSET = 4\n",
    "        self.vocab = OrderedDict()\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj), 0)\n",
    "        result = obj[:self.max_len] + [self.PADDING_VAL] * n_pads\n",
    "        result[-1] = self.END_VAL\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def check_word(self, word):\n",
    "        current_vocab_size = self.get_current_vocab_size() # 0\n",
    "        if current_vocab_size <= self.max_vocab_size:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab.update({word: current_vocab_size + self.INDEX_OFFSET}) #{'apple': 0}\n",
    "        try:\n",
    "            return self.vocab[word]\n",
    "        except KeyError:\n",
    "            return self.MISSING_VAL\n",
    "        \n",
    "    def __call__(self, corpus, merge_ents=True):\n",
    "        docs = []\n",
    "        if merge_ents:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False):\n",
    "                for ent in doc.ents:\n",
    "                    ent.merge()\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                tokens = [self.START_VAL] + tokens + [self.END_VAL]\n",
    "                docs.append(self.padder(tokens))\n",
    "        else:\n",
    "            for doc in self.nlp.pipe(corpus, parse=False, tag=False, entity=False):\n",
    "                tokens = list(map(self.process_token, doc[:self.max_len]))\n",
    "                docs.append(self.padder(tokens))\n",
    "        \n",
    "        return docs\n",
    "  \n",
    "    def process_token(self, token):\n",
    "        if token.like_url:\n",
    "            return self.check_word(\"URL\")\n",
    "        elif token.like_email:\n",
    "            return self.check_word(\"EMAIL\")\n",
    "        elif token.like_num:\n",
    "            return self.check_word(\"NUM\")\n",
    "        else:\n",
    "            return self.check_word(token.lower_)\n",
    "\n",
    "\n",
    "#nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset = fetch_20newsgroups()\n",
    "#corpus = dataset.data\n",
    "processor = TextProcesser(nlp=nlp, max_len=100)\n",
    "processed_corpus = processor(corpus, merge_ents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, X, y, max_seq_len, lstm_size=1, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.max_seq_len = max_seq_len or () #len of X\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        self.vocab_size\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.max_seq_len))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.embeddings = tf.Variable(shape=(self.vocab_size, self.hidden_dim))\n",
    "                self.lstm_state = tf.zeros([batch_size, lstm.state_size])\n",
    "                self.lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, word_input):\n",
    "        hidden = tf.nn.embedding_lookup(self.embeddings, word_inputs)\n",
    "        output, self.lstm_state = self.lstm(hidden, self.lstm_state)\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = label_binarize(cancer.target, classes=[0,1,2])[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
    "\n",
    "class LSTM(object):\n",
    "    def __init__(self, X, y, max_seq_len, lstm_size=1, layer_size = 1000):\n",
    "        \n",
    "        \"\"\"\n",
    "        32-512 rows per batch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.hidden_dim = layer_size\n",
    "        self.max_seq_len = max_seq_len or () #len of X\n",
    "        self.model_path = 'model.chkpt'\n",
    "        self.saver = None\n",
    "        self.graph = tf.Graph()\n",
    "        self.default_dtype = tf.float64\n",
    "        self.vocab_size\n",
    "        with self.graph.as_default():\n",
    "            with tf.variable_scope('mlp_model') as scope:\n",
    "                self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "                self.x_input = tf.placeholder(X.dtype, shape = (None, self.max_seq_len))\n",
    "                self.y_output = tf.placeholder(X.dtype, shape = (None, self.n_classes))\n",
    "                self.embeddings = tf.Variable(shape=(self.vocab_size, self.hidden_dim))\n",
    "                self.lstm_state = tf.zeros([batch_size, lstm.state_size])\n",
    "                self.lstm = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "                self.get_logit_op = self.feed_forward(self.x_input, self.weights)\n",
    "                self.predict_proba_op = tf.sigmoid(self.get_logit_op)\n",
    "                self.predict_op = tf.argmax(self.predict_proba_op, axis=1)\n",
    "                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_output, logits=self.get_logit_op))\n",
    "                self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)            \n",
    "    \n",
    "    def feed_forward(self, word_input):\n",
    "        hidden = tf.nn.embedding_lookup(self.embeddings, word_inputs)\n",
    "        output, self.lstm_state = self.lstm(hidden, self.lstm_state)\n",
    "        return output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "            \n",
    "    def predict_proba(self, X, session):\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver.restore(sess, self.model_path)\n",
    "            preds = sess.run(self.predict_proba_op, feed_dict={self.x_input:X})\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, epochs=100):\n",
    "        \n",
    "        epochs = range(epochs)\n",
    "  \n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            self.saver = tf.train.Saver()\n",
    "            for var in self.graph.get_collection('variables'):\n",
    "                sess.run(var.initializer)\n",
    "                \n",
    "            for epoch in epochs:\n",
    "                sess.run(self.optimizer, feed_dict={self.x_input: self.X, \n",
    "                                                    self.y_output: self.y})\n",
    "            \n",
    "            self.saver.save(sess, self.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
