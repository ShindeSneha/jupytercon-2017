In the previous video we briefly looked at Valency, a foundational concept for modern grammar models. To recap, Valency relationships describe the types of arguments that governor words require. Constitueny grammar builds on this concept by providing a notion of a constituent, which is a word or a group of words that is treated as a single syntactic unit. This is similar to object oriented programming, where we can compose units into abstract structures, which themselves are composable. 

If we take the sentence "He went to the store". We initially assign syntactic categories, or parts of speech, to each word. We can then compile these words categories into known phrase structures, like noun, verb, and prepositional phrases. A prepositional phrase for instance requires a preposition and a noun or noun phrase. Modeling the sentence this way makes it easy to explain the role each word is playing at varying levels of abstraction. 

SpaCy provides syntactic parsing under a dependency grammar framework. There is a long standing debate about how constituency grammar and dependency grammar relate. Practically however, dependency grammar is computationally easier to parse, and is a bit more robust to languages with free word order. Lets dive into dependency grammar in order to understand how it works, and what its assumptions are.

The key idea of dependency grammar is that words in a syntax all belong to a heirarchy of dependence. A dependency relation is a directed link between a head and its dependent. The root verb of the sentence is at the top, and all other words are either dependents of the root or dependents of other dependents. 

As an exercise, lets unpack a sentence to see the intuition behind dependency grammar. Lets start with the word "provides". To construct a complete sentence, we have a few questions. First, "who provides?" The answer is the dependent subject "tree". We then ask "which tree?, to which we can respond the dependent determiner "this". We follow a similar chain with the object of "provides". Note that the questions we ask flow naturally from the valency requirements of the head word. The dependents, or answer, tend to be modifiers or compliments of the head. 

In order to be able to reason about dependency trees, and to parse them, we need to contstrain what defines a well formed sentence, and thus a well formed parse. There is lot of really interesting literature on how expressive certain grammar systems are given their assumptions. There are versions of dependency grammars with different sets of assumptions, but mostly commonly we parse projective dependency trees, the rules for which we'll address now. Without these assumptions parsing would be extremely difficult. When we parse dependency structures using spaCy, all of these rules will be followed.

The first rule is that there is only one root of a sentence. The sentence “he went to the beach” has the single root “went”, and can be represented correctly by spacy.

Consider the compound sentence, “he went to the beach and she went to the beach”. Though it would seem unfair to treat either instance of “went” as the single root of the sentence, we are restricted to only one. Spacy will choose the first instance of “went” as the root, and treat the second instance as a conjunct to the first. A conjunct is the relation between a parent and a child that are connected by a conjunction such as “and” or “or”.

Lastly, the phrase “to the beach” is not a valid independent clause, so there is no true root of the sentence. However, Spacy will always identify one token as a root, in this case the work “to”.

The second rule is that every word other than the root must be a dependent of another word. You cannot have a syntactically independent word as part of a sentence. This is a very reasonable constraint in most domains, though it might for instance break down if we were parsing twitter data. Hashtags have no inherent dependents, though by our axioms spaCy would assign it some dependent.

Another rule is that words cannot dependent on more than one element. This is also called the single-headed assumption. This prevents us from expressing a few linguistic concepts, such as conjuctive subjects of a verb. In the sentence Jack and jill love to eat, we parse jack to be the subject, where the words "and Jill" serve as conjunctive dependents of jack. We might instead want a parse reflecting both jack and jill as subjects of the verb, and where both are argumenets of the word "and". This however requires both jack and jill to be dependents of multiple words, which is not legal. In practice, we might get around this limitation by identifying jack and all of Jack's conjucts as the subject, though the parse will not be able to immediately represent that. There are a few other types of syntactical and semantic relationships that are unexpressable without multi-headed parses, there is a great paper on this linked in the show notes. [http://cs.jhu.edu/~jason/papers/hong+eisner.tlt14.paper.pdf]

The last rule requires the parse tree to be planar, meaning that none of the dependencies cross each other. To understand this rule, and the contraints of expressivity it causes, consider the sentence “I saw a movie today that made me laugh”. The parse shown is how Spacy parses the sentence. 

The problem here is the word "today". its an adverb that is modifies the verb "saw". The verb "made" is modifying the word movie. Given the placement of the adverb today in the sentence,  we would need to cross these two connecting arcs. For a parse to be projective, a word and all its children must form a continuous sequence of words. Here, the adverb today separates “movie” and “made” but is,  but is a dependent of word that is outside their connecting window.

Now that we've seen what dependency grammar is and what the rules are, in the next video we'll use SpaCy's dependency parser for a couple interesting tasks, both to see how to intereact with the spaCy parse and to see why its useful.









#APPLICATIONS SECTION, PERHAPS IN THE NEXT VIDEO.


This explanation of dependency grammar helps explain how we build a new sentence, but does it help us after the sentence is contructed. What can we do with a representation of a sentence that reflects its syntactic dependency structure?


There is a class of problems in natural language processing related to information retrieval for which dependency parsing can be very helpful. 




Application profile 1:
goal: get tuples of (product, feature, sentiment statement)


