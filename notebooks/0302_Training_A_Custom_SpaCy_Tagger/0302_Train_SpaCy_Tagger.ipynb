{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<h3>Basic Recipe for Training a POS Tagger with SpaCy</h3>\n",
    "<ol>\n",
    "<li> Load Data\n",
    "<ol><li>We'll be using a sample from Web Treebank corpus, in ConllX format</ol>\n",
    "<li> Prepare environment for a new model\n",
    "<ol><li>New model directory, with tagger and parser subdirectories. (Ensure you have permission)</ol>\n",
    "<li> Build a vocabulary\n",
    "<ol><li>We are just going to load the default English Vocabulary</ol>\n",
    "<li> Build a Tagger (Ensure tagmap is provided if needed)\n",
    "<ol><li>Our corpus has tags from Universal Tagset, so no need for a custom tag map</ol>\n",
    "<li> Train Tagger\n",
    "\n",
    "<ol><li>For each epoch: \n",
    "<ol><li>For each document in training data:\n",
    "<ol><li>For each sentence in document:\n",
    "<ol>\n",
    "    <li>Create document with sentence words (tagger not yet applied)\n",
    "    <li>Create GoldParse object with annotated labels\n",
    "    <li>Apply the tagger to the document to get predictions\n",
    "    <li>Update the tagger with GoldParse, Document (actual v predicted)\n",
    "    \n",
    "</ol>\n",
    "</ol>\n",
    "<li> Score predictions on validation set\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def read_conllx(text):\n",
    "    \n",
    "    for sent in text.strip().split('\\n\\n'):\n",
    "        lines = sent.strip().split('\\n')\n",
    "        if lines:\n",
    "            while lines[0].startswith('#'):\n",
    "                lines.pop(0)\n",
    "            tokens = []\n",
    "            for line in lines:\n",
    "                id_, word, lemma, tag, pos, morph, head, dep, _1, _2 = line.split()\n",
    "                if '-' in id_:\n",
    "                    continue\n",
    "                try:\n",
    "                    id_ = int(id_) - 1\n",
    "                    head = (int(head) - 1) if head != '0' else id_\n",
    "                    dep = 'ROOT' if dep == 'root' else dep\n",
    "                    tokens.append((int(id_), unicode(word), unicode(pos), int(head), unicode(dep), 'O'))\n",
    "                except:\n",
    "                    print(line)\n",
    "                    raise\n",
    "            tuples = [list(t) for t in zip(*tokens)]\n",
    "            yield (None, [[tuples, []]])\n",
    "\n",
    "            \n",
    "            \n",
    "def LoadData(url, make_projective = False):\n",
    "    conll_string = requests.get(url).content\n",
    "    sents = list(read_conllx(conll_string))\n",
    "    return sents\n",
    "    \n",
    "    \n",
    "train_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-train.conllu'\n",
    "test_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-test.conllu'\n",
    "train_sents = LoadData(train_url)\n",
    "test_sents = LoadData(test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Prepare Environment for New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def prepare_environment_for_new_tagger(model_path, new_tagger_name):\n",
    "    if not (model_dir / new_tagger_name).exists():\n",
    "        (model_dir / new_tagger_name).mkdir()\n",
    "        \n",
    "\n",
    "model_dir = Path('/usr/local/lib/python2.7/dist-packages/spacy/data/en-1.1.0')\n",
    "new_tagger_name = 'web-treebank-tagger'\n",
    "prepare_environment_for_new_tagger(model_dir, new_tagger_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "def build_vocab(model_dir, vec_path = None, lexeme_path = None):\n",
    "    vocab = Vocab.load(model_dir)\n",
    "    if lexeme_path:\n",
    "        vocab.load_lexemes(lexeme_path)\n",
    "    if vec_path:\n",
    "        vocab.load_vectors_from_bin_loc(vec_path)\n",
    "        \n",
    "    return vocab\n",
    "    \n",
    "lexeme_path = model_dir / 'vocab' / 'lexemes.bin'\n",
    "vocab = build_vocab(model_dir, vec_path=vec_path, lexeme_path=lexeme_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Value for 'He': 126\n"
     ]
    }
   ],
   "source": [
    "#test clusters are available\n",
    "\n",
    "doc = Doc(vocab, words=[u'He',u'ate',u'pizza',u'.'])\n",
    "print \"Cluster Value for '{}': {}\".format(*[doc[0], doc[0].cluster])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Build a Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "from spacy.tagger import Tagger\n",
    "def build_tagger(vocab, tag_map=None):\n",
    "    tagger = Tagger(vocab, tag_map=tag_map)\n",
    "    return tagger\n",
    "\n",
    "tagger = build_tagger(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Train Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_datascience": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining:\t0.000\n",
      "0:\t89.620\n",
      "1:\t89.488\n",
      "2:\t90.819\n",
      "3:\t91.078\n",
      "4:\t91.178\n",
      "5:\t90.839\n",
      "6:\t91.369\n",
      "7:\t91.947\n",
      "8:\t92.353\n",
      "9:\t92.565\n"
     ]
    }
   ],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "import random\n",
    "\n",
    "\n",
    "def score_model(vocab, tagger, gold_docs, verbose=False):\n",
    "    scorer = Scorer()\n",
    "    for _, gold_doc in gold_docs:\n",
    "        for (ids, words, tags, heads, deps, entities), _ in gold_doc:\n",
    "            doc = Doc(vocab, words=map(unicode,words))\n",
    "            tagger(doc)\n",
    "            gold = GoldParse(doc, tags=tags)\n",
    "            scorer.score(doc, gold, verbose=verbose)\n",
    "    return scorer  \n",
    "\n",
    "\n",
    "def train(tagger, vocab, train_sents, test_sents, model_dir, n_iter=20, seed = 0, feat_set = u'basic'):\n",
    "    scorer = score_model(vocab, tagger, test_sents)\n",
    "    print('%s:\\t%.3f' % (\"Pretraining\", scorer.tags_acc))        \n",
    "    for itn in range(n_iter):\n",
    "        for _, doc_sents in train_sents:\n",
    "            for (ids, words, tags, heads, deps, ner), _ in doc_sents:\n",
    "                doc = Doc(vocab, words=map(unicode,words))\n",
    "                gold = GoldParse(doc, tags=tags, heads=heads, deps=deps)\n",
    "                tagger(doc)\n",
    "                tagger.update(doc, gold)\n",
    "        random.shuffle(train_sents)\n",
    "        scorer = score_model(vocab, tagger, test_sents)\n",
    "        print('%d:\\t%.3f' % (itn, scorer.tags_acc))\n",
    "    return tagger\n",
    "trained_tagger = train(tagger, vocab, train_sents, test_sents, model_dir, n_iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Save Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_datascience": {}
   },
   "outputs": [],
   "source": [
    "def ensure_dir(path):\n",
    "    if not path.exists():\n",
    "        path.mkdir()\n",
    "        \n",
    "ensure_dir(model_dir / new_tagger_name)\n",
    "trained_tagger.model.dump(str(model_dir / new_tagger_name / 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_datascience": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
