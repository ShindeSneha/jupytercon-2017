{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import sys\n",
    "from IPython.display import HTML\n",
    "sys.path.append('/tmp/dd10b84e-1fb0-4dcd-b55d-79d2aa0d553f/lib/python2.7/site-packages/')\n",
    "\n",
    "# !pip install spacy\n",
    "# !python -m spacy.en.download all\n",
    "# !pip install nltk\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading parsing model\r\n",
      "Model already installed. Please run 'python -m spacy.en.download --force' to reinstall.\r\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "corpus = nltk.corpus.reuters\n",
    "sents = [map(unicode, tokens) for tokens in corpus.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "nlp = spacy.en.English(parse = False)\n",
    "\n",
    "def merge_phrases(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    if i != len(matches)-1:\n",
    "        return None\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "    for ent_id, label, span in spans:\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_)\n",
    "        \n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add_entity('USA', on_match = merge_phrases)\n",
    "matcher.add_pattern(\"USA\", [{ORTH: \"U\"}, {IS_PUNCT: True}, {ORTH: \"S\"}, {IS_PUNCT:True}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "def process_token(token):\n",
    "    if token.is_punct or token.is_space:\n",
    "        return False\n",
    "    elif token.like_url:\n",
    "        return'URL'\n",
    "    elif token.like_email:\n",
    "        return'EMAIL'\n",
    "    elif token.like_num:\n",
    "        return'NUM'\n",
    "    else:\n",
    "        return token.lower_.replace(\" \",'')\n",
    "    \n",
    "    \n",
    "def process_sentence(tokenized_sent):\n",
    "    tokens = []\n",
    "    \n",
    "    doc = Doc(nlp.vocab, words = tokenized_sent)\n",
    "    nlp.tagger(doc)\n",
    "    matcher(doc)\n",
    "    \n",
    "    for token in doc:\n",
    "        processed_token = process_token(token)\n",
    "        if processed_token:\n",
    "            tokens.append(processed_token)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'u . s .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sents = map(process_sentence, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=processed_sents, ###tokenized senteces, list of list of strings\n",
    "                 size=300,  #size of embedding vectors\n",
    "                 workers=8, #how many threads?\n",
    "                 min_count=5, #minimum number of token instances to be considered\n",
    "                 sample=0, #weight of downsampling common words? \n",
    "                 sg = 0, #should we use skip-gram? if 0, then cbow\n",
    "                 hs=0, #heirarchical softmax?\n",
    "                 iter=5 #training epocs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "#write to disk\n",
    "vector_file = 'custom-vectors.word2vec'\n",
    "model.save_word2vec_format(str(vector_file), binary=False)\n",
    "\n",
    "\n",
    "#delete first 2 lines; contains non-decodeable stuff\n",
    "with open(str(vector_file)) as f:\n",
    "    data = f.readlines()\n",
    "f.close()\n",
    "with open(str(vector_file), 'wb') as f:\n",
    "    f.writelines(data[2:])\n",
    "    \n",
    "\n",
    "#compress via bz2\n",
    "compressed_filename = vector_file + '.bz2'\n",
    "z = bz2.compress(open(vector_file).read())\n",
    "with open(compressed_filename, 'w+') as out:\n",
    "    out.write(z)\n",
    "\n",
    "    \n",
    "#write to binary with spacy    \n",
    "spacy.vocab.write_binary_vectors(compressed_filename,'custom-vectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/gensim/models/word2vec.py:459: UserWarning: C extension compilation failed, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension compilation failed, training will be slow. Install a C compiler and reinstall gensim for fast training.\")\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "v = Vocab()\n",
    "\n",
    "nlp.vocab.load_vectors_from_bin_loc(str(vector_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_datascience": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
