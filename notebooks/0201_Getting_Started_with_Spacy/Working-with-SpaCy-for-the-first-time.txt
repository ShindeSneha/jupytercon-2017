In this section we’ll begin working with spacy to process text. After this video, you’ll have a high level view of the functionality that spacy provides, and have an sense of how grab annotations from spacy data structures like documents, spans, and tokens. 

[SCROLL DOWN SO DIAGRAM TAKES UP TOP PART OF SCREEN]
Lets begin by creating a variable holding some text. This could be a word, phrase, sentence, or even a multi-paragraph document. We prefix the string with the letter u, which encodes our text to unicode. Unicode is a system for defining characters, such as letters, numbers, and symbols as integers. Requiring unicode has some technical benefits such as ensuring that Python and C playing nicely together across versions, and that string types are uniform. 

As part of its language processing, Spacy will automatically identify sentence boundaries in your text if any exist and split them up, so it really doesn't matter if your input has one or multiple sentences. While passing in words and phrases is OK, some of what Spacy does assumes complete sentences, so certain annotations like parts of speech and dependencies may not make complete sense. 

Unlike nltk, which is mainly composed of pure functions that return strings and other native data structures, spacy is object oriented.  Objects are created, and then mutated and queried to get work done. In this example, we are using the spacy language object to create a document object. The Language object is used as a document constructor, or a function that builds documents for us. We can instantiate a language object either by calling spacy.load and passing in a language ID, english in our case, or by grabbing the module directly with spacy.en.Engish(). The language object will take our unicode string as an argument, and return a document object with a lot of useful annotations. Note that to have to manually download a spacy language model before you can use it.

[SCROLL DOWN]
The language module runs a series of functions over the document to build the annotations. These functions are stored in the model's pipeline object. As we can see, the functions include a part of speech tagger, a dependency parser, a matcher, and a named entity recognizer. Dont worry if any of these tasks are new to you, we will cover them in later videos.

Some of these submodules require a lot of resources, and as such, they take a while to load, and do take some memory. You’ll want to avoid creating multiple language model instances, as a single instance can take up a couple gigs of RAM. Once a language model is instanciated, it can be used over and over again to process text.

Language models are fully customizable. We can alter, add, or remove functions from the pipeline. Lets create a pipeline that contains all the default processing steps, but includes as a final step a function that tags the lowercase token "starwars" as a proper noun.

Our function, called identify_starwars, should take a document as its sole argument. For each token in the document, we'll check whether its text form is "Star Wars". If so, we'll change one of the annotations associated with this token, in this case we'll assign its part of speech tag to NNP, the abbreviation for proper nouns. 

To add our function to a pipeline, we create a function that takes a language model as an argument, and returns the pipeline we want as a list. Note that we've added our identify starwars function to the end of the pipeline. Each function in pipeline mutates documents in place, so our function's effects would have been overwritten if placed before the tagger function, which assigns parts of speech tags to tokens in our document. Lastly, we'll create our custom language model by passing in our return_pipeline function to spacy.load.

[SCROLL DOWN]
While we can call the model directly to create a document, the model also contains a method called pipe, which allows us to use multiple threads to process an array or generator of texts. Usually, natural language processing is considered to be embarrassingly parallel, meaning that processing one document is completely independent of processing another document. Therefore, we can split up the task of processing multiple texts to distinct computing resources. nlp.pipe returns a generator of document objects, meaning that documents are not actually run until we request them from the generator. In this example, we request them by iterating over the generator with a for loop.

[SCROLL TO PLOT CODE]
To show you the effect of using multiple threads, lets process an array of 10000 texts. We'll vary the number of threads from 1 to 8, and record the processing time at each pass. 

[SCROLL TO PLOT IMAGE]

The runtime for running the operations on 8 threads is about a third of that of using just a single thread. If we were to keep increasing the thread count, the runtime would start to increase due to the extra overhead of using that many threads. While choosing the optimal number of threads is outside the scope of this lesson, it will depend on the number and complexity of the calculations youre doing, and the type of system of youre using. Generally if youre going to reuse a parallelized operation,  its always a good idea to try a number of different configurations and record runtimes, as we've done here.
  
[SCROLL DOWN PAST PLOT]
Let's now take a look these document objects in a big more detail. Because spacy tokenizes our text, we can grab individual tokens by indexing the document. We can also grab sequences of tokens, called spans in Spacy, by slicing the document.

[SCROLL TO SENTENCE BOUNDARY DETECTION]
These objects contain a lot of very useful annotations. As mentioned earlier, spacy performs sentence boundary detection. Calling a document's .sents method returns an iterable of sentences. Again, because spacy returns generators, it postpones doing work until we actually need results, and allows us to process individual elements of an array without needing to load everything into memory all at once. 

[SCROLL TO TOKENIZATION]
Similarly, by iterating over the document, we can get access to individual tokens. The best way to see what annotations are available at the token level is to play around with a token yourself, but lets take a look at a couple so you can get a sense of how to access them.

[SCROLL TO MORPHOLOGICAL DECOMP]
Morphemes are little bits of language, words or parts of words, that convey meaning. For instance, "un" isnt a standalone word, but as a prefix connotes the negation of whatever follows. Identifying these morphemes in natural language can be helpful, such as trying to define a new word, or determining a word's part of speech. Spacy does some morphological decomposition for us, providing word suffixes and prefixes. We'll grab these annotations from individual tokens.

[SCROLL TO POS TAGGING]
Spacy provides part of speech tagging and dependency parsing. These annotations tell us the role each word plays syntactically. Each token in the document contains these annotations, so in this example, we'll use map to grab these annotations from each token in our document. Map takes two arguments, a function and something you can iterate over, and the returns the result of the function applied to each element in the iterable as a list.

[SCROLL TO CHUNKING AND NER]
Spacy also provides annotations for spans within the document, or sequences of tokens. Sometimes we want to indicate that a sequence of tokens has a property. For example, a noun chunk is a span of tokens that collectively form some noun phrase. Recall that spacy returns generators when we ask for an iterable, so we'll convert the generator into a list to get access to all of the noun chunks in the document. Spacy also annotates spans as entities, which we'll look at by using list comprehension over the iterables of entities within the document.

[SCROLL TO TEXT SIMILARITY]
Lastly Spacy provides vector representations of tokens, spans, and documents. We'll cover this in more detail later on, but think of word embeddings as mappings from natural language to a metric space. Vectors are used to represent elements of language, and the more similar two vectors are, the more similar the underlying words, phrases, or documents are in meaning. This is a bit of an oversimplification, but its a useful mental model for now if youre unfamiliar with the concept. Spacy not only provides the vectors for tokens, spans, and documents, but also computes the similarity between any two vectors. In this example, we'll create a new document, and compute the similarity between each individual word in the document with the word "computer". Because iterating on a document gives us access to tokens, we can use the map function over the document to get similarity scores for each token. The results are then plotted.










