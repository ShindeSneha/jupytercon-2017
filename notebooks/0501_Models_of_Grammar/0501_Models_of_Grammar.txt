In the next few videos, we'll learn about systems of grammar; we'll start by  addressing what a grammar model is, begin looking at valency grammar, which is the general framework under which dependency grammar is a special case.


[GO TO WHAT IS GRAMMAR?]

In both natural and artificial languages, we compose lexical units into larger clauses to represent complex meaning. When forming these compositions, we follow rules that govern which types of lexical units can be combined with others, and how these combinations are to be interpreted.Generally, these rules can be broken down into morphology and syntax.
[scroll to morphology]
Morphology is the set of rules that govern word form; how does mutating a word's suffixes, prefixes, conjugations, and the like affect that words meaning? Morphemes are building blocks of words. Composing morphemes together can change the semantics of a word, or its syntactic role and requirements. The word unreachable for example is composed of a negating prefix "un", and a suffix that specifies the adjective form of the word. 


Syntax is the set of rules that govern word order and agreement. We first construct a taxonomy of primitives or syntactic units. In programming languages, these can be integers and strings; in natural languages these are words and phrases. With respect to syntax, all members of syntactic category are exchanged with each other, and still form grammatically correct sentences. Elements of these syntactic categories are then placed together for compilation. 

[scroll to other thing]

In the sentence "He went to the store", we can exchange the word "he" with members of the same syntactic category, like she, john, and the man. 

With syntactic categories defined, we compile sytactic elements into more and more abstract structures and higher order semantic statements.

Lets briefly look at an example just to illustrate why preserving word order and parsing syntax is important. If we take the sentence,

he did not like the movie, and told the cashier he wanted his money back.

We can make really slight adjustments to the sentence's morphology and word order, and obtain a completely different sentence. These language features tell us what modifies what, who did what to whom, and when. In short, these rules govern how we relate words to each other in order to express complex meaning. For some applications, the bag of words assumption will clearly fail, as word order is important to establish the meaning of a sentence. In addition, preprocessing that removes important morphological elements of words in a sentence can degrade our ability to model it effectively.

Even if we retain the morphology and word order of a sentence, we will sometimes miss out on important semantics unless we actually parse the syntax of the sentence. This is a famous Groucho Marx line, "I shot an elephant in my pajamas". He then goes on to ponder how the elephant couldve ended up in his pajamas, playing on the dual syntactic interpretations of the sentence. 

When we parse syntax from some natural language text, then we have more immediate access to its underlying semantics, which in turn we can leveraged for building applications or statistcal analysis. It also can help define the role of a word within its context, which can be a lot more powerful than looking at  word in isolation. When we build a simple question answering answering system, we'll parse syntactic relationships to determine what the question is asking and what the answer should be. 

In order to parse syntax, we need to create an abstraction; what sort of relationships and data structures can we use to model how words are combined into higher order statements. One of the more fundamental frameworks for representing relationships is called valency grammar. Relationships between words are thought of those between a function and its arguments. 

The function itself is thought of as the root or nucleus word. Its the word that glues all the arguments together. The nucleus belongs to a syntactic category, and therefore the types of words that it can be combined with, as passed as arguments, are pretermined. This is pretty similar to writing a typed function. A transitive verb can be formed into an independent clause if we provide it a subject and a direct object. In this analogy, the output is called a result segment, in this case a verb expression. The arguments, a noun or nominative and a direct object or accusative, are called the valency fillers. 

The abstraction that words take typed arguments is the foundation for spacy-style dependency grammar, which we will cover in the next video.
