{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import spacy\n",
    "import autocorrect\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    'He playedd baseball',\n",
    "    'He plays football',\n",
    "    'He had a sandwich'\n",
    "]\n",
    "\n",
    "toDTM = CountVectorizer(lowercase=False)\n",
    "DTM = toDTM.fit_transform(documents).todense()\n",
    "words = toDTM.get_feature_names()\n",
    "\n",
    "summary = pd.DataFrame(DTM, columns = words, index = documents)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Reducing the number of columns in Document Term Matrix:\n",
    "* Normalization and lemmatization\n",
    "* Spelling correction\n",
    "* Setting vocabulary size thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the processing function\n",
    "def process(document):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    document: str\n",
    "        The document we want to process\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    \"\"\"\n",
    "    #create spacy object\n",
    "    \n",
    "    spacy_doc = nlp(unicode(document), parse=False, entity=False)\n",
    "    \n",
    "    #grab the lemma for each token in the document\n",
    "    processed_tokens = map(lambda token: token.lemma_, spacy_doc)\n",
    "    \n",
    "    #join lemmas to a string\n",
    "    result = \" \".join(processed_tokens)\n",
    "    return result\n",
    "\n",
    "\n",
    "#Create the Custom tokenizer\n",
    "class SpellTokenizer(object):\n",
    "    \n",
    "    def __init__(self, nlp):\n",
    "        self.vocab = nlp.vocab\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        doc = nlp.tokenizer(unicode(text))\n",
    "        words = [autocorrect.spell(i.orth_) for i in doc]\n",
    "        return spacy.tokens.Doc(self.vocab, words = words)\n",
    "\n",
    "#create a language model that uses the custom tokenizer\n",
    "nlp = spacy.load('en')\n",
    "nlp.make_doc = SpellTokenizer(nlp)        \n",
    "    \n",
    "#pass in the process function to sklearns vectorizer\n",
    "toDTM = CountVectorizer(preprocessor=process \n",
    "                        , min_df = 0. #set minimum of token instance\n",
    "                       )\n",
    "\n",
    "DTM = toDTM.fit_transform(documents).todense()\n",
    "words = toDTM.get_feature_names()\n",
    "\n",
    "summary = pd.DataFrame(DTM, columns = words, index = documents)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Retrieving Documents in a DTM\n",
    "\n",
    "0) Encode documents as a DTM\n",
    "\n",
    "1) Encode the query\n",
    "\n",
    "2) Compute similarities/distances of query vector and dtm\n",
    "\n",
    "3) Pick argmin/argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def getMostSimilarSentence(query, documents):\n",
    "    \"\"\"\n",
    "    Transforms query into vector, and computes cosine similarity \n",
    "    of query vector against training documents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query:                (string) document to compare\n",
    "    vectorizer:            sklearn vectorizer class \n",
    "    document_term_matrix: (pandas.DataFrame) table of \n",
    "                          term instances in each document\n",
    "                          \n",
    "    Returns\n",
    "    -------\n",
    "    most similar document (string)\n",
    "    \"\"\"\n",
    "    #create vectorizer and use it to build dtm\n",
    "    vectorizer = CountVectorizer(preprocessor=process)\n",
    "    dtm = vectorizer.fit_transform(documents).todense()\n",
    "    \n",
    "    #transform query to vector\n",
    "    query_vector = encodeQuery(query, vectorizer)\n",
    "    \n",
    "    #compute similarityes\n",
    "    similarities = computeSimilarities(query_vector, dtm)\n",
    "    \n",
    "    #grab most similar document\n",
    "    closest_idx = getMostSimilarIdx(similarities)\n",
    "    return documents[closest_idx]\n",
    "\n",
    "def encodeQuery(query, vectorizer):\n",
    "    \n",
    "    #transform query to vector\n",
    "    query_vector = vectorizer.transform([query]).todense()\n",
    "    return query_vector\n",
    "\n",
    "def computeSimilarities(query_vector, dtm):\n",
    "    \n",
    "    #compute similarities\n",
    "    all_vectors = np.concatenate((dtm, query_vector))\n",
    "    similarities = cosine_similarity(all_vectors)[-1][:-1]\n",
    "    return similarities\n",
    "\n",
    "def getMostSimilarIdx(similarities):\n",
    "    #grab most similar document\n",
    "    return np.argmax(similarities)\n",
    "    \n",
    "query = 'Foootball'\n",
    "getMostSimilarSentence(query, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make labels 0/1 about sports\n",
    "about_sports = {\n",
    "    'He played baseball':1,\n",
    "    'He plays football':1,\n",
    "    'He had a sandwich':0\n",
    "}\n",
    "summary['about_sports'] = pd.Series(about_sports)\n",
    "\n",
    "X = summary[words].values\n",
    "y = summary['about_sports']\n",
    "\n",
    "#regress labels on elements of DTM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C = 100000.)\n",
    "model.fit(X, y)\n",
    "\n",
    "#show coeficients\n",
    "pd.Series(model.coef_[0], index = words).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mod = LogisticRegression(C = 100000.)\n",
    "X = summary[words].values\n",
    "y = summary['about_sports']\n",
    "\n",
    "mod.fit(X, y)\n",
    "pd.Series(mod.coef_[0], index = words).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "pd.DataFrame(cosine_similarity(summary.T.values), index = words, columns = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<img src='UsingEmbeddingsForML.svg' width=650 height=500/>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = u'Word vectors are fantastic!'\n",
    "doc = nlp(text)\n",
    "token = doc[1]\n",
    "print token.vector[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "average_of_token_vectors = np.mean([token.vector for token in doc])\n",
    "document_vector = doc.vector\n",
    "assert all(average_of_token_vectors - document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "\n",
    "offline.init_notebook_mode()\n",
    "\n",
    "\n",
    "#grab word vectors for each word\n",
    "words = [u'cat',u'dog',u'man',u'woman']\n",
    "vectors = map(lambda word: nlp(word).vector, words)\n",
    "\n",
    "#create a dataframe of similarities\n",
    "similarities = cosine_similarity(vectors)\n",
    "similarity_matrix = pd.DataFrame(similarities, index = words, columns = words)\n",
    "\n",
    "\n",
    "data = [go.Heatmap( z=similarity_matrix.T.values.tolist()\n",
    "                   , colorscale='OrRd'\n",
    "                   ,x = words\n",
    "                   ,y = words\n",
    "                  )]\n",
    "\n",
    "\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grab word vectors for each word\n",
    "words = [u'cat',u'dog',u'man',u'woman',u'women']\n",
    "vectors = map(lambda word: nlp(word).vector, words)\n",
    "\n",
    "plural_men = nlp(u'men').vector - nlp(u'man').vector\n",
    "plural_dogs = nlp(u'dogs').vector - nlp(u'dog').vector\n",
    "\n",
    "plural = (plural_men + plural_dogs) / 2\n",
    "\n",
    "vectors.append(plural + vectors[words.index('woman')])\n",
    "words.append('Plural Plus Woman')\n",
    "\n",
    "similarities = cosine_similarity(np.array(vectors))\n",
    "similarity_matrix = pd.DataFrame(similarities, index = words, columns = words)\n",
    "\n",
    "data = [go.Heatmap( z=similarity_matrix.T.values.tolist()\n",
    "                   , colorscale='OrRd'\n",
    "                   ,x = words\n",
    "                   ,y = words\n",
    "                  )]\n",
    "\n",
    "\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Document Term Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open('dtm.html').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### The Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "def nltk_corpus(corpus_name):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "def corpus_to_x_y(corpus):\n",
    "    fileids = corpus.fileids()\n",
    "    tuples = map(lambda i: (\" \".join(corpus.words(i)), corpus.categories(i)[0]),fileids)\n",
    "    x, y = zip(*tuples)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "#prepare data for classification\n",
    "documents, categories = corpus_to_x_y(nltk_corpus('brown'))\n",
    "documents, categories = shuffle(documents, categories)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(categories)\n",
    "\n",
    "#Category Breakdown\n",
    "c = Counter(categories)\n",
    "for i in c:\n",
    "    print i, c[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Classifying documents with the DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create DTM\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "doc_train, doc_test, y_train, y_test = train_test_split(documents, y, test_size = .33)\n",
    "\n",
    "#create document term matrix with CountVectorizer\n",
    "Vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "#create training and testing DTM\n",
    "X_train_dtm = Vectorizer.fit_transform(doc_train).todense()\n",
    "X_test_dtm = Vectorizer.transform(doc_test).todense()\n",
    "\n",
    "print \"Shape of Document Term Matrix: {}\".format(X_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classify using DTM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import log_loss \n",
    "from sklearn.metrics import classification_report, f1_score, precision_score\n",
    "import pandas as pd\n",
    "\n",
    "f1_scores = {}\n",
    "models = {}\n",
    "losses = {}\n",
    "\n",
    "def train_and_validate(name, model_classes,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    if name not in f1_scores:\n",
    "        f1_scores[name] = {}\n",
    "        \n",
    "    if name not in models:\n",
    "        models[name] = {}\n",
    "        \n",
    "    if name not in losses:\n",
    "        losses[name] = {}        \n",
    "        \n",
    "    for model in model_classes:\n",
    "        model.fit(X_train, y_train)    \n",
    "        predictions = model.predict(X_test)\n",
    "        probabilities = model.predict_proba(X_test)\n",
    "    \n",
    "        losses[name][model.__module__] = log_loss(y_test, probabilities)\n",
    "        f1_scores[name][model.__module__] = f1_score(y_test, predictions, average = 'weighted')\n",
    "        models[name][model.__module__] = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_classes = [LogisticRegression()  \n",
    "              , GaussianNB()\n",
    "              , SVC(kernel='linear', probability=True)\n",
    "              , RandomForestClassifier(n_estimators=100)\n",
    "              , DummyClassifier()]\n",
    "\n",
    "train_and_validate('DTM', model_classes, X_train_dtm, X_test_dtm, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "\n",
    "model_names = f1_scores['DTM'].keys()\n",
    "\n",
    "dtm_f1_trace = go.Bar(\n",
    "                    y=[f1_scores['DTM'][model] for model in model_names],\n",
    "                    x=model_names\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    barmode='group', title='F1 Scores for Using Document Term Matrices'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[dtm_f1_trace], layout = layout)\n",
    "py.iplot(fig, filename='make-subplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Classifying with Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use spacy to get word vectors\n",
    "import numpy as np\n",
    "\n",
    "X_train_vec = []\n",
    "X_test_vec = []\n",
    "\n",
    "\n",
    "for doc in nlp.pipe(doc_train, n_threads=4, tag=False, parse = False, entity=False):\n",
    "    X_train_vec.append(doc.vector)\n",
    "        \n",
    "\n",
    "for doc in nlp.pipe(doc_test, n_threads=4, tag=False, parse = False, entity=False):\n",
    "    X_test_vec.append(doc.vector)\n",
    "    \n",
    "X_train_vec = np.array(X_train_vec)\n",
    "X_test_vec = np.array(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_classes =[LogisticRegression(C = 1000.)\n",
    "              , GaussianNB()\n",
    "              , SVC(C = 10000., kernel='linear', probability = True)\n",
    "              , RandomForestClassifier(n_estimators=100)\n",
    "              , DummyClassifier()]\n",
    "\n",
    "train_and_validate('Word Vectors', model_classes, X_train_vec, X_test_vec, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_names = [i.__module__ for i in model_classes]\n",
    "\n",
    "dtm_f1_trace = go.Bar(\n",
    "                    y=[f1_scores['DTM'][model] for model in model_names],\n",
    "                    x=model_names,\n",
    "                    name = 'Document Term Matrix'\n",
    ")\n",
    "\n",
    "vect_f1_trace = go.Bar(\n",
    "                    y=[f1_scores['Word Vectors'][model] for model in model_names],\n",
    "                    x=model_names,\n",
    "                    name = 'Word Vectors'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    barmode='group', title='F1 Scores for Using Document Term Matrices', yaxis=dict(title = 'F1 Score')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[dtm_f1_trace, vect_f1_trace], layout = layout)\n",
    "py.iplot(fig, filename='make-subplots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_inputs(text):\n",
    "    \n",
    "    if hasattr(text, '__iter__'):\n",
    "        vec = np.array([nlp(unicode(t)).vector for t in text])\n",
    "        dtm = Vectorizer.transform(text)\n",
    "    else:\n",
    "        vec = nlp(unicode(text)).vector\n",
    "        dtm = Vectorizer.transform([text])\n",
    "\n",
    "    return {'DTM':dtm,\n",
    "            'Word Vector': vec\n",
    "           }\n",
    "\n",
    "\n",
    "new_docs = ['Obama was in office before Trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_vec = text_to_inputs(new_docs)['Word Vector']\n",
    "x_dtm = text_to_inputs(new_docs)['DTM']\n",
    "\n",
    "vec_test = pd.DataFrame(models['Word Vectors']['sklearn.linear_model.logistic'].predict_proba(x_vec), columns = encoder.classes_)\n",
    "dtm_test = pd.DataFrame(models['DTM']['sklearn.linear_model.logistic'].predict_proba(x_dtm), columns = encoder.classes_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'roosevelt' in Vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order = vec_test.T.sort(0).index.values\n",
    "\n",
    "dtm_f1_trace = go.Bar(\n",
    "                    y=vec_test.loc[0].loc[order].values,\n",
    "                    x=order,\n",
    "                    name = 'Word Vector Predicted'\n",
    ")\n",
    "\n",
    "vect_f1_trace = go.Bar(\n",
    "                    y=dtm_test.loc[0].loc[order].values,\n",
    "                    x=order,\n",
    "                    name = 'Document Term Predicted'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    barmode='group', title='Predicted Classes of \"Obama was in office before Trump\"', yaxis=dict(title = 'F1 Score')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[dtm_f1_trace, vect_f1_trace], layout = layout)\n",
    "py.iplot(fig, filename='make-subplots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the table, keep me hidden\n",
    "N_WORDS = 10\n",
    "N_DOCS = 10\n",
    "\n",
    "random_words = np.random.choice(cv.get_feature_names(), size=N_WORDS)\n",
    "\n",
    "def gen_random_vec(n):\n",
    "    ps = map(lambda x: 1 / (x + 1) ** 3, range(10))\n",
    "    ps = map(lambda p: p / sum(ps), ps)\n",
    "    return np.random.choice(range(10), p = ps, size = n)\n",
    "\n",
    "d = {word: gen_random_vec(N_DOCS) for word in random_words}\n",
    "dtm_random = pd.DataFrame.from_dict(d, orient = 'index')\n",
    "c = map(list,zip(*[('Documents', column) for column in dtm_random.columns]))\n",
    "dtm_random.columns  = c\n",
    "\n",
    "c = map(list,zip(*[('Words', i) for i in dtm_random.index.values]))\n",
    "dtm_random.index  = c\n",
    "\n",
    "\n",
    "with open('dtm.html', 'wb') as f:\n",
    "    f.write(dtm_random.T.to_html())"
   ]
  }
 ],
 "metadata": {
  "_datascience": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
